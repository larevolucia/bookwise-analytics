{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6eaf6d",
   "metadata": {},
   "source": [
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook performs **exploratory data analysis (EDA)** on the enriched datasets to understand book performance, catalog diversity, and user behavior.\n",
    "\n",
    "The goal is to extract insights that guide:\n",
    "\n",
    "* Feature engineering\n",
    "* Predictive modeling\n",
    "* Dashboard visualisation\n",
    "* Business interpretation (ratings, satisfaction, catalog suitability)\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* `en_supply_catalog.csv` — enriched BBE (supply) catalog\n",
    "* `en_internal_catalog.csv` — enriched Goodbooks catalog\n",
    "* `ratings_clean.csv` — cleaned user–book interactions\n",
    "* `model_dataset_warm_start.csv` — unified metadata + external signals\n",
    "* `model_dataset_cold_start.csv` — unified metadata (no external signals)\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks in This Notebook\n",
    "\n",
    "1. **Load Data**\n",
    "    Read in enriched catalogs and interaction data.\n",
    "\n",
    "2. **Book Performance Analysis**\n",
    "   Ratings, popularity, publication trends, outliers.\n",
    "\n",
    "3. **Correlation & Feature Signal Study**\n",
    "   Identify which variables show predictive potential.\n",
    "\n",
    "4. **Internal vs Supply Catalog Diversity Comparison**\n",
    "   Genre mix, popularity spread, publication-year coverage.\n",
    "\n",
    "5. **User Behavior Exploration**\n",
    "   Rating patterns, user activity, interaction sparsity.\n",
    "\n",
    "6. **Feature Engineering Recommendations**\n",
    "   Identify variables needing transformation, encoding, or imputation.\n",
    "\n",
    "7. **Generate Dashboard-Ready Visualisations**\n",
    "   Plots to be reused in the Streamlit “Data Explorer” page.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Summary tables and plots for book performance, diversity, and behavior\n",
    "* Correlation & feature signal diagnostics\n",
    "* Dashboard-ready visualisations (saved to `outputs/eda_plots/`)\n",
    "* Notes on recommended feature transformations for Notebook 05\n",
    "\n",
    ">**Note:**\n",
    ">This notebook focuses on **analysis only**.\n",
    ">Feature engineering and modeling are completed in the following notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035cad0a",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "## Navigate to the Parent Directory\n",
    "\n",
    "Before combining and saving datasets, it’s often helpful to move to a parent directory so that file operations (like loading or saving data) are easier and more organized. \n",
    "\n",
    "Before using the Python’s built-in os module to move one level up from the current working directory, it is advisable to inspect the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3fb90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f'Current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4c2af",
   "metadata": {},
   "source": [
    "To change to parent directory (root folder), run the code below. If you are already in the root folder, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c542c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to its parent\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print('Changed directory to parent.')\n",
    "\n",
    "# Get the new current working directory (the parent directory)\n",
    "current_dir = os.getcwd()\n",
    "print(f'New current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032580c3",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "In this step, we load the previously cleaned datasets for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60114adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "internal_catalog = pd.read_csv(\n",
    "    'outputs/datasets/cleaned/en_internal_catalog.csv',\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    "    )\n",
    "ratings_clean = pd.read_csv('outputs/datasets/cleaned/ratings_clean_v1.csv')\n",
    "supply_catalog = pd.read_csv(\n",
    "    \"outputs/datasets/cleaned/en_supply_catalog.csv\",\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f33a9",
   "metadata": {},
   "source": [
    "# Book Performance Analysis\n",
    "\n",
    "In this section, we analyze book performance metrics such as ratings distribution, popularity trends, and publication year analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb285b",
   "metadata": {},
   "source": [
    "## Correlation & Feature Signal Study\n",
    "\n",
    "In this section, we will analyze the correlations between different features in the dataset and their predictive power regarding the target variable, which is `rating_clean`. This analysis will help us identify which features are most relevant for our predictive modeling tasks.\n",
    "\n",
    " To be able to do this, we will use the enriched datasets loaded in the previous step. Majority of our data is in textual form, so we need to convert them into numerical format for analysis. \n",
    " \n",
    " We created a feature engineering module in `src/modeling/feature_engineering.py` to help with this task. Some values will need preparation steps before feature engineering, those are `genres_clean` and `publication_date_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39272cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cleaning.utils.helpers import safe_literal_eval\n",
    "\n",
    "# apply to both catalogs before feature engineering\n",
    "internal_catalog['genres_clean'] = internal_catalog['genres_clean'].apply(safe_literal_eval)\n",
    "supply_catalog['genres_clean'] = supply_catalog['genres_clean'].apply(safe_literal_eval)\n",
    "\n",
    "print(\"Genres converted to lists successfully.\")\n",
    "print(\"\\nSample after conversion:\")\n",
    "print(internal_catalog['genres_clean'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cleaning.utils.dates import extract_year\n",
    "\n",
    "# transform both catalogs' publication_date_clean to extract year\n",
    "internal_catalog['publication_year_clean'] = internal_catalog['publication_date_clean'].apply(extract_year)\n",
    "supply_catalog['publication_year_clean'] = supply_catalog['publication_date_clean'].apply(extract_year)\n",
    "\n",
    "print(\"Year extraction complete.\")\n",
    "print(f\"\\nInternal catalog - Valid years: {internal_catalog['publication_year_clean'].notna().sum()}\")\n",
    "print(f\"Supply catalog - Valid years: {supply_catalog['publication_year_clean'].notna().sum()}\")\n",
    "print(f\"\\nYear range (internal): {internal_catalog['publication_year_clean'].min()} - {internal_catalog['publication_year_clean'].max()}\")\n",
    "print(f\"Year range (supply): {supply_catalog['publication_year_clean'].min()} - {supply_catalog['publication_year_clean'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf0430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modeling.feature_engineering import fe_engineering\n",
    "\n",
    "print(\"Feature engineering module imported.\")\n",
    "\n",
    "# apply feature engineering to internal catalog\n",
    "internal_catalog_fe = fe_engineering(\n",
    "    df=internal_catalog,\n",
    "    encode_text_embeddings=False,\n",
    "    top_n_authors=50,\n",
    "    top_n_genres=30,\n",
    "    bool_cols=['has_award', 'is_major_publisher'], \n",
    "    text_col='description_clean',\n",
    "    genres_col='genres_clean',\n",
    "    author_col='author_clean',\n",
    "    publisher_col='publisher_clean',\n",
    "    series_col='series_clean'\n",
    ")\n",
    "\n",
    "print(f\"Internal catalog shape: {internal_catalog_fe.shape}\")\n",
    "print(f\"New features added: {set(internal_catalog_fe.columns) - set(internal_catalog.columns)}\")\n",
    "display(internal_catalog_fe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6e373",
   "metadata": {},
   "source": [
    "Next, we will remove any columns that may lead to data leakage. These columns contain information that would not be available at the time of prediction and could artificially inflate the performance of our models. And apply correlation and predictive power analysis on the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "leakage_cols = [\n",
    "    # Raw rating distribution components (leakage)\n",
    "    'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5',\n",
    "    'ratings_1_share', 'ratings_2_share', 'ratings_3_share',\n",
    "    'ratings_4_share', 'ratings_5_share',\n",
    "]\n",
    "\n",
    "# Drop leakage columns if they exist\n",
    "internal_clean = internal_catalog_fe.drop(\n",
    "    columns=[c for c in leakage_cols if c in internal_catalog_fe.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171bac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the features for correlation analysis\n",
    "num_cols = internal_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Exclude ratings share columns from correlation\n",
    "# Ensure rating_clean is the first column (target)\n",
    "num_cols = ['rating_clean'] + [c for c in num_cols if c != 'rating_clean']\n",
    "\n",
    "print(f\"Numeric features found ({len(num_cols)}):\")\n",
    "print(num_cols)\n",
    "\n",
    "correlation_features = [\n",
    "    'rating_clean',\n",
    "    'numratings_clean',\n",
    "    'pages_clean',\n",
    "    'publication_year_clean',\n",
    "    'has_award_encoded',\n",
    "    'is_major_publisher_encoded',\n",
    "    'genre_count',\n",
    "    'has_genres',\n",
    "    'is_top_genre',\n",
    "    'author_book_count',\n",
    "    'is_top_author',\n",
    "    'publisher_book_count',\n",
    "    'in_series',\n",
    "    'description_length',\n",
    "    'description_word_count'\n",
    "    'work_text_reviews_log',\n",
    "]\n",
    "\n",
    "correlation_features = [c for c in correlation_features if c in internal_clean.columns]\n",
    "\n",
    "df_corr = internal_clean[correlation_features]\n",
    "\n",
    "df_num = df_corr.dropna(how='all')\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = df_num.corr()\n",
    "\n",
    "print(\"\\nCORRELATION SUMMARY\")\n",
    "\n",
    "if 'rating_clean' in df_corr.columns:\n",
    "    rating_correlations = corr_matrix['rating_clean'].drop('rating_clean').sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nTop Features Positively Correlated with Rating:\")\n",
    "    print(rating_correlations[rating_correlations > 0].head(10))\n",
    "\n",
    "    print(\"\\nTop Features Negatively Correlated with Rating:\")\n",
    "    print(rating_correlations[rating_correlations < 0].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03febd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppscore as pps\n",
    "import pandas as pd\n",
    "\n",
    "# PPS of all features predicting rating_clean\n",
    "pps_rating = pps.predictors(\n",
    "    internal_clean,\n",
    "    y=\"rating_clean\",\n",
    "    output=\"df\"   # returns a tidy DataFrame\n",
    ").sort_values(\"ppscore\", ascending=False)\n",
    "\n",
    "pps_rating.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top = pps_rating.head(15)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(top[\"x\"], top[\"ppscore\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Predictive Power Score\")\n",
    "plt.title(\"Top PPS predictors of rating_clean\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9c245",
   "metadata": {},
   "source": [
    "Both correlation analysis and Predictive Power Score (PPS) showed low correlation and predictive power for most features. We will validate if this is due to the nature of the data by performing similar analysis on the supply catalog dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953399b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# apply feature engineering to supply catalog\n",
    "supply_catalog_fe = fe_engineering(\n",
    "    df=supply_catalog,\n",
    "    encode_text_embeddings=False,\n",
    "    top_n_authors=50,\n",
    "    top_n_genres=30,\n",
    "    bool_cols=['has_award', 'is_major_publisher'],\n",
    "    text_col='description_clean',\n",
    "    genres_col='genres_clean',\n",
    "    author_col='author_clean',\n",
    "    publisher_col='publisher_clean',\n",
    "    series_col='series_clean'\n",
    ")\n",
    "\n",
    "print(f\"Supply catalog shape: {supply_catalog_fe.shape}\")\n",
    "print(f\"New features added: {set(supply_catalog_fe.columns) - set(supply_catalog.columns)}\")\n",
    "display(supply_catalog_fe.head())\n",
    "\n",
    "# Drop only if the columns exist\n",
    "supply_clean = supply_catalog_fe.drop(\n",
    "    columns=[c for c in leakage_cols if c in supply_catalog_fe.columns],\n",
    "    errors='ignore'\n",
    ")\n",
    "\n",
    "print(f\"Supply catalog shape after leakage removal: {supply_clean.shape}\")\n",
    "\n",
    "\n",
    "correlation_features = [\n",
    "    'rating_clean',\n",
    "    'numRatings_clean',\n",
    "    'pages_clean',\n",
    "    'likedPercent_clean',\n",
    "    'bookFormat_cleaned',\n",
    "    'bbeVotes_clean',\n",
    "    'bbeScore_clean',\n",
    "    'price_clean',\n",
    "    'publication_year_clean',\n",
    "    'has_award_encoded',\n",
    "    'is_major_publisher_encoded',\n",
    "    'genre_count',\n",
    "    'has_genres',\n",
    "    'is_top_genre',\n",
    "    'author_book_count',\n",
    "    'is_top_author',\n",
    "    'publisher_book_count',\n",
    "    'in_series',\n",
    "    'description_length',\n",
    "    'description_word_count'\n",
    "]\n",
    "\n",
    "correlation_features = [\n",
    "    c for c in correlation_features if c in supply_clean.columns\n",
    "]\n",
    "\n",
    "df_corr_sup = supply_clean[correlation_features]\n",
    "\n",
    "corr_matrix_sup = df_corr_sup.corr()\n",
    "\n",
    "print(\"\\n=== SUPPLY CATALOG — CORRELATION SUMMARY ===\")\n",
    "\n",
    "if 'rating_clean' in correlation_features:\n",
    "    rating_corr_sup = corr_matrix_sup['rating_clean'] \\\n",
    "                        .drop('rating_clean') \\\n",
    "                        .sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nTop Positive Correlations:\")\n",
    "    print(rating_corr_sup[rating_corr_sup > 0].head(10))\n",
    "\n",
    "    print(\"\\nTop Negative Correlations:\")\n",
    "    print(rating_corr_sup[rating_corr_sup < 0].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b98498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppscore as pps\n",
    "import pandas as pd\n",
    "\n",
    "pps_supply = (\n",
    "    pps.predictors(\n",
    "        supply_clean,\n",
    "        y=\"rating_clean\",\n",
    "        output=\"df\"\n",
    "    )\n",
    "    .sort_values(\"ppscore\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n=== SUPPLY CATALOG — TOP PPS FEATURES ===\")\n",
    "display(pps_supply.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c6e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_sup = pps_supply.head(15)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.barh(top_sup[\"x\"], top_sup[\"ppscore\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Predictive Power Score\")\n",
    "plt.title(\"Top PPS Predictors of rating_clean (Supply Catalog)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3b60a",
   "metadata": {},
   "source": [
    "The correlation and PPS results from the supply catalog closely match those of the internal catalog, confirming that the overall low predictive signal is inherent to the nature of book metadata rather than a limitation of a specific dataset. \n",
    "\n",
    "Across both catalogs, the only feature showing meaningful predictive strength is `likedPercent_clean`, which is intuitively expected since it reflects direct user satisfaction. Its high correlation (≈0.80) and strong PPS (≈0.50) reinforce this interpretation. \n",
    "\n",
    "This demonstrates that without behavioral indicators, both correlation and predictive power will naturally remain low for this type of data. With this understanding, the next step is to use the **warm-start dataset** to evaluate whether external behavioral signals also correlate with internal user behavior, thereby validating our modeling approach and confirming whether cross-platform signals can reliably enhance prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "warm_start_path = Path('outputs/datasets/modeling/model_dataset_warm_start.csv')\n",
    "pd_warm_start = pd.read_csv(warm_start_path,\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    ")\n",
    "print(f\"Warm-start dataset shape: {pd_warm_start.shape}\")\n",
    "display(pd_warm_start.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# copy warm-start dataset\n",
    "warm_clean = pd_warm_start.copy() \n",
    "\n",
    "# select only numeric columns\n",
    "num_cols_warm = warm_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# ensure gb_rating_clean is in the matrix\n",
    "if 'gb_rating_clean' not in num_cols_warm:\n",
    "    raise ValueError(\"gb_rating_clean not found in warm start dataset.\")\n",
    "\n",
    "# compute correlation matrix\n",
    "corr_matrix_warm = warm_clean[num_cols_warm].corr()\n",
    "\n",
    "# extract correlation with internal rating\n",
    "warm_rating_corr = (\n",
    "    corr_matrix_warm['gb_rating_clean']\n",
    "    .drop('gb_rating_clean')\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n=== WARM START — TOP POSITIVE CORRELATIONS ===\")\n",
    "print(warm_rating_corr[warm_rating_corr > 0].head(10))\n",
    "\n",
    "print(\"\\n=== WARM START — TOP NEGATIVE CORRELATIONS ===\")\n",
    "print(warm_rating_corr[warm_rating_corr < 0].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a92c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ppscore as pps\n",
    "import pandas as pd\n",
    "\n",
    "pps_warm = (\n",
    "    pps.predictors(\n",
    "        warm_clean,\n",
    "        y=\"gb_rating_clean\",\n",
    "        output=\"df\"\n",
    "    )\n",
    "    .sort_values(\"ppscore\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n=== WARM START — TOP PPS FEATURES ===\")\n",
    "display(pps_warm.head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6d981",
   "metadata": {},
   "source": [
    "The warm-start analysis shows that external behavioral metrics, such as `external_rating`, `external_bbe_ratings_5_share`, `external_bbe_ratings_3_share`, and `external_likedpct`, are the strongest predictors of our internal rating (`gb_rating_clean`), with correlations as high as 0.99 and PPS scores up to 0.89. \n",
    "\n",
    "This confirms that the low signal observed earlier is not a dataset issue but an inherent limitation of metadata for predicting user satisfaction, and that meaningful predictive signal emerges only when behavioral indicators are available. With this evidence, we can now confidently proceed with a warm-start modeling strategy focused on cross-platform validation, using external behavioral data as a reliable proxy for internal preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d52d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.modeling.feature_engineering import fe_engineering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create sample data\n",
    "sample_books = pd.DataFrame({\n",
    "    'book_id': [1, 2, 3, 4],\n",
    "    'description_clean': [\n",
    "        'A dystopian novel about totalitarian surveillance and thought control.',\n",
    "        'An epic fantasy adventure featuring hobbits, wizards, and dragons.',\n",
    "        'A romantic story of love and social class in 19th century England.',\n",
    "        None  # Test missing description\n",
    "    ],\n",
    "    'genres_clean': [\n",
    "        ['science fiction', 'dystopia'],\n",
    "        ['fantasy', 'adventure'],\n",
    "        ['romance', 'classic'],\n",
    "        ['fiction']\n",
    "    ],\n",
    "    'author_clean': ['George Orwell', 'J.R.R. Tolkien', 'Jane Austen', 'Unknown'],\n",
    "    'publisher_clean': ['Penguin', 'HarperCollins', 'Penguin', 'Small Press'],\n",
    "    'series_clean': [None, 'The Lord of the Rings', None, 'Series X'],\n",
    "    'has_award': [True, True, False, False],\n",
    "    'is_major_publisher': [True, True, True, False]\n",
    "})\n",
    "\n",
    "print(\"=== TESTING TEXT EMBEDDINGS ===\\n\")\n",
    "\n",
    "# Apply feature engineering with embeddings enabled\n",
    "result = fe_engineering(\n",
    "    df=sample_books,\n",
    "    encode_text_embeddings=True,\n",
    "    top_n_authors=2,\n",
    "    top_n_genres=2,\n",
    "    bool_cols=['has_award', 'is_major_publisher'],\n",
    "    text_col='description_clean',\n",
    "    genres_col='genres_clean',\n",
    "    author_col='author_clean',\n",
    "    publisher_col='publisher_clean',\n",
    "    series_col='series_clean'\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"Total books processed: {len(result)}\")\n",
    "print(f\"Books with embeddings: {result['text_embedding'].notna().sum()}\")\n",
    "\n",
    "# Check first embedding\n",
    "first_embedding = result.loc[0, 'text_embedding']\n",
    "print(f\"\\nFirst embedding shape: {first_embedding.shape}\")\n",
    "print(f\"Embedding dimension: {len(first_embedding)}\")\n",
    "print(f\"\\nFirst 10 values: {first_embedding[:10]}\")\n",
    "\n",
    "# Print statistics for each book\n",
    "print(\"\\n=== EMBEDDING STATISTICS ===\\n\")\n",
    "for idx, row in result.iterrows():\n",
    "    desc = row['description_clean']\n",
    "    emb = row['text_embedding']\n",
    "    desc_preview = desc[:60] if isinstance(desc, str) else \"None\"\n",
    "    print(f\"Book {idx + 1}: '{desc_preview}...'\")\n",
    "    print(f\"  Mean: {emb.mean():.4f}, Std: {emb.std():.4f}\")\n",
    "    print(f\"  Min: {emb.min():.4f}, Max: {emb.max():.4f}\")\n",
    "    print(f\"  L2 Norm: {np.linalg.norm(emb):.4f}\\n\")\n",
    "\n",
    "# Similarity analysis\n",
    "print(\"=== COSINE SIMILARITY ANALYSIS ===\\n\")\n",
    "embeddings_matrix = np.vstack(result['text_embedding'].values)\n",
    "similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "similarity_df = pd.DataFrame(\n",
    "    similarity_matrix,\n",
    "    index=[f\"Book {i+1}\" for i in range(len(result))],\n",
    "    columns=[f\"Book {i+1}\" for i in range(len(result))]\n",
    ")\n",
    "print(similarity_df.round(4))\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Text Embedding Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Heatmap of first 50 dimensions\n",
    "embeddings_50 = embeddings_matrix[:, :50]\n",
    "im1 = axes[0, 0].imshow(embeddings_50, aspect='auto', cmap='coolwarm', vmin=-0.5, vmax=0.5)\n",
    "axes[0, 0].set_title('First 50 Embedding Dimensions')\n",
    "axes[0, 0].set_xlabel('Dimension')\n",
    "axes[0, 0].set_ylabel('Book')\n",
    "axes[0, 0].set_yticks(range(len(result)))\n",
    "axes[0, 0].set_yticklabels([f\"Book {i+1}\" for i in range(len(result))])\n",
    "plt.colorbar(im1, ax=axes[0, 0], label='Value')\n",
    "\n",
    "# Plot 2: Distribution of values for Book 1\n",
    "axes[0, 1].hist(first_embedding, bins=40, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 1].set_title('Embedding Value Distribution (Book 1)')\n",
    "axes[0, 1].set_xlabel('Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: L2 Norm comparison\n",
    "magnitudes = [np.linalg.norm(emb) for emb in result['text_embedding']]\n",
    "colors = ['steelblue' if mag > 0 else 'red' for mag in magnitudes]\n",
    "axes[1, 0].bar(range(len(magnitudes)), magnitudes, color=colors, alpha=0.7)\n",
    "axes[1, 0].set_title('Embedding Magnitude (L2 Norm)')\n",
    "axes[1, 0].set_xlabel('Book')\n",
    "axes[1, 0].set_ylabel('Magnitude')\n",
    "axes[1, 0].set_xticks(range(len(result)))\n",
    "axes[1, 0].set_xticklabels([f\"Book {i+1}\" for i in range(len(result))])\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Cosine similarity heatmap\n",
    "im4 = axes[1, 1].imshow(similarity_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[1, 1].set_title('Cosine Similarity Matrix')\n",
    "axes[1, 1].set_xlabel('Book')\n",
    "axes[1, 1].set_ylabel('Book')\n",
    "axes[1, 1].set_xticks(range(len(result)))\n",
    "axes[1, 1].set_yticks(range(len(result)))\n",
    "axes[1, 1].set_xticklabels([f\"B{i+1}\" for i in range(len(result))])\n",
    "axes[1, 1].set_yticklabels([f\"B{i+1}\" for i in range(len(result))])\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(result)):\n",
    "    for j in range(len(result)):\n",
    "        text = axes[1, 1].text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
    "                              ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.colorbar(im4, ax=axes[1, 1], label='Similarity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/eda_plots/text_embeddings_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✓ Visualization saved to: outputs/eda_plots/text_embeddings_analysis.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
