{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d8b2adf",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook implements **feature engineering** actions recommended by the exploratory analysis in Notebook 04. The goal is to transform raw and enriched datasets into modeling-ready features that address rating inflation, correct skewed distributions, and aggregate user-level signals for clustering and personalization.\n",
    "\n",
    "Key priorities:\n",
    "\n",
    "* Bayesian/weighted ratings to correct sample-size bias\n",
    "* Log transformations for skewed count features\n",
    "* User profile aggregation for segmentation and clustering\n",
    "* Save feature-engineered datasets for modeling\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* `internal_catalog_analysis.csv` — feature-engineered internal catalog\n",
    "* `supply_catalog_analysis.csv` — feature-engineered supply catalog\n",
    "* `ratings_clean_v1.csv` — cleaned user–book interactions\n",
    "* `user_activity.csv`, `user_diversity.csv`, `user_genre_prefs.csv` — user-level analysis datasets\n",
    "* `model_dataset_warm_start.csv` — unified metadata + external signals (for validation)\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks in This Notebook\n",
    "\n",
    "1. **Load Data**\n",
    "    Import feature-engineered catalogs and user interaction datasets.\n",
    "\n",
    "2. **Implement Bayesian/Weighted Ratings**\n",
    "   Apply Bayesian average formula to correct rating inflation in low-volume books, authors, genres, and publishers.\n",
    "\n",
    "3. **Apply Log Transformations**\n",
    "   Transform skewed count features (ratings, author/publisher book counts) for regression compatibility.\n",
    "\n",
    "4. **Aggregate User Profile Features**\n",
    "   Merge rating, diversity, and preference metrics into user-level features for clustering and segmentation.\n",
    "\n",
    "5. **Save Feature-Engineered Datasets**\n",
    "   Export modeling-ready datasets for downstream tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Feature-engineered catalogs with weighted ratings and log-transformed features\n",
    "* Aggregated user profile dataset for clustering and personalization\n",
    "* Modeling-ready datasets saved to `outputs/datasets/modeling/`\n",
    "* Documentation of feature engineering logic and business rationale\n",
    "\n",
    ">**Note:**\n",
    ">This notebook focuses on **feature engineering only**.\n",
    ">Model training and evaluation are completed in the following notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c19708",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "## Navigate to the Parent Directory\n",
    "\n",
    "Before combining and saving datasets, it’s often helpful to move to a parent directory so that file operations (like loading or saving data) are easier and more organized. \n",
    "\n",
    "Before using the Python’s built-in os module to move one level up from the current working directory, it is advisable to inspect the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c2259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f'Current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b672f2",
   "metadata": {},
   "source": [
    "To change to parent directory (root folder), run the code below. If you are already in the root folder, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f13ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to its parent\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print('Changed directory to parent.')\n",
    "\n",
    "# Get the new current working directory (the parent directory)\n",
    "current_dir = os.getcwd()\n",
    "print(f'New current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a74b04",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "In this step, we load the previously cleaned datasets for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# catalogs\n",
    "internal_catalog_fe = pd.read_csv('outputs/datasets/analysis/internal_catalog_analysis.csv')\n",
    "\n",
    "# ratings and user-level analysis\n",
    "ratings = pd.read_csv('outputs/datasets/cleaned/ratings_clean.csv')\n",
    "user_activity = pd.read_csv('outputs/datasets/analysis/user_activity.csv')\n",
    "user_diversity = pd.read_csv('outputs/datasets/analysis/user_diversity.csv')\n",
    "book_genre_mapping = pd.read_csv('outputs/datasets/analysis/internal_book_genre_mapping.csv')\n",
    "\n",
    "# unified metadata for validation\n",
    "warm_start = pd.read_csv('outputs/datasets/cleaned/model_dataset_warm_start.csv')\n",
    "\n",
    "print(f\"Internal catalog shape: {internal_catalog_fe.shape}\")\n",
    "print(f\"Ratings shape: {ratings.shape}\")\n",
    "print(f\"User activity shape: {user_activity.shape}\")\n",
    "print(f\"User diversity shape: {user_diversity.shape}\")\n",
    "print(f\"Warm start dataset shape: {warm_start.shape}\")\n",
    "print(f\"Internal Catalog book-genre mapping: {book_genre_mapping.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2de24",
   "metadata": {},
   "source": [
    "# Log Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "\n",
    "def check_skew_and_range(df, features, plot=True):\n",
    "    results = []\n",
    "    for col in features:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        vals = df[col].dropna()\n",
    "        if not np.issubdtype(vals.dtype, np.number):\n",
    "            continue\n",
    "        rng = vals.max() - vals.min()\n",
    "        skw = skew(vals)\n",
    "        pct_zeros = (vals == 0).mean()\n",
    "        pct_ones = (vals == 1).mean()\n",
    "        results.append({\n",
    "            'feature': col,\n",
    "            'min': vals.min(),\n",
    "            'max': vals.max(),\n",
    "            'range': rng,\n",
    "            'skew': skw,\n",
    "            'pct_zeros': pct_zeros,\n",
    "            'pct_ones': pct_ones,\n",
    "            'mean': vals.mean(),\n",
    "            'std': vals.std(),\n",
    "        })\n",
    "        if plot:\n",
    "            plt.figure(figsize=(6,2))\n",
    "            sns.histplot(vals, bins=50, kde=True)\n",
    "            plt.title(f\"{col} (skew={skw:.2f}, range={rng:.2e})\")\n",
    "            plt.show()\n",
    "    return pd.DataFrame(results).sort_values('skew', ascending=False)\n",
    "\n",
    "# internal catalog features to check\n",
    "features_to_check = [\n",
    "    'numRatings_clean', 'author_book_count', 'publisher_book_count',\n",
    "    'work_text_reviews_count', 'pages_clean', 'description_length', 'description_word_count',\n",
    "]\n",
    "check_skew_and_range(internal_catalog_fe, features_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5315a",
   "metadata": {},
   "source": [
    "| Feature                   | Range   | Skew    | Power Law                  | Distribution Shape         | Log Transform? |\n",
    "|---------------------------|---------|---------|----------------------------|---------------------------|:--------------:|\n",
    "| numRatings_clean          | High    | High    | Yes, extreme long tail     | Extreme right skew, power law |      Yes       |\n",
    "| work_text_reviews_count   | High    | High    | Yes, long tail             | Extreme right skew        |      Yes       |\n",
    "| pages_clean               | High    | High    | Weak, some long tail       | Right skew                |      Yes       |\n",
    "| author_book_count         | Medium  | High    | Yes, long tail             | Right skew                |      Yes       |\n",
    "| description_length        | High    | Low     | No, mild tail              | Mild right skew           |   Optional     |\n",
    "| description_word_count    | Medium  | Low     | No, mild tail              | Mild right skew           |   Optional     |\n",
    "| publisher_book_count      | High    | Low     | Bimodal, some long tail    | Bimodal, moderate skew    |   Consider     |\n",
    "\n",
    "\n",
    "**Summary:**  \n",
    "- **Apply log transform** to features with high skew (>2) and/or very large ranges:  \n",
    "  `numRatings_clean` (already done), `work_text_reviews_count`(already done), `pages_clean`, `author_book_count`\n",
    "- **Optional** for features with mild skew but large range:  \n",
    "  `description_length`, `description_word_count`\n",
    "- **Consider** for `publisher_book_count` if model performance improves.\n",
    "\n",
    "\n",
    "\n",
    "> note: A power law is a type of statistical distribution where a small number of items have very large values, while most items have small values. This results in a long tail on the right side of the distribution curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log1 to selected features\n",
    "internal_catalog_fe['author_book_count_log'] = np.log1p(internal_catalog_fe['author_book_count'])\n",
    "internal_catalog_fe['pages_log'] = np.log1p(internal_catalog_fe['pages_clean'])\n",
    "internal_catalog_fe['description_length_log'] = np.log1p(internal_catalog_fe['description_length'])\n",
    "internal_catalog_fe['description_word_count_log'] = np.log1p(internal_catalog_fe['description_word_count'])\n",
    "internal_catalog_fe['publisher_book_count_log'] = np.log1p(internal_catalog_fe['publisher_book_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be92ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# external features to check\n",
    "external_features_to_check = [\n",
    "    'external_likedpct',\n",
    "    'external_numratings',\n",
    "    'external_votes',\n",
    "    'external_score',\n",
    "    'external_price'\n",
    "]\n",
    "check_skew_and_range(warm_start, external_features_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88300fbe",
   "metadata": {},
   "source": [
    "| Feature             | Range   | Skew    | Power Law                  | Distribution Shape         | Log Transform? |\n",
    "|---------------------|---------|---------|----------------------------|---------------------------|:--------------:|\n",
    "| external_score      | High    | High    | Yes, extreme long tail     | Extreme right skew        |      Yes       |\n",
    "| external_votes      | High    | High    | Yes, long tail             | Extreme right skew        |      Yes       |\n",
    "| external_price      | High    | High    | Yes, long tail             | Extreme right skew        |      Yes       |\n",
    "| external_numratings | High    | High    | Yes, extreme long tail     | Extreme right skew        |      Yes       |\n",
    "| external_likedpct   | Low     | Low     | No, concentrated           | Mild left skew            |      No        |\n",
    "\n",
    "**Summary:**  \n",
    "- **Apply log transform** to all external features with high skew and/or very large ranges and power-law tails:  \n",
    "  `external_score`, `external_votes`, `external_price`, `external_numratings`\n",
    "- **No log transform** needed for `external_likedpct` due to low skew and narrow range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bda3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform external features with high skew and range\n",
    "warm_start['external_score_log'] = np.log1p(warm_start['external_score'])\n",
    "warm_start['external_votes_log'] = np.log1p(warm_start['external_votes'])\n",
    "warm_start['external_price_log'] = np.log1p(warm_start['external_price'])\n",
    "warm_start['external_numratings_log'] = np.log1p(warm_start['external_numratings'])\n",
    "# No log transform for external_likedpct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96631065",
   "metadata": {},
   "source": [
    "# Popularity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (z-score)\n",
    "for col in ['work_text_reviews_log', 'numRatings_log', 'rating_clean']:\n",
    "    internal_catalog_fe[f'{col}_z'] = (internal_catalog_fe[col] - internal_catalog_fe[col].mean()) / internal_catalog_fe[col].std()\n",
    "\n",
    "# Create popularity score (equal weights)\n",
    "internal_catalog_fe['popularity_score'] = (\n",
    "    internal_catalog_fe['work_text_reviews_log_z'] +\n",
    "    internal_catalog_fe['numRatings_log_z'] +\n",
    "    internal_catalog_fe['rating_clean_z']\n",
    ")\n",
    "\n",
    "# Preview top popular books\n",
    "internal_catalog_fe[['title_clean', 'popularity_score', 'work_text_reviews_log', 'numRatings_log', 'rating_clean']].sort_values('popularity_score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Standardize features (z-score)\n",
    "for col in ['external_rating', 'external_numratings_log', 'external_votes_log', 'external_score_log', 'external_likedpct']:\n",
    "    warm_start[f'{col}_z'] = (warm_start[col] - warm_start[col].mean()) / warm_start[col].std()\n",
    "\n",
    "# Create external popularity score (equal weights)\n",
    "warm_start['external_popularity_score'] = (\n",
    "    warm_start['external_rating_z'] +\n",
    "    warm_start['external_numratings_log_z'] +\n",
    "    warm_start['external_votes_log_z'] +\n",
    "    warm_start['external_score_log_z'] +\n",
    "    warm_start['external_likedpct_z']\n",
    ")\n",
    "\n",
    "# Preview top popular items\n",
    "warm_start[['title_final', 'external_popularity_score', 'external_rating', 'external_numratings', 'external_votes', 'external_score', 'external_likedpct']].sort_values('external_popularity_score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47afe75",
   "metadata": {},
   "source": [
    "# User Profile Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_activity.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join ratings with internal catalog features on book_id\n",
    "user_book = ratings.merge(\n",
    "    internal_catalog_fe[['book_id', 'publication_year_clean', 'pages_clean', 'has_award_encoded', 'is_major_publisher_encoded', 'genre_count', 'popularity_score']],\n",
    "    on='book_id', how='left'\n",
    ")\n",
    "\n",
    "# aggregate to user level\n",
    "user_profile = user_book.groupby('user_id').agg(\n",
    "    rating_count=('rating', 'count'),\n",
    "    rating_mean=('rating', 'mean'),\n",
    "    rating_std=('rating', 'std'),\n",
    "    pub_year_mean=('publication_year_clean', 'mean'),\n",
    "    pages_mean=('pages_clean', 'mean'),\n",
    "    award_pref=('has_award_encoded', 'mean'),\n",
    "    major_pub_pref=('is_major_publisher_encoded', 'mean'),\n",
    "    genre_count_mean=('genre_count', 'mean'),\n",
    "    popularity_mean=('popularity_score', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# merge with user activity and diversity\n",
    "user_profile = user_profile.merge(\n",
    "    user_diversity, on='user_id', how='left'\n",
    ")\n",
    "user_profile = user_profile.merge(\n",
    "    user_activity, on='user_id', how='left'\n",
    ")\n",
    "\n",
    "print(user_profile.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9dee1",
   "metadata": {},
   "source": [
    "# Save Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d2de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile.to_csv('outputs/datasets/modeling/user_profile_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
