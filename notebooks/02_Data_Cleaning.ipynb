{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51fc4fb3",
   "metadata": {},
   "source": [
    "\n",
    "# Data Cleaning\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The purpose of this notebook is to **clean, standardize, and prepare the collected datasets** for subsequent exploratory analysis and modeling tasks.\n",
    "\n",
    "The goal is to transform raw inputs from multiple book datasets into a **reliable, consistent, and mergeable analytical base**, ensuring data integrity and comparability across platforms.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "| Dataset                    | Source                     | Description                                                               | Format |\n",
    "| -------------------------- | -------------------------- | ------------------------------------------------------------------------- | ------ |\n",
    "| `bbe_books.csv`            | Zenodo – *Best Books Ever* | Book metadata including title, author, rating, genres, and description.   | CSV    |\n",
    "| `books.csv`, `ratings.csv` | GitHub – *Goodbooks-10k*   | Book metadata and user–book interaction data for recommendation modeling. | CSV    |\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks in This Notebook\n",
    "\n",
    "This notebook will execute the following cleaning and preparation steps:\n",
    "\n",
    "1. **Standardize column formats:**\n",
    "   Ensure consistent data types and naming conventions across datasets (e.g., convert `isbn` to string, align `author`, `rating`, and `title` formats).\n",
    "\n",
    "2. **Clean and normalize missing values:**\n",
    "   Replace placeholder NaNs (`9999999999999`, empty lists, or `\"None\"`) with `np.nan`, then impute or drop based on analytical importance.\n",
    "\n",
    "3. **Detect and resolve duplicates:**\n",
    "   Identify duplicate records using key identifiers (`bookId`, `isbn`, `title + author`) and retain the most complete or relevant entries.\n",
    "\n",
    "4. **Validate and align categorical values:**\n",
    "   Standardize genre labels, language codes, and rating scales to ensure comparability between datasets.\n",
    "\n",
    "5. **Merge compatible datasets:**\n",
    "   Integrate *BestBooksEver* and *Goodbooks-10k_books* into a unified schema while maintaining referential integrity with the ratings dataset.\n",
    "\n",
    "6. **Outlier and consistency checks:**\n",
    "   Review numerical and date fields (e.g., `pages`, `price`, `publishDate`) for unrealistic or extreme values and adjust as needed.\n",
    "\n",
    "7. **Feature enrichment (optional):**\n",
    "   Derive or enhance fields such as `popularity_score`, `recency`, or missing genre information using external APIs where beneficial.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **Cleaned, schema-aligned datasets** ready for exploratory data analysis and modeling.\n",
    "* **Summary statistics** on completeness, duplicates, and outliers.\n",
    "* **Processed CSV files** saved for reproducibility in `data/processed/`.\n",
    "\n",
    "> **Note:** This notebook focuses on the *Data Cleaning and Preparation*. Further feature engineering and model-specific transformations will follow in later notebooks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383657df",
   "metadata": {},
   "source": [
    "## Navigate to the Parent Directory\n",
    "\n",
    "Before combining and saving datasets, it’s often helpful to move to a parent directory so that file operations (like loading or saving data) are easier and more organized. \n",
    "\n",
    "Before using the Python’s built-in os module to move one level up from the current working directory, it is advisable to inspect the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc6e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\reisl\\OneDrive\\Documents\\GitHub\\bookwise-analytics\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f'Current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f3220",
   "metadata": {},
   "source": [
    "To change to parent directory (root folder), run the code below. If you are already in the root folder, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c41cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to parent.\n",
      "New current directory: c:\\Users\\reisl\\OneDrive\\Documents\\GitHub\\bookwise-analytics\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory to its parent\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print('Changed directory to parent.')\n",
    "\n",
    "# Get the new current working directory (the parent directory)\n",
    "current_dir = os.getcwd()\n",
    "print(f'New current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1120c",
   "metadata": {},
   "source": [
    "## Load and Inspect Books Datasets\n",
    "\n",
    "In this step, we load the previously collected datasets: **Goodbooks-10k** (books) and **Best Books Ever**. We will inspect their structure one more time before starting any merging or cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load datasets\n",
    "books_raw = pd.read_csv('data/raw/books.csv')\n",
    "bbe_raw = pd.read_csv('data/raw/bbe_books.csv')\n",
    "\n",
    "# create copies for cleaning\n",
    "books_clean = books_raw.copy()\n",
    "bbe_clean = bbe_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "interim_bbe_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "interim_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 0\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "books_clean.to_csv(interim_gb_path / f\"books_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "display(bbe_clean.head(3))\n",
    "display(books_clean.head(3))\n",
    "\n",
    "# Check shape and missing values\n",
    "for name, df in {'BBE': bbe_clean, 'Books': books_clean,}.items():\n",
    "    print(f\"\\n{name} — Shape: {df.shape}\")\n",
    "    print(df.info())\n",
    "    print(df.isna().sum().sort_values(ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b59d8",
   "metadata": {},
   "source": [
    "We will check if the datasets share common identifiers and compatible data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4382d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_only_columns = set(bbe_clean.columns) - set(books_clean.columns)\n",
    "print(f'Columns only in BBE: {bbe_only_columns}')\n",
    "\n",
    "goodbooks_only_columns = set(books_clean.columns) - set(bbe_clean.columns)\n",
    "print(f'Columns only in Goodbooks: {goodbooks_only_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66be32",
   "metadata": {},
   "source": [
    "Based on the initial inspection, we can create a mapping table to align columns from both datasets for merging and analysis.\n",
    "\n",
    "| **BestBooksEver (BBE)** | **Goodbooks10k_books (GB10k)** | **Notes / Alignment Rationale** |\n",
    "| --------------------------------- | ------------------------------------------------ | -------------------------------------------------------------------------- |\n",
    "| `bookId` | `book_id` | Main identifier; ensure both are numeric. |\n",
    "| `bookId_num` | `goodreads_book_id` | Goodreads identifier; ensure both are numeric for joining. |\n",
    "| `title` | `title` | Direct match. Used as secondary join key. |\n",
    "| `series` | — | Only in BBE; could enrich GB10k if available via API. |\n",
    "| `author` | `authors` | Same meaning. Normalize format. |\n",
    "| `rating` | `average_rating` | Equivalent — rename to unified `average_rating`. |\n",
    "| `numRatings` | `ratings_count` | Same measure of total user ratings. |\n",
    "| `ratingsByStars` | `ratings_1` … `ratings_5` | BBE has dict, GB10k has explicit columns. Expand or aggregate accordingly. |\n",
    "| `likedPercent` | — | BBE-only; optional metric of user sentiment. |\n",
    "| `isbn` | `isbn` / `isbn13` | Common linking key; keep both (string). Use for merges when present. |\n",
    "| `language` | `language_code` | Standardize to ISO 639-1 (lowercase). |\n",
    "| `description` | — | BBE-only; valuable for NLP features. |\n",
    "| `genres` | — | BBE-only; can enrich GB10k tags later. |\n",
    "| `characters` | — | bbe_clean-only; low modeling priority, but could add narrative metadata. |\n",
    "| `bookFormat` | — | BBE-only; possible categorical feature. |\n",
    "| `edition` | — | BBE-only. |\n",
    "| `pages` | — | BBE-only; numeric, may enrich GB10k metadata. |\n",
    "| `publisher` | — | bbe_clean_clean-only; possible future feature. |\n",
    "| `publishDate` | — | bbe_clean_clean-only; can approximate from GB10k’s `original_publication_year`. |\n",
    "| `firstPublishDate` | `original_publication_year` | Equivalent (date vs year). |\n",
    "| `coverImg` | `image_url` / `small_image_url` | Same function (cover link). |\n",
    "| `bbeScore` | — | BBE-only; internal popularity score. |\n",
    "| `bbeVotes` | `work_ratings_count` | Comparable as popularity proxy. |\n",
    "| `price` | — | BBE-only; likely non-essential for satisfaction prediction. |\n",
    "| `setting` | — | BBE-only; can support content enrichment. |\n",
    "| `awards` | — | BBE-only; categorical enrichment. |\n",
    "| — | `goodreads_book_id` / `best_book_id` / `work_id` | GB10k-only identifiers; may be used for deeper Goodreads linking. |\n",
    "| — | `books_count` | GB10k-only; number of editions per work. |\n",
    "| — | `work_text_reviews_count` | GB10k-only; can complement `numRatings` as engagement metric. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5069ac1",
   "metadata": {},
   "source": [
    "## Data Cleaning Steps\n",
    "\n",
    "### Best Books Ever\n",
    "\n",
    "- Handle identifier columns\n",
    "- Standardize key columns: `author`, `language`\n",
    "- Missing data handling strategies\n",
    "- Normalize genre and format\n",
    "- Validate for no nulls or duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e6ae8",
   "metadata": {},
   "source": [
    "#### 1. Handle identifier columns\n",
    "On the previous notebook, we created a new field `bookId_num` in the BBE dataset to align with `goodreads_book_id` in the Goodbooks10k dataset. We have also ensured that they were both converted to numeric types and that all `bookId` values generated a valid `bookId_num`. So we can skip the handle identifier columns, as it was already done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c6e5f",
   "metadata": {},
   "source": [
    "#### 2. Standardize key columns\n",
    "\n",
    "**Author**\n",
    "\n",
    "We will proceed with the standardization of key columns, starting with the `author` column. The author column in the BBE dataset often contains a qualifier such as \"(Goodreads Author)\". We will remove such qualifiers to standardize the format. We will also create an additional list column to store multiple authors as a list rather than a single string. This way, its is ready to use for feature engineering later on if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_split_authors(name):\n",
    "    \"\"\"\n",
    "    Cleans author names and returns a list of authors.\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "\n",
    "    # Remove role descriptors\n",
    "    cleaned = re.sub(r\"\\s*\\([^)]*\\)\", \"\", name)\n",
    "    \n",
    "    # Split into list if multiple authors exist\n",
    "    authors_list = [a.strip() for a in cleaned.split(\",\") if a.strip()]\n",
    "    \n",
    "    return authors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145894ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to BestBooksEver dataset\n",
    "bbe_clean[\"authors_list\"] = bbe_clean[\"author\"].apply(clean_and_split_authors)\n",
    "bbe_clean[\"author_clean\"] = bbe_clean[\"authors_list\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else None)\n",
    "\n",
    "# Quick check\n",
    "bbe_clean[[\"author\", \"author_clean\", \"authors_list\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 1\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim author datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a74c31",
   "metadata": {},
   "source": [
    "**Language**\n",
    "\n",
    "The `language` column in the Best Books Ever dataset used full names such as “English”, “German”, and “Arabic”.  Before transforming the values, we will check for all unique values to identify any unexpected entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique language values\n",
    "print(\"Unique language values in BBE dataset:\")\n",
    "bbe_clean['language'] = bbe_clean['language'].astype(str).str.strip()\n",
    "unique_languages = bbe_clean['language'].unique()\n",
    "\n",
    "print(f\"\\nTotal unique values: {len(unique_languages)}\\n\")\n",
    "print(unique_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d9853",
   "metadata": {},
   "source": [
    "We can see that there are some unexpected values such as:\n",
    "- _historical forms_ (“English, Middle (1100-1500)”, “French, Middle (ca.1400-1600)”)\n",
    "- _combined or semicolon-separated entries_ (“Filipino; Pilipino”, “Catalan; Valencian”)\n",
    "- _multi-language / uncertain cases_ (“Multiple languages”, “Undetermined”)\n",
    "- _rare or dialects_ (“Bokmål, Norwegian; Norwegian Bokmål”, “Aromanian; Arumanian; Macedo-Romanian”)\n",
    "\n",
    "We will clean the unusual entries by mapping them to the closest language present in the ISO 639-1 standard. Unrecognized values will be flagged and replaced with `\"unknown\"`. It was decided to distinguish the `\"unknown\"` from the `NaN` values to retain information about missingness versus unrecognized entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a83c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Standardize capitalization & spacing\n",
    "bbe_clean['language'] = bbe_clean['language'].astype(str).str.strip().str.title()\n",
    "\n",
    "# Handle NaNs that became strings\n",
    "bbe_clean['language'] = bbe_clean['language'].replace({'Nan': np.nan})\n",
    "\n",
    "# Simplify and unify multi-language / dialect forms\n",
    "replace_map = {\n",
    "    'Multiple Languages': 'Multilingual',\n",
    "    'Undetermined': 'Unknown',\n",
    "    'Iranian (Other)': 'Persian',\n",
    "    'Farsi': 'Persian',\n",
    "    'Filipino; Pilipino': 'Filipino',\n",
    "    'Catalan; Valencian': 'Catalan',\n",
    "    'Panjabi; Punjabi': 'Punjabi',\n",
    "    'Bokmål, Norwegian; Norwegian Bokmål': 'Norwegian',\n",
    "    'Norwegian Nynorsk; Nynorsk, Norwegian': 'Norwegian',\n",
    "    'Greek, Modern (1453-)': 'Greek',\n",
    "    'Greek, Ancient (To 1453)': 'Greek',\n",
    "    'French, Middle (Ca.1400-1600)': 'French',\n",
    "    'English, Middle (1100-1500)': 'English',\n",
    "    'Dutch, Middle (Ca.1050-1350)': 'Dutch',\n",
    "    'Aromanian; Arumanian; Macedo-Romanian': 'Romanian',\n",
    "    'Mayan Languages': 'Mayan',\n",
    "    'Australian Languages': 'English'\n",
    "}\n",
    "\n",
    "bbe_clean['language'] = bbe_clean['language'].replace(replace_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5060ad13",
   "metadata": {},
   "source": [
    "After transforming the values, we apply a mapping to standardize the `language` column using **ISO 639-1 two-letter codes**.\n",
    "The mapping dictionaries are stored in the `src/cleaning/mappings/` folder to keep the notebooks cleaner and improve readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37289380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"src/cleaning/mappings/languages_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    languages_dict = json.load(f)\n",
    "\n",
    "# Apply dictionary\n",
    "bbe_clean['language_clean'] = bbe_clean['language'].str.lower().map(languages_dict)\n",
    "\n",
    "# Fill remaining NaNs\n",
    "bbe_clean['language_clean'] = bbe_clean['language_clean'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again for unique language values\n",
    "print(\"Unique language values in BBE dataset:\")\n",
    "unique_languages = bbe_clean['language_clean'].unique()\n",
    "\n",
    "print(f\"\\nTotal unique values: {len(unique_languages)}\\n\")\n",
    "print(unique_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_breakdown = (\n",
    "    bbe_clean['language']\n",
    "    .value_counts()\n",
    "    .to_frame('count')\n",
    ")\n",
    "\n",
    "language_breakdown['percentage'] = (\n",
    "    language_breakdown['count'] / len(bbe_clean) * 100\n",
    ").round(2)\n",
    "\n",
    "print(language_breakdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 2\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim language datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6b8f9",
   "metadata": {},
   "source": [
    "**Dates**\n",
    "\n",
    "BBE dataset has two publication fields: `publishDate` and `firstPublishDate`. The `firstPublishDate` represents the original publication date, while `publishDate` refers to a more recent edition or reprint date. Publishing experts assumption is that the recency of the `firstPublishDate` is more relevant for modeling book satisfaction, as it reflects when the book was first introduced to readers. Therefore, we will focus on cleaning and standardizing the `firstPublishDate` column and use `publishDate` only if `firstPublishDate` is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2685c",
   "metadata": {},
   "source": [
    "While majority of the dates follow the 'MM/DD/YY' format, after a first attemp at cleaning, we noticed some dates do not conform to this format. Therefore, we will implement a more robust date parsing strategy, focusing first on transforming textual formats into 'MM/DD/YYYY' format before attempting to parse them into datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6d327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "def clean_date_string(date_str):\n",
    "    \"\"\"Remove ordinal suffixes and unwanted characters from a date string.\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return np.nan\n",
    "    # remove st, nd, rd, th (like 'April 27th 2010' → 'April 27 2010')\n",
    "    cleaned = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', str(date_str))\n",
    "    return cleaned.strip()\n",
    "\n",
    "def parse_mixed_date(date_str):\n",
    "    \"\"\"Try to parse a variety of date formats safely.\"\"\"\n",
    "    if pd.isna(date_str) or date_str == '':\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Use dateutil to parse most human-readable formats\n",
    "        return parser.parse(date_str, fuzzy=True)\n",
    "    except Exception:\n",
    "        # Try year-only fallback (e.g. '2003')\n",
    "        match = re.match(r'^\\d{4}$', str(date_str))\n",
    "        if match:\n",
    "            return pd.to_datetime(f\"{date_str}-01-01\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7cc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to both columns\n",
    "for col in ['firstPublishDate', 'publishDate']:\n",
    "       bbe_clean[f'{col}_clean'] = (\n",
    "        bbe_clean[col]\n",
    "        .astype(str)\n",
    "        .replace({'nan': np.nan, '': np.nan})\n",
    "        .apply(clean_date_string)\n",
    "        .apply(parse_mixed_date)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0493da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine using your logic: prefer firstPublishDate, else publishDate\n",
    "bbe_clean['publication_date_clean'] = (\n",
    "    bbe_clean['firstPublishDate_clean'].combine_first(bbe_clean['publishDate_clean'])\n",
    ")\n",
    "# Reconvert to datetime safely before using .dt\n",
    "bbe_clean['publication_date_clean'] = pd.to_datetime(bbe_clean['publication_date_clean'], errors='coerce')\n",
    "\n",
    "# Format as ISO standard\n",
    "bbe_clean['publication_date_clean'] = bbe_clean['publication_date_clean'].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Check a sample of remaining nulls\n",
    "bbe_clean[bbe_clean['publication_date_clean'].isna()][['title', 'firstPublishDate', 'publishDate', 'publication_date_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the unified publication date is missing\n",
    "total = len(bbe_clean)\n",
    "bbe_missing_dates = bbe_clean.loc[bbe_clean['publication_date_clean'].isna()]\n",
    "missing_count = len(bbe_missing_dates)\n",
    "\n",
    "print(f\"Missing publication dates: {missing_count} of {total} ({missing_count/total:.2%})\")\n",
    "\n",
    "# Preview key columns\n",
    "bbe_missing_dates[['title', 'author', 'firstPublishDate', 'publishDate', 'publication_date_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 3\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim dates datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f7a9c",
   "metadata": {},
   "source": [
    "**Publisher**\n",
    "\n",
    "Publisher names can vary significantly in formatting, including differences in capitalization, punctuation, and spacing. To standardize the `publisher` column, we will convert all entries to lowercase and strip any leading or trailing whitespace. This will help reduce variability and improve consistency across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample publishers:\")\n",
    "print(bbe_clean['publisher'].drop_duplicates().sample(30, random_state=42).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Strip, lowercase, remove extra spaces and punctuation\n",
    "bbe_clean['publisher'] = (\n",
    "    bbe_clean['publisher']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace('\"', '', regex=False)\n",
    "    .str.replace(\"'\", '', regex=False)\n",
    "    .str.replace(r'[.,]', '', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e51a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique publisher values \n",
    "bbe_clean['publisher'] = bbe_clean['publisher'].astype(str).str.strip() \n",
    "unique_publisher = bbe_clean['publisher'].unique() \n",
    "\n",
    "print(f\"\\nTotal unique publisher values: {len(unique_publisher)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize numeric publishers names:\n",
    "def clean_numeric_publishers(x):\n",
    "    if re.match(r'^\\d+$', x.strip()):\n",
    "        return 'unknown'\n",
    "    return x\n",
    "\n",
    "bbe_clean['publisher'] = bbe_clean['publisher'].apply(clean_numeric_publishers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34928226",
   "metadata": {},
   "source": [
    "This cleaning step reduced the number of unique publisher names from **11,111 to 10,764**.\n",
    "Since **English-language books represent 81% of the catalogue**, the analysis will focus on this segment.\n",
    "We will **standardize major English-language publishing groups**, consolidating their **imprints and subsidiaries**, and apply **fuzzy matching** to unify names with **minor variations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load publishers dictionary\n",
    "with open(\"src/cleaning/mappings/publishers_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    publishers_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83945720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Get top 10000 most common publishers\n",
    "top_n = 10000\n",
    "publisher_counts = bbe_clean['publisher'].value_counts()\n",
    "top_publishers = publisher_counts.head(top_n).index.tolist()\n",
    "\n",
    "# Create a mapping for top publishers only\n",
    "standardization_map = {}\n",
    "processed = set()\n",
    "\n",
    "for pub in top_publishers:\n",
    "    if pub in processed:\n",
    "        continue\n",
    "    \n",
    "    # Find similar publishers in the top list\n",
    "    matches = process.extract(pub, top_publishers, scorer=fuzz.ratio, limit=5)\n",
    "    \n",
    "    # Group similar ones (score > 90)\n",
    "    similar = [m[0] for m in matches if m[1] > 90]\n",
    "    canonical = similar[0]  # Use first as canonical\n",
    "    \n",
    "    for similar_pub in similar:\n",
    "        standardization_map[similar_pub] = canonical\n",
    "        processed.add(similar_pub)\n",
    "\n",
    "# Apply the mapping\n",
    "bbe_clean['publisher_standardized'] = bbe_clean['publisher'].replace(standardization_map)\n",
    "\n",
    "# Then apply manual mapping\n",
    "bbe_clean['publisher_standardized'] = bbe_clean['publisher_standardized'].replace(publishers_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c7a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_unique_publisher = bbe_clean['publisher_standardized'].unique() \n",
    "\n",
    "print(f\"\\nTotal unique publisher values: {len(standardized_unique_publisher)}\\n\") \n",
    "\n",
    "print(\"Sample publishers:\")\n",
    "print(bbe_clean['publisher_standardized'].drop_duplicates().sample(30, random_state=42).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ec14d",
   "metadata": {},
   "source": [
    "The cleaning process reduced the number of unique publisher names from **11,111 to 9993**, representing a **10% decrease**.\n",
    "Given that the dataset includes books in multiple languages and many small or independent publishers, this reduction is a **satisfactory outcome**.\n",
    "\n",
    "To further evaluate the effectiveness of the cleaning, we will analyze the **proportion of titles associated with the most common publishers**.\n",
    "This will help us assess how well the standardization process **consolidated the publisher catalog** and captured the main publishing groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b78810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your core publisher groups\n",
    "major_publishers = [\n",
    "    'penguin random house', 'harpercollins', 'macmillan',\n",
    "    'simon & schuster', 'hachette', 'bloomsbury',\n",
    "    'amazon publishing', 'scholastic'\n",
    "]\n",
    "\n",
    "# Create a flag\n",
    "bbe_clean['is_major_publisher'] = bbe_clean['publisher_standardized'].isin(major_publishers)\n",
    "\n",
    "# Count results\n",
    "total_books = len(bbe_clean)\n",
    "major_books = bbe_clean['is_major_publisher'].sum()\n",
    "share_major = major_books / total_books * 100\n",
    "\n",
    "print(f\"Books from mapped major publishers: {major_books} of {total_books} ({share_major:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0672c79",
   "metadata": {},
   "source": [
    "About 17% of all titles now belong to one of the standardized major publisher groups.\n",
    "The remaining publishers represent independent, regional, or self-published works.\n",
    "Further improvements (e.g., mapping academic and international publishers) could expand this coverage to 25–30%. But we'll leave it as is for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 4\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim publisher datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd851a",
   "metadata": {},
   "source": [
    "**Book Format**\n",
    "\n",
    "This step standardizes the `bookFormat` field across multiple languages and inconsistent label variations found in the dataset.  \n",
    "The goal is to translate all format names into English and consolidate equivalent values (e.g., *“Capa dura”*, *“Gebundene Ausgabe”*, *“Hard back”*) under unified categories such as **Hardcover**, **Paperback**, **Ebook**, and **Audiobook**.\n",
    "\n",
    "This cleaning ensures that:\n",
    "- Format values are consistent for analysis and visualization.  \n",
    "- Non-English or rare variants are translated and grouped appropriately.  \n",
    "- Missing or unrecognized entries are handled under a neutral category: **Other / Unknown**.  \n",
    "\n",
    "By applying a mapping dictionary, we make the variable suitable for aggregation, comparison, and predictive modeling. After transformation, we verify the result by inspecting the number of unique standardized values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique format values \n",
    "bbe_clean['bookFormat'] = bbe_clean['bookFormat'].astype(str).str.strip() \n",
    "unique_format = bbe_clean['bookFormat'].unique() \n",
    "\n",
    "print(f\"\\nTotal unique book format values: {len(unique_format)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load format dictionary\n",
    "with open(\"src/cleaning/mappings/format_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    format_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631890b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['bookFormat_clean'] = (\n",
    "    bbe_clean['bookFormat']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .replace(format_dict)\n",
    ")\n",
    "\n",
    "# Replace remaining unknowns or NaN with a unified label\n",
    "bbe_clean['bookFormat_clean'] = bbe_clean['bookFormat_clean'].replace(['nan', 'none', ''], np.nan)\n",
    "bbe_clean['bookFormat_clean'] = bbe_clean['bookFormat_clean'].fillna('Other / Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c61a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_format_clean = bbe_clean['bookFormat_clean'].unique() \n",
    "\n",
    "print(f\"\\nTotal unique book format values: {len(unique_format_clean)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3adabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_format_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 5\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim format datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a412a95",
   "metadata": {},
   "source": [
    "After applying the standardization mapping, the number of unique book format values was reduced from **135** to **10**.  This represents a substantial improvement in data consistency and interpretability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b535cfd",
   "metadata": {},
   "source": [
    "**ISBN and ASIN Cleaning**\n",
    "\n",
    "The BBE dataset includes a single `isbn` column, which initially contained numerous missing or invalid entries (e.g. placeholder values such as `9999999999999`).\n",
    "\n",
    "Our initial cleaning flow focused solely on standardizing **ISBN** values, but upon further inspection, we identified additional patterns such as **Amazon ASINs** (10-character alphanumeric codes) and prefixed identifiers like `10:` or `13:`.\n",
    "\n",
    "These findings led to an adjustment to the cleaning logic and the order of operations in the pipeline.\n",
    "\n",
    "The final cleaning process:\n",
    "\n",
    "- Removes punctuation and non-digit characters to standardize ISBN formatting.\n",
    "- Detects and separates ASINs (`asin` column) to preserve them for potential cross-dataset enrichment.\n",
    "- Handles prefixed identifiers (e.g., `13:9780615700`) by removing prefixes before validation.\n",
    "- Filters out placeholder or invalid entries (`999…`, `000…`) and ensures consistent string representation.\n",
    "- Creates a new `isbn_clean` column containing only valid ISBN-10 or ISBN-13 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36afe4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect ISBN column\n",
    "bbe_clean[['title','isbn']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing and invalid patterns\n",
    "n_missing_isbn = bbe_clean['isbn'].isna().sum()\n",
    "print(f'Number of missing ISBN entries: {n_missing_isbn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify invalid placeholders (like 9999999999999)\n",
    "n_invalid_isbn = bbe_clean[bbe_clean['isbn'].astype(str).str.contains('9999999999')].shape[0]\n",
    "print(f'Number of placeholder ISBN entries: {n_invalid_isbn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_asin(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    x = str(x).strip()\n",
    "    if re.fullmatch(r'[A-Z0-9]{10}', x) and not x.isdigit():  # must have at least one letter\n",
    "        return x\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e2970",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['asin'] = bbe_clean['isbn'].apply(detect_asin)\n",
    "has_asin = bbe_clean[bbe_clean['asin'].notna()] \n",
    "print(f'Books with ASINs: {len(has_asin)}')\n",
    "has_asin[['title','isbn', 'asin']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fabfea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_isbn(x):\n",
    "    # handle missing\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "\n",
    "    # detect ASIN first\n",
    "    asin_val = detect_asin(x)\n",
    "    if pd.notna(asin_val):\n",
    "        # return NaN for ISBN cleaning, because it's an ASIN\n",
    "        return np.nan  \n",
    "\n",
    "    # clean numeric ISBNs\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r'^(10:|13:)', '', s)       # remove leading prefixes\n",
    "    s = re.sub(r'\\D', '', s)               # keep only digits\n",
    "\n",
    "    # handle placeholders\n",
    "    if re.fullmatch(r'(9{10}|9{13}|0{10}|0{13})', s):\n",
    "        return np.nan\n",
    "\n",
    "    # keep valid ISBN-10 or ISBN-13\n",
    "    if len(s) in [10, 13]:\n",
    "        return s\n",
    "\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc1c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['isbn_clean'] = bbe_clean['isbn'].apply(clean_isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b056cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect ISBN columns after cleaning\n",
    "bbe_clean[['title','isbn', 'isbn_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9baf8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_remaining = bbe_clean[bbe_clean['isbn_clean'].astype(str).str.fullmatch(r'(9{10}|9{13}|0{10}|0{13})', na=False)]\n",
    "print(f\"Remaining placeholder ISBNs: {len(placeholder_remaining)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where isbn_clean is NaN\n",
    "missing_isbn_clean = bbe_clean[bbe_clean['isbn_clean'].isna()]\n",
    "\n",
    "# Print the number of missing and show the first few examples\n",
    "print(f\"Missing isbn_clean: {missing_isbn_clean.shape[0]}\")\n",
    "missing_isbn_clean[['title', 'bookFormat', 'isbn', 'asin','isbn_clean']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31f098",
   "metadata": {},
   "source": [
    "To inspect if there are other cases of invalid ISBNs, we will filter the rows where the `isbn_type` is either `'wrong_length'` or `'missing'`. This will help us identify any additional issues with the ISBN data that may need to be addressed. For that a custom function `isbn_type` was created to classify the reason for invalidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isbn_type(x):\n",
    "    if pd.isna(x):\n",
    "        return 'missing'\n",
    "\n",
    "    s = str(x).strip()\n",
    "\n",
    "    # Detect ASIN (10-char alphanumeric, must have at least one letter)\n",
    "    if re.fullmatch(r'[A-Z0-9]{10}', s.upper()) and not s.isdigit():\n",
    "        return 'asin'\n",
    "\n",
    "    # Remove non-digits for numeric checks\n",
    "    x = re.sub(r'\\D', '', s)\n",
    "\n",
    "    # Placeholder patterns\n",
    "    if re.fullmatch(r'9{10}|9{13}', x):\n",
    "        return 'placeholder_9'\n",
    "    if re.fullmatch(r'0{10}|0{13}', x):\n",
    "        return 'placeholder_0'\n",
    "\n",
    "    # Length checks\n",
    "    if len(x) in [10, 13]:\n",
    "        return 'valid'\n",
    "    if len(x) > 0:\n",
    "        return 'wrong_length'\n",
    "\n",
    "    return 'missing'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeec353",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['isbn_type'] = bbe_clean['isbn'].apply(isbn_type)\n",
    "bbe_clean['isbn_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with type either 'wrong_length' or 'missing'\n",
    "invalid_isbn = bbe_clean[bbe_clean['isbn_type'].isin(['wrong_length', 'missing'])]\n",
    "\n",
    "# Show total count\n",
    "print(f\"Total invalid (wrong_length + missing): {invalid_isbn.shape[0]}\")\n",
    "\n",
    "# Preview relevant columns\n",
    "invalid_isbn[['title', 'author_clean', 'bookFormat', 'isbn', 'asin', 'isbn_type']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676aef5e",
   "metadata": {},
   "source": [
    "Out of all records, **9,081 entries (≈18%)** were identified as invalid ISBNs, leaving roughly **82%** valid.\n",
    "Only **34** cases were tagged as `'wrong_length'` and **1** as `'missing'`.\n",
    "These mostly represent truncated or prefixed identifiers, while the `isbn_type` function accurately distinguished valid ISBNs, ASINs, and placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 6\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim ISBN/ASIN datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e35f4",
   "metadata": {},
   "source": [
    "**Ratings**\n",
    "\n",
    "In this step, we will first evaluate the quality and consistency of the `rating` field.\n",
    "We first check for missing or invalid values and calculate the percentage of available ratings to assess data completeness. Then, we use the `describe()` method to verify whether the ratings follow the expected 1–5 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7651de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where rating is not NaN\n",
    "total_books = len(bbe_clean)\n",
    "has_ratings = bbe_clean[bbe_clean['rating'].notna()]\n",
    "has_ratings_num = has_ratings.shape[0]\n",
    "share_ratings = has_ratings_num / total_books * 100\n",
    "\n",
    "# Print the number of titles with ratings and show the first few examples\n",
    "print(f\"Books with ratings: {has_ratings_num} of {total_books} ({share_ratings:.2f}%)\")\n",
    "has_ratings[['title', 'rating', 'numRatings','ratingsByStars']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['rating'].unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bff8e3",
   "metadata": {},
   "source": [
    "The inspection confirms that the dataset is generally clean; however, a small number of entries have a value of `0`, which represents missing evaluations. These will be replaced with `NaN` to ensure the ratings remain within the valid range (1–5). Since all valid values already follow the standard Goodreads scale, no normalization is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f90c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (bbe_clean['rating'] == 0)\n",
    "print(f'Items with value equal 0: {bbe_clean[mask].shape[0]}')\n",
    "bbe_clean[mask][['title', 'author_clean','rating','numRatings','ratingsByStars']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['rating_clean'] = bbe_clean['rating'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332efe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['rating_clean'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3563f",
   "metadata": {},
   "source": [
    "**NumRating**\n",
    "\n",
    "Next we will handle `numRatings`. The `numRatings` feature represents the total count of user ratings per book. We seen know from the mask we created that where `ratings` equals `0`, `numRatings` tends to be `0` too. We will check if that is always the case by checking for invalid values and using the `.describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4254c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_numRatings = bbe_clean[bbe_clean['numRatings'].isna()]\n",
    "na_numRatings_num = na_numRatings.shape[0]\n",
    "share_numRatings = na_numRatings_num / total_books * 100\n",
    "\n",
    "# Print the number of titles with ratings and show the first few examples\n",
    "print(f\"Books with no numRatings values: {na_numRatings_num} of {total_books} ({share_numRatings}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c234c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['numRatings'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bc87b",
   "metadata": {},
   "source": [
    "Since it’s a valid count metric (0 values indicate unrated books), no replacement with NaN is required. However, because most books have relatively few ratings while a few very popular titles have millions, the distribution is heavily right-skewed. To better visualize and later analyze relationships with other variables, we apply a logarithmic transformation (`log1p`) to smooth out the long tail and reveal underlying patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(np.log1p(bbe_clean['numRatings']), bins=50)\n",
    "plt.title(\"Distribution of Log(Number of Ratings)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74e109",
   "metadata": {},
   "source": [
    "The log transformation reveals a near-normal distribution centered around books with moderate popularity.\n",
    "This confirms that `numRatings` is a valid and informative feature, and no normalization or imputation is needed at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the count to reduce skew\n",
    "bbe_clean['numRatings_log'] = np.log1p(bbe_clean['numRatings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee88d05",
   "metadata": {},
   "source": [
    "**ratingsByStars**\n",
    "\n",
    "In this step, we examine how complete the `ratingsByStars` field is across all books.  \n",
    "This feature represents the 1–5 star breakdown of user ratings and is essential for modelling engagement quality and satisfaction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e520ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_ratings_stars = bbe_clean[bbe_clean['ratingsByStars'].isna()]\n",
    "na_ratings_stars_num = na_ratings_stars.shape[0]\n",
    "share_na_ratings_stars = na_ratings_stars_num / total_books * 100\n",
    "\n",
    "# Print the number of titles with ratingsByStars and show the first few examples\n",
    "print(f\"Books with ratingsByStars: {na_ratings_stars_num} of {total_books} ({share_na_ratings_stars}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff53e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['ratingsByStars'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_ratings_stars_mask = (bbe_clean['ratingsByStars'] == '[]')\n",
    "empty_ratings_stars = bbe_clean[empty_ratings_stars_mask].shape[0]\n",
    "print(f'Items with empty values: {empty_ratings_stars}')\n",
    "bbe_clean[empty_ratings_stars_mask][['title', 'author_clean','rating','numRatings','ratingsByStars']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fbbc6",
   "metadata": {},
   "source": [
    "By quantifying missing or empty values, we identify potential inconsistencies between overall ratings (`rating`, `numRatings`) and their detailed distribution.\n",
    "After counting missing and empty entries, we compare these against books that *do* have `rating` and `numRatings` values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d49164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask for books with ratings\n",
    "mask_has_ratings = (bbe_clean['numRatings'] > 0) & (bbe_clean['rating'] > 0)\n",
    "# mask for books without rating distributions\n",
    "mask_no_distribution = (bbe_clean['ratingsByStars'] == '[]')\n",
    "# combine masks: have ratings but no distribution:\n",
    "mask_rated_no_distribution = mask_has_ratings & mask_no_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a51959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and inspect missing star distributions among rated books\n",
    "\n",
    "# count how many books have ratings but no ratingsByStars distribution\n",
    "num_missing_dist = mask_rated_no_distribution.sum()\n",
    "share_missing_dist = num_missing_dist / len(bbe_clean) * 100\n",
    "\n",
    "# count total books with empty or missing distributions (regardless of ratings)\n",
    "total_empty_dist = empty_ratings_stars_mask.sum()\n",
    "\n",
    "# compute what share of those empty distributions actually have valid ratings\n",
    "share_with_ratings = (num_missing_dist / total_empty_dist) * 100\n",
    "share_without_ratings = 100 - share_with_ratings\n",
    "\n",
    "# print results\n",
    "print(f\"Total books: {len(bbe_clean):,}\")\n",
    "print(f\"Books with ratings but missing distribution: {num_missing_dist:,} ({share_missing_dist:.2f}%)\")\n",
    "print(f\"  ↳ Of all empty distributions ({total_empty_dist:,} total):\")\n",
    "print(f\"      • With ratings: {share_with_ratings:.2f}%\")\n",
    "print(f\"      • Without ratings: {share_without_ratings:.2f}%\")\n",
    "\n",
    "# inspect a few examples\n",
    "bbe_clean.loc[mask_rated_no_distribution, ['title', 'author_clean', 'rating', 'numRatings', 'ratingsByStars']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb30c75",
   "metadata": {},
   "source": [
    "This highlights a critical gap: books with ratings but without a distribution breakdown.  \n",
    "\n",
    "Such gaps likely stem from export limitations or missing historical data from Goodreads, and must be addressed before feature engineering or predictive tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5d1fe",
   "metadata": {},
   "source": [
    "To handle missing `ratingsByStars` while preserving analytical completeness, we implement a probabilistic estimation function.  \n",
    "The approach assumes a normal distribution around the book’s average rating, proportionally allocating counts across 1–5 stars.  \n",
    "\n",
    "This preserves both the **total number of ratings** and the **shape of expected user sentiment**, ensuring downstream models can use these reconstructed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb063abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def estimate_star_distribution(avg_rating, num_ratings):\n",
    "    # define 1–5 star levels\n",
    "    stars = np.arange(1, 6)\n",
    "\n",
    "    # normal distribution around avg_rating\n",
    "    # - (stars - avg_rating): distance of each star value from the mean\n",
    "    # - **2: squares the distance to emphasize larger deviations\n",
    "    # - -0.5 * (...): converts distance into a negative exponent (closer = less negative)\n",
    "    # - np.exp(...): applies exponential decay, giving higher weights to values near the mean\n",
    "    # - 0.5 controls the curve’s spread (smaller = narrower, larger = wider)\n",
    "    weights = np.exp(-0.5 * ((stars - avg_rating) ** 2) / 0.5**2)\n",
    "    weights /= weights.sum()  # normalize to 1\n",
    "    \n",
    "    # Scale to total ratings\n",
    "    estimated_counts = np.round(weights * num_ratings).astype(int)\n",
    "\n",
    "    # Adjust rounding error so sum matches exactly\n",
    "    diff = num_ratings - estimated_counts.sum()\n",
    "    estimated_counts[np.argmax(weights)] += diff\n",
    "\n",
    "    return estimated_counts.tolist()\n",
    "\n",
    "# code inspiration: \n",
    "# https://www.geeksforgeeks.org/machine-learning/gaussian-distribution-in-machine-learning/\n",
    "# https://www.geeksforgeeks.org/python/python-normal-distribution-in-statistics/\n",
    "# https://www.geeksforgeeks.org/numpy/binning-data-in-python-with-scipy-numpy/\n",
    "# https://blog.quantinsti.com/gaussian-distribution/\n",
    "# https://www.freecodecamp.org/news/how-to-explain-data-using-gaussian-distribution-and-summary-statistics-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e24a4",
   "metadata": {},
   "source": [
    "The function is applied to all titles with valid ratings but missing distributions.  \n",
    "We then validate that each reconstructed list of star counts sums to its corresponding `numRatings`, ensuring internal consistency.  \n",
    "A high proportion of valid totals indicates that the imputation strategy worked as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['ratingsByStars_clean'] = bbe_clean['ratingsByStars']\n",
    "# For books missing star distributions but with valid ratings,\n",
    "# estimate a plausible 1–5 star breakdown using avg_rating and numRatings,\n",
    "# and store the result in 'ratingsByStars_clean'.\n",
    "bbe_clean.loc[mask_rated_no_distribution, 'ratingsByStars_clean'] = (\n",
    "    bbe_clean.loc[mask_rated_no_distribution]\n",
    "    .apply(lambda x: estimate_star_distribution(x['rating'], int(x['numRatings'])), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check how many were filled\n",
    "filled_count = bbe_clean.loc[mask_rated_no_distribution, 'ratingsByStars_clean'].notna().sum()\n",
    "print(f\"Filled distributions: {filled_count:,} (of {mask_rated_no_distribution.sum():,} missing)\")\n",
    "\n",
    "# preview examples\n",
    "print(\"\\nSample of estimated distributions:\")\n",
    "display(\n",
    "    bbe_clean.loc[mask_rated_no_distribution, \n",
    "                  ['title', 'author_clean', 'rating', 'numRatings', 'ratingsByStars_clean']\n",
    "                 ].head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5582ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that each estimated list sums correctly\n",
    "check_sum = bbe_clean.loc[mask_rated_no_distribution].apply(\n",
    "    lambda x: sum(x['ratingsByStars_clean']) == int(x['numRatings']), axis=1\n",
    ")\n",
    "valid_share = check_sum.mean() * 100\n",
    "print(f\"\\nDistributions matching numRatings total: {valid_share:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3658b5ae",
   "metadata": {},
   "source": [
    "We extend the validation to the full dataset, verifying that all `ratingsByStars_clean` entries, both original and estimated, correctly sum to `numRatings`. This serves as a final data integrity checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def safe_sum_ratings(row):\n",
    "    val = row['ratingsByStars_clean']\n",
    "    # Convert stringified lists into Python lists\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            val = ast.literal_eval(val)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    # If it's a list, make sure elements are integers\n",
    "    if isinstance(val, list):\n",
    "        try:\n",
    "            val = [int(v) for v in val]  # convert each element to int\n",
    "            return sum(val) == int(row['numRatings'])\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# https://dev.to/mstuttgart/using-literal-eval-for-string-to-object-conversion-in-python-46i\n",
    "# https://www.educative.io/answers/what-is-astliteralevalnodeorstring-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sum_all = bbe_clean.loc[bbe_clean['ratingsByStars_clean'].notna()].apply(safe_sum_ratings, axis=1)\n",
    "valid_share_all = check_sum_all.mean() * 100\n",
    "\n",
    "print(f\"All distributions matching numRatings total: {valid_share_all:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b43433",
   "metadata": {},
   "source": [
    "As a last step, we replace remaining `'[]'` values using `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d36f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_empty_ratings_stars_mask = (bbe_clean['ratingsByStars_clean'] == '[]')\n",
    "remaining_empty_ratings_stars = bbe_clean[remaining_empty_ratings_stars_mask].shape[0]\n",
    "print(f'Remaining empty values: {remaining_empty_ratings_stars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6696a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['ratingsByStars_clean'] = bbe_clean['ratingsByStars_clean'].replace('[]', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 7\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim ratings datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4546c",
   "metadata": {},
   "source": [
    "**Genres**\n",
    "\n",
    "In memory-based recommender systems, categorical attributes such as genre serve as key features for similarity computation. In this step, we first identify and handle missing values, then parse the genre lists using `ast.literal_eval` to ensure proper data structure representation, and finally quantify the unique genres and analyze their distribution across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd91499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and handle missing genre values\n",
    "empty_genres_mask = (bbe_clean['genres'] == '[]')\n",
    "empty_genres = bbe_clean[empty_genres_mask].shape[0]\n",
    "share_missing_genres = (empty_genres / len(bbe_clean)) * 100\n",
    "print(f'Books with empty genre values: {empty_genres} ({share_missing_genres:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de50adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# GENERIC PARSING AND CLEANING FUNCTIONS\n",
    "\n",
    "def parse_list_field(val):\n",
    "    \"\"\"\n",
    "    Safely parse a stringified list (e.g. '[\"x\", \"y\"]') into a Python list.\n",
    "    Returns np.nan for missing, invalid, or empty values.\n",
    "    \"\"\"\n",
    "    if pd.isna(val) or val in ['[]', '', None]:\n",
    "        return np.nan\n",
    "    try:\n",
    "        parsed = ast.literal_eval(val)\n",
    "        if isinstance(parsed, list) and len(parsed) > 0:\n",
    "            return parsed\n",
    "        else:\n",
    "            return np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def clean_text_item(text, keep_pattern=r'[^a-z0-9\\s-]'):\n",
    "    \"\"\"\n",
    "    Lowercase and remove noise, keeping only letters, digits, hyphens and spaces.\n",
    "    Can be reused for genres, awards, etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(keep_pattern, '', text)  # clean unwanted chars\n",
    "    text = re.sub(r'\\s+', ' ', text)       # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_list_field(lst, keep_pattern=r'[^a-z0-9\\s-]'):\n",
    "    \"\"\"\n",
    "    Clean and deduplicate elements from a list of strings.\n",
    "    Returns np.nan for invalid or empty lists.\n",
    "    \"\"\"\n",
    "    if not isinstance(lst, list):\n",
    "        return np.nan\n",
    "    cleaned = [clean_text_item(item, keep_pattern) for item in lst if isinstance(item, str) and item.strip()]\n",
    "    cleaned = [c for c in cleaned if c]\n",
    "    return list(set(cleaned)) if cleaned else np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['genres_parsed'] = bbe_clean['genres'].apply(parse_list_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['genres_parsed'] = bbe_clean['genres_parsed'].apply(clean_list_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(bbe_clean['genres_parsed'].apply(type).value_counts())\n",
    "print(bbe_clean['genres_parsed'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_nan_genres = bbe_clean['genres_parsed'].apply(lambda x: isinstance(x, float))\n",
    "bbe_clean[mask_nan_genres][['title', 'author_clean', 'genres', 'genres_parsed']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9370b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusable dictionary counts\n",
    "def count_unique_items(df, column):\n",
    "    \"\"\"\n",
    "    Count the frequency of each unique element in a list-type column.\n",
    "    Returns a dictionary {item: count}.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for lst in df[column].dropna():\n",
    "        if isinstance(lst, list):\n",
    "            for item in lst:\n",
    "                counts[item] = counts.get(item, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres_dict = count_unique_items(bbe_clean, 'genres_parsed')\n",
    "unique_genres_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33fa8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# convert to Series for easy analysis\n",
    "genre_counts = pd.Series(unique_genres_dict).sort_values(ascending=False)\n",
    "top_n = 30\n",
    "\n",
    "#plot top N genres\n",
    "top_n = 30\n",
    "plt.figure(figsize=(10,6))\n",
    "genre_counts.head(top_n).plot(kind='bar', color='slateblue')\n",
    "plt.title(f\"Top {top_n} Genres by Frequency\")\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.ylabel(\"Book Count\")\n",
    "plt.xticks(rotation=75)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7ca10",
   "metadata": {},
   "source": [
    "The genre frequency plot reveals a highly skewed distribution typical of book markets, dominated by broad categories like _Fiction_ and _Romance_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['title','author_clean', 'genres_parsed', 'genres']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0393517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate genre-level and book-level coverage\n",
    "total_genres = genre_counts.sum()\n",
    "top10 = set(genre_counts.head(10).index)\n",
    "\n",
    "top10_share = (genre_counts.head(10).sum() / total_genres) * 100\n",
    "mask_top10 = bbe_clean['genres_parsed'].apply(\n",
    "    lambda lst: any(g in top10 for g in lst) if isinstance(lst, list) else False\n",
    ")\n",
    "book_share_top10 = (mask_top10.sum() / len(bbe_clean)) * 100\n",
    "\n",
    "print(f\"Top 10 genres account for {top10_share:.2f}% of all genre occurrences.\")\n",
    "print(f\"Books with at least one of the top 10 genres: {book_share_top10:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124fe4e2",
   "metadata": {},
   "source": [
    "Broad, mainstream genres dominate both in tag volume and book coverage. Nearly one-third of all genre tags (**30.47%**) in the dataset come from the same 10 genres, showing strong catalog concentration. Nearly nine out of ten books (**86.54%**) fall within those top categories, confirming their dominance at the book level. This distribution supports the design of **segment-based recommendation strategies** for mainstream readers while maintaining a “long-tail” of niche genres to personalize discovery.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33edb942",
   "metadata": {},
   "source": [
    "Since genre tags are extremely long-tail: a few popular genres dominate while hundreds of niche labels appear rarely. Collapsing the tail into a single bucket (_other_) reduces feature sparsity, speeds up modeling, and keeps the vectors interpretable for the dashboard. This aligns with CRISP-DM Data Preparation and the assessment’s requirement to collect, arrange, and process data before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0826e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplification rules\n",
    "TOP_K = 100          # keep the 100 most frequent genres\n",
    "TAIL_LABEL = 'other' # name of the long-tail bucket\n",
    "FILL_MISSING = True  # set to False if you prefer to leave NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a58833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "valid_lists = bbe_clean['genres_parsed'].dropna()\n",
    "genre_counts = Counter(g for lst in valid_lists for g in lst)\n",
    "genre_counts = pd.Series(genre_counts).sort_values(ascending=False)\n",
    "top_genres = set(genre_counts.head(TOP_K).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc80f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_genre_list(lst, *, keep=top_genres, tail=TAIL_LABEL):\n",
    "    if not isinstance(lst, list):  # NaN / missing\n",
    "        return ['unknown'] if FILL_MISSING else pd.NA\n",
    "    kept = [g if g in keep else tail for g in lst]\n",
    "    # dedupe while preserving order\n",
    "    seen = set()\n",
    "    simplified = [x for x in kept if not (x in seen or seen.add(x))]\n",
    "    # if everything was mapped to tail and list became ['other'] it's fine; if it became empty, fill fallback\n",
    "    return simplified or (['unknown'] if FILL_MISSING else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf59c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['genres_simplified'] = bbe_clean['genres_parsed'].apply(simplify_genre_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee35f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of rows simplified successfully\n",
    "ok_share = bbe_clean['genres_simplified'].notna().mean() * 100\n",
    "print(f\"Simplified rows available: {ok_share:.2f}%\")\n",
    "\n",
    "# How many rows include the tail label\n",
    "has_tail = bbe_clean['genres_simplified'].apply(lambda lst: isinstance(lst, list) and TAIL_LABEL in lst).sum()\n",
    "print(f\"Rows containing '{TAIL_LABEL}': {has_tail}\")\n",
    "\n",
    "# Coverage of the head vs tail (by occurrences)\n",
    "from collections import Counter\n",
    "simp_counts = Counter(g for lst in bbe_clean['genres_simplified'].dropna() for g in lst)\n",
    "head_occ = sum(simp_counts[g] for g in simp_counts if g in top_genres)\n",
    "tail_occ = simp_counts.get(TAIL_LABEL, 0)\n",
    "total_occ = head_occ + tail_occ\n",
    "print(f\"Head coverage: {head_occ/total_occ*100:.2f}% | Tail coverage: {tail_occ/total_occ*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(bbe_clean['genres_simplified'].apply(type).value_counts())\n",
    "print(bbe_clean['genres_simplified'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def genre_completeness_report(df, columns):\n",
    "    \"\"\"\n",
    "    Check completeness (non-empty lists) for given genre columns.\n",
    "    Returns a DataFrame with counts and percentages.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    summary = []\n",
    "    \n",
    "    for col in columns:\n",
    "        mask_valid = df[col].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "        valid = mask_valid.sum()\n",
    "        missing = total - valid\n",
    "        summary.append({\n",
    "            \"column\": col,\n",
    "            \"total_books\": total,\n",
    "            \"valid_genres\": valid,\n",
    "            \"missing_genres\": missing,\n",
    "            \"valid_%\": round((valid / total) * 100, 2),\n",
    "            \"missing_%\": round((missing / total) * 100, 2)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "# completeness check\n",
    "completeness = genre_completeness_report(\n",
    "    bbe_clean, \n",
    "    ['genres_parsed', 'genres_simplified']\n",
    ")\n",
    "print(\"Genre Completeness Summary\\n\")\n",
    "display(completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 8\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim genres datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a2887",
   "metadata": {},
   "source": [
    "**Awards**\n",
    "\n",
    "In this step, we will reuse some of the functions created for Genres to process the awards. After a first analysis we can see that the data has a temporal component that adds noise and inconsistency to the feature. Since temporality is already being captured in the `firstPublicationDate` we will strip this to clean the data and, therefore, creating a clean function specific for awards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b653caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>awards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>['Locus Award Nominee for Best Young Adult Boo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>['Bram Stoker Award for Works for Young Reader...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>['Pulitzer Prize for Fiction (1961)', 'Audie A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "0                           The Hunger Games   \n",
       "1  Harry Potter and the Order of the Phoenix   \n",
       "2                      To Kill a Mockingbird   \n",
       "\n",
       "                                              awards  \n",
       "0  ['Locus Award Nominee for Best Young Adult Boo...  \n",
       "1  ['Bram Stoker Award for Works for Young Reader...  \n",
       "2  ['Pulitzer Prize for Fiction (1961)', 'Audie A...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbe_clean[['title','awards']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5527a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books with empty genre values: 41864 (79.77%)\n"
     ]
    }
   ],
   "source": [
    "# identify and handle missing awards values\n",
    "empty_awards_mask = (bbe_clean['awards'] == '[]')\n",
    "empty_awards = bbe_clean[empty_awards_mask].shape[0]\n",
    "share_missing_awards = (empty_awards / len(bbe_clean)) * 100\n",
    "print(f'Books with empty genre values: {empty_awards} ({share_missing_awards:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924d240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_awards_list(lst):\n",
    "    \"\"\"\n",
    "    Clean and normalize awards list:\n",
    "    - Lowercase and remove punctuation noise\n",
    "    - Remove year patterns like (2009) or (2010)\n",
    "    - Deduplicate entries\n",
    "    \"\"\"\n",
    "    if not isinstance(lst, list):\n",
    "        return np.nan\n",
    "\n",
    "    cleaned = []\n",
    "    for a in lst:\n",
    "        if not isinstance(a, str) or not a.strip():\n",
    "            continue\n",
    "        a = a.lower().strip()\n",
    "        # remove (YYYY) patterns\n",
    "        a = re.sub(r'\\(\\s*\\d{4}\\s*\\)', '', a)\n",
    "        # remove leftover punctuation and extra spaces\n",
    "        a = re.sub(r'[^a-z0-9\\s\\-\\&\\'\"]', '', a)\n",
    "        a = re.sub(r'\\s+', ' ', a).strip()\n",
    "        cleaned.append(a)\n",
    "\n",
    "    cleaned = list(set(cleaned))\n",
    "    return cleaned if cleaned else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0b8a761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>awards</th>\n",
       "      <th>awards_parsed</th>\n",
       "      <th>awards_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>['Locus Award Nominee for Best Young Adult Boo...</td>\n",
       "      <td>[Locus Award Nominee for Best Young Adult Book...</td>\n",
       "      <td>[georgia peach book award, dorothy canfield fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>['Bram Stoker Award for Works for Young Reader...</td>\n",
       "      <td>[Bram Stoker Award for Works for Young Readers...</td>\n",
       "      <td>[books i loved best yearly bilby awards for ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>['Pulitzer Prize for Fiction (1961)', 'Audie A...</td>\n",
       "      <td>[Pulitzer Prize for Fiction (1961), Audie Awar...</td>\n",
       "      <td>[audie award for classic, national book award ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "0                           The Hunger Games   \n",
       "1  Harry Potter and the Order of the Phoenix   \n",
       "2                      To Kill a Mockingbird   \n",
       "\n",
       "                                              awards  \\\n",
       "0  ['Locus Award Nominee for Best Young Adult Boo...   \n",
       "1  ['Bram Stoker Award for Works for Young Reader...   \n",
       "2  ['Pulitzer Prize for Fiction (1961)', 'Audie A...   \n",
       "\n",
       "                                       awards_parsed  \\\n",
       "0  [Locus Award Nominee for Best Young Adult Book...   \n",
       "1  [Bram Stoker Award for Works for Young Readers...   \n",
       "2  [Pulitzer Prize for Fiction (1961), Audie Awar...   \n",
       "\n",
       "                                        awards_clean  \n",
       "0  [georgia peach book award, dorothy canfield fi...  \n",
       "1  [books i loved best yearly bilby awards for ol...  \n",
       "2  [audie award for classic, national book award ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbe_clean['awards_parsed'] = bbe_clean['awards'].apply(parse_list_field)\n",
    "bbe_clean['awards_clean'] = bbe_clean['awards_parsed'].apply(clean_awards_list)\n",
    "bbe_clean[['title','awards','awards_parsed','awards_clean']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f55f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique awards: 5616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"dorothy canfield fisher children's book award nominee\", 324),\n",
       " ('lincoln award nominee', 246),\n",
       " ('rhode island teen book award nominee', 219),\n",
       " ('goodreads choice award nominee for young adult fiction', 165),\n",
       " ('carnegie medal nominee', 164),\n",
       " ('goodreads choice award nominee for romance', 147),\n",
       " (\"rebecca caudill young readers' book award nominee\", 147),\n",
       " ('goodreads choice award nominee for fiction', 143),\n",
       " ('missouri gateway readers award nominee', 137),\n",
       " ('goodreads choice award nominee for young adult fantasy & science fiction',\n",
       "  136)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_awards_dict = count_unique_items(bbe_clean, 'awards_clean')\n",
    "num_unique_awards = len(unique_awards_dict)\n",
    "print(f\"Number of unique awards: {num_unique_awards}\")\n",
    "\n",
    "sorted_awards = sorted(unique_awards_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_awards[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f6983",
   "metadata": {},
   "source": [
    "Analysis shows that only a small subset, around **20%** includes award data, with **5,616 unique award names**. The high number of unique labels, combined with low frequency counts per award, makes this feature extremely **sparse and fragmented**. Even the most common award (“Dorothy Canfield Fisher Children's Book Award Nominee”) appears only **324 times** in a dataset of over **52,000 books**, representing less than **1%** of the records.\n",
    "\n",
    "Such high-cardinality categorical data introduces:\n",
    "\n",
    "* **Noise:** because similar awards appear under slightly different names or languages\n",
    "* **Inefficiency:** since one-hot or text encoding would explode feature dimensions\n",
    "* **Weak signal strength:** as most awards occur too infrequently to influence model patterns\n",
    "\n",
    "By simplifying this column into a **binary indicator (`has_award`)**, we preserve the meaningful information; whether a book has received any recognition — while removing the sparsity and variability that would degrade model performance.\n",
    "This boolean variable captures the *prestige signal* without the complexity, improving both interpretability and computational efficiency for downstream tasks such as **recommendation and clustering**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eec3c3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>awards_clean</th>\n",
       "      <th>has_award</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>[georgia peach book award, dorothy canfield fi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>[books i loved best yearly bilby awards for ol...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>[audie award for classic, national book award ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>[georgia peach book award, missouri gateway re...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "0                           The Hunger Games   \n",
       "1  Harry Potter and the Order of the Phoenix   \n",
       "2                      To Kill a Mockingbird   \n",
       "3                        Pride and Prejudice   \n",
       "4                                   Twilight   \n",
       "\n",
       "                                        awards_clean  has_award  \n",
       "0  [georgia peach book award, dorothy canfield fi...       True  \n",
       "1  [books i loved best yearly bilby awards for ol...       True  \n",
       "2  [audie award for classic, national book award ...       True  \n",
       "3                                                NaN      False  \n",
       "4  [georgia peach book award, missouri gateway re...       True  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boolean flag for books with any valid awards\n",
    "bbe_clean['has_award'] = bbe_clean['awards_clean'].apply(\n",
    "    lambda x: isinstance(x, list) and len(x) > 0\n",
    ")\n",
    "bbe_clean[['title','awards_clean','has_award']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9b6173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interim awards datasets saved successfully in data/interim/ directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 9\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim awards datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56168233",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "Before cleaning, we inspected several book descriptions to identify common issues such as editorial notes, missing spaces after punctuation, escaped characters, and residual metadata (e.g., “Librarian’s note”, ISBN mentions, or “(Note: this title…)”).\n",
    "These observations informed the creation of a custom regex-based cleaning function.\n",
    "\n",
    "We then implemented a `clean_description()` function to remove noise, normalize spacing, and prepare text for analysis.\n",
    "Although NLP is currently a stretch goal, we decided to clean the text field proactively to ensure it’s ready for both readability and potential future feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd3bb597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books with no description: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# identify and handle missing awards values\n",
    "no_description_mask = (bbe_clean['description'] == ' ')\n",
    "no_description = bbe_clean[no_description_mask].shape[0]\n",
    "share_no_description = (no_description / len(bbe_clean)) * 100\n",
    "print(f'Books with no description: {no_description} ({share_no_description:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42fef9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>WINNING MEANS FAME AND FORTUNE.LOSING MEANS CE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>There is a door at the end of a silent corrido...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>The unforgettable novel of a childhood in a sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Alternate cover edition of ISBN 9780679783268S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>About three things I was absolutely positive.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Book Thief</td>\n",
       "      <td>Librarian's note: An alternate cover edition c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>Librarian's note: There is an Alternate Cover ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Chronicles of Narnia</td>\n",
       "      <td>Journeys to the end of the world, fantastic cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>J.R.R. Tolkien 4-Book Boxed Set: The Hobbit an...</td>\n",
       "      <td>This four-volume, boxed set contains J.R.R. To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gone with the Wind</td>\n",
       "      <td>Scarlett O'Hara, the beautiful, spoiled daught...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                   The Hunger Games   \n",
       "1          Harry Potter and the Order of the Phoenix   \n",
       "2                              To Kill a Mockingbird   \n",
       "3                                Pride and Prejudice   \n",
       "4                                           Twilight   \n",
       "5                                     The Book Thief   \n",
       "6                                        Animal Farm   \n",
       "7                           The Chronicles of Narnia   \n",
       "8  J.R.R. Tolkien 4-Book Boxed Set: The Hobbit an...   \n",
       "9                                 Gone with the Wind   \n",
       "\n",
       "                                         description  \n",
       "0  WINNING MEANS FAME AND FORTUNE.LOSING MEANS CE...  \n",
       "1  There is a door at the end of a silent corrido...  \n",
       "2  The unforgettable novel of a childhood in a sl...  \n",
       "3  Alternate cover edition of ISBN 9780679783268S...  \n",
       "4  About three things I was absolutely positive.\\...  \n",
       "5  Librarian's note: An alternate cover edition c...  \n",
       "6  Librarian's note: There is an Alternate Cover ...  \n",
       "7  Journeys to the end of the world, fantastic cr...  \n",
       "8  This four-volume, boxed set contains J.R.R. To...  \n",
       "9  Scarlett O'Hara, the beautiful, spoiled daught...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbe_clean[['title','description']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc9e83cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Librarian's note: There is an Alternate Cover Edition for this edition of this book here.A farm is taken over by its overworked, mistreated animals. With flaming idealism and stirring slogans, they set out to create a paradise of progress, justice, and equality. Thus the stage is set for one of the most telling satiric fables ever penned –a razor-edged fairy tale for grown-ups that records the evolution from revolution against tyranny to a totalitarianism just as terrible. When Animal Farm was first published, Stalinist Russia was seen as its target. Today it is devastatingly clear that wherever and whenever freedom is attacked, under whatever banner, the cutting clarity and savage comedy of George Orwell’s masterpiece have a meaning and message still ferociously fresh.\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check a few different descriptions for inspection of things to be cleaned\n",
    "n = 6\n",
    "bbe_clean.loc[n, 'description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def clean_description(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # add a missing period and space between sentences when a lowercase letter \n",
    "    # that is immediately followed by an uppercase letter\n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1. \\2\", text)\n",
    "\n",
    "    # remove librarian/editorial notes and metadata (Note:, Alternate cover, ISBN refs)\n",
    "    text = re.sub(r\"\\(Note:.*?\\)\", \"\", text, flags=re.IGNORECASE)    \n",
    "    text = re.sub(r\"Librarian's note:.*?(?:\\.)\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r\"Alternate cover edition of ISBN \\d+\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"edition of ISBN \\d+\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "    # fix missing space after punctuation (.,;!?) when followed by letter/number\n",
    "    text = re.sub(r'([.,;!?])(?=[A-Za-z0-9])', r'\\1 ', text)\n",
    "\n",
    "    # fix missing space after ISBN numbers (e.g., \"9780679783268Since\")\n",
    "    text = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', text)\n",
    "\n",
    "    # normalize escaped quotes\n",
    "    text = text.replace(\"\\\\'\", \"'\").replace('\\\\\"', '\"')\n",
    "\n",
    "    # keep readable characters, include # for \"#1\"\n",
    "    text = re.sub(r\"[^A-Za-zÀ-ÖØ-öø-ÿ0-9#\\s.,;!?\\'\\\"-]\", \" \", text)\n",
    "\n",
    "    # collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8220a7ea",
   "metadata": {},
   "source": [
    "Before applying the cleaning function to the full dataset, we tested it on **synthetic example** representing all major issues (missing punctuation, uppercase continuity, notes, etc.). After validation, we split the results into two versions:\n",
    "\n",
    "* `description_clean`: preserves original casing — ideal for the **Streamlit dashboard** or display.\n",
    "* `description_nlp`: lowercase version — ready for **NLP or feature engineering** if needed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "644b38d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since today. It was 1939. Nazi Germany. The country is holding its breath. Death has never been busier! By her brother's graveside, Liesel's life changes when she picks up The Gravedigger's Handbook. \"WINNING MEANS FAME AND FORTUNE. LOSING MEANS CERTAIN DEATH.\" Jeune astronome convaincue de l'existence d'une vie extraterrestre intelligente... From #1 New York Times bestselling author Brandon Sanderson, The Way of Kings, book one of The Stormlight Archive begins an incredible new saga of epic proportion. Roshar is a world of stone and storms.\n"
     ]
    }
   ],
   "source": [
    "# test on synthetic description\n",
    "raw_description = \"\"\"\n",
    "Alternate cover edition of ISBN 9780679783268Since today.\n",
    "Librarian's note: An alternate cover edition can be found hereIt was 1939.Nazi Germany.The country is holding its breath.Death has never been busier!\n",
    "By her brother's graveside,Liesel's life changes when she picks up The Gravedigger's Handbook.\n",
    "(Note: this title was not published as YA fiction)\n",
    "\"WINNING MEANS FAME AND FORTUNE.LOSING MEANS CERTAIN DEATH.\"\n",
    "Jeune astronome convaincue de l'existence d'une vie extraterrestre intelligente...\n",
    "From #1 New York Times bestselling author Brandon Sanderson, The Way of Kings, book one of The Stormlight Archive begins an incredible new saga of epic proportion.Roshar is a world of stone and storms.\n",
    "\"\"\"\n",
    "print(clean_description(raw_description))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5ff1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep readable version for UI\n",
    "bbe_clean['description_clean'] = bbe_clean['description'].apply(clean_description)\n",
    "\n",
    "# Lowercase version for NLP\n",
    "bbe_clean['description_nlp'] = bbe_clean['description_clean'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c92777",
   "metadata": {},
   "source": [
    "Finally, we verified both `description_clean` and `description_nlp` for completeness and formatting consistency to confirm that the cleaning pipeline performed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4844985c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>description_nlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>WINNING MEANS FAME AND FORTUNE. LOSING MEANS C...</td>\n",
       "      <td>winning means fame and fortune. losing means c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>There is a door at the end of a silent corrido...</td>\n",
       "      <td>there is a door at the end of a silent corrido...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>The unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Since its immediate success in 1813, Pride and...</td>\n",
       "      <td>since its immediate success in 1813, pride and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>About three things I was absolutely positive. ...</td>\n",
       "      <td>about three things i was absolutely positive. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Book Thief</td>\n",
       "      <td>It is 1939. Nazi Germany. The country is holdi...</td>\n",
       "      <td>it is 1939. nazi germany. the country is holdi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>A farm is taken over by its overworked, mistre...</td>\n",
       "      <td>a farm is taken over by its overworked, mistre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Chronicles of Narnia</td>\n",
       "      <td>Journeys to the end of the world, fantastic cr...</td>\n",
       "      <td>journeys to the end of the world, fantastic cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>J.R.R. Tolkien 4-Book Boxed Set: The Hobbit an...</td>\n",
       "      <td>This four-volume, boxed set contains J. R. R. ...</td>\n",
       "      <td>this four-volume, boxed set contains j. r. r. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gone with the Wind</td>\n",
       "      <td>Scarlett O'Hara, the beautiful, spoiled daught...</td>\n",
       "      <td>scarlett o'hara, the beautiful, spoiled daught...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                   The Hunger Games   \n",
       "1          Harry Potter and the Order of the Phoenix   \n",
       "2                              To Kill a Mockingbird   \n",
       "3                                Pride and Prejudice   \n",
       "4                                           Twilight   \n",
       "5                                     The Book Thief   \n",
       "6                                        Animal Farm   \n",
       "7                           The Chronicles of Narnia   \n",
       "8  J.R.R. Tolkien 4-Book Boxed Set: The Hobbit an...   \n",
       "9                                 Gone with the Wind   \n",
       "\n",
       "                                   description_clean  \\\n",
       "0  WINNING MEANS FAME AND FORTUNE. LOSING MEANS C...   \n",
       "1  There is a door at the end of a silent corrido...   \n",
       "2  The unforgettable novel of a childhood in a sl...   \n",
       "3  Since its immediate success in 1813, Pride and...   \n",
       "4  About three things I was absolutely positive. ...   \n",
       "5  It is 1939. Nazi Germany. The country is holdi...   \n",
       "6  A farm is taken over by its overworked, mistre...   \n",
       "7  Journeys to the end of the world, fantastic cr...   \n",
       "8  This four-volume, boxed set contains J. R. R. ...   \n",
       "9  Scarlett O'Hara, the beautiful, spoiled daught...   \n",
       "\n",
       "                                     description_nlp  \n",
       "0  winning means fame and fortune. losing means c...  \n",
       "1  there is a door at the end of a silent corrido...  \n",
       "2  the unforgettable novel of a childhood in a sl...  \n",
       "3  since its immediate success in 1813, pride and...  \n",
       "4  about three things i was absolutely positive. ...  \n",
       "5  it is 1939. nazi germany. the country is holdi...  \n",
       "6  a farm is taken over by its overworked, mistre...  \n",
       "7  journeys to the end of the world, fantastic cr...  \n",
       "8  this four-volume, boxed set contains j. r. r. ...  \n",
       "9  scarlett o'hara, the beautiful, spoiled daught...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbe_clean[['title','description_clean','description_nlp']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662cdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interim description datasets saved successfully in data/interim/ directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 10\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim description datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a3e56",
   "metadata": {},
   "source": [
    "**Title**\n",
    "\n",
    "To ensure consistent and reproducible page-count retrieval from the Google Books API, we cleaned the `title` column into a new feature: `title_clean`.\n",
    "\n",
    "Cleaning steps applied:\n",
    "- Removed any parenthetical or bracketed information (e.g., \"(Box Set)\", \"[Hardcover Edition]\")  \n",
    "- Removed format indicators such as “Edition”, “Collection”, or “Book #”  \n",
    "- Normalized punctuation and spacing  \n",
    "- Lowercased all titles for consistent querying  \n",
    "\n",
    "This feature (`title_clean`) will be used as input for API-based data imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ce9d220d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twilight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title\n",
       "0                           The Hunger Games\n",
       "1  Harry Potter and the Order of the Phoenix\n",
       "2                      To Kill a Mockingbird\n",
       "3                        Pride and Prejudice\n",
       "4                                   Twilight"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbe_clean[['title']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "77b51e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean book titles for Google Books API queries.\n",
    "    \n",
    "    Steps:\n",
    "    - Remove bracketed or parenthetical notes (e.g. '(Box Set)', '[Hardcover]')\n",
    "    - Remove common edition/format words\n",
    "    - Remove ellipsis or truncated markers\n",
    "    - Normalize spaces and lowercase\n",
    "    \"\"\"\n",
    "    if not isinstance(title, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = title.strip()\n",
    "    \n",
    "    # remove bracketed or parenthetical content\n",
    "    text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    \n",
    "    # remove common edition/format terms\n",
    "    text = re.sub(r\"\\b(box set|collection|illustrated|edition|volume|vol\\.|book\\s*\\d+)\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # remove ellipsis or truncation markers\n",
    "    text = text.replace(\"...\", \"\")\n",
    "    \n",
    "    # remove extra punctuation and multiple spaces\n",
    "    text = re.sub(r\"[^\\w\\s'\\-]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # lowercase for API consistency\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4e75061d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     the hunger games\n",
       "1            harry potter and the order of the phoenix\n",
       "2                                to kill a mockingbird\n",
       "3                                  pride and prejudice\n",
       "4                                             twilight\n",
       "5                                       the book thief\n",
       "6                                          animal farm\n",
       "7                             the chronicles of narnia\n",
       "8    jrr tolkien 4-book boxed set the hobbit and th...\n",
       "9                                   gone with the wind\n",
       "Name: title_clean, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbe_clean['title_clean'] = bbe_clean['title'].apply(clean_title)\n",
    "bbe_clean['title_clean'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29b465",
   "metadata": {},
   "source": [
    "After cleaning the title field into `title_clean`, we focused on ensuring consistency in the dataset by filtering only English-language books. This decision was based on majority of English language books in the dataset and on maintaining data homogeneity for downstream analysis.\n",
    "\n",
    "We also chose to keep only one version of each title per author, prioritizing the record with the highest `numRatings`. This ensures that the retained record represents the most widely rated (and therefore most validated) version of the book, avoiding redundancy while preserving author attribution. By checking both `title_clean` and `author_clean`, we safeguard against incorrectly dropping books with similar titles but different authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "14a0a1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>language_clean</th>\n",
       "      <th>rating_clean</th>\n",
       "      <th>numRatings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>'salem's lot</td>\n",
       "      <td>Stephen King, Jerry N. Uelsmann</td>\n",
       "      <td>en</td>\n",
       "      <td>4.25</td>\n",
       "      <td>94874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>'salem's lot</td>\n",
       "      <td>Stephen King</td>\n",
       "      <td>en</td>\n",
       "      <td>4.03</td>\n",
       "      <td>334051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48062</th>\n",
       "      <td>'til death</td>\n",
       "      <td>Sharon Sala</td>\n",
       "      <td>en</td>\n",
       "      <td>4.23</td>\n",
       "      <td>1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27557</th>\n",
       "      <td>'til death</td>\n",
       "      <td>Miasha</td>\n",
       "      <td>en</td>\n",
       "      <td>4.20</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11569</th>\n",
       "      <td>1919</td>\n",
       "      <td>John Dos Passos, E.L. Doctorow</td>\n",
       "      <td>en</td>\n",
       "      <td>3.99</td>\n",
       "      <td>2536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10578</th>\n",
       "      <td>1919</td>\n",
       "      <td>أحمد مراد</td>\n",
       "      <td>ar</td>\n",
       "      <td>3.65</td>\n",
       "      <td>20860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32466</th>\n",
       "      <td>1919</td>\n",
       "      <td>أحمد خالد توفيق</td>\n",
       "      <td>ar</td>\n",
       "      <td>3.60</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>1q84</td>\n",
       "      <td>Haruki Murakami, Jay Rubin, Philip Gabriel</td>\n",
       "      <td>en</td>\n",
       "      <td>3.92</td>\n",
       "      <td>201096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4463</th>\n",
       "      <td>1q84</td>\n",
       "      <td>Haruki Murakami, 村上 春樹</td>\n",
       "      <td>ja</td>\n",
       "      <td>3.90</td>\n",
       "      <td>28602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>1q84</td>\n",
       "      <td>Haruki Murakami, 村上 春樹</td>\n",
       "      <td>ja</td>\n",
       "      <td>4.00</td>\n",
       "      <td>49121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24612</th>\n",
       "      <td>6 steps to forgiveness a live forgiveness sess...</td>\n",
       "      <td>Josephine Hasan-Kerr, Elaine Earle MA, Carey K...</td>\n",
       "      <td>en</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28339</th>\n",
       "      <td>6 steps to forgiveness a live forgiveness sess...</td>\n",
       "      <td>Josephine D. Hasan-Kerr, Elaine Earle, Carey L...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34084</th>\n",
       "      <td>a cottage by the sea</td>\n",
       "      <td>Ciji Ware</td>\n",
       "      <td>en</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39633</th>\n",
       "      <td>a cottage by the sea</td>\n",
       "      <td>Carole Matthews</td>\n",
       "      <td>en</td>\n",
       "      <td>4.10</td>\n",
       "      <td>3531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29371</th>\n",
       "      <td>a death in vienna</td>\n",
       "      <td>Frank Tallis, Esti A. Budihabsari</td>\n",
       "      <td>id</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16410</th>\n",
       "      <td>a death in vienna</td>\n",
       "      <td>Daniel Silva</td>\n",
       "      <td>en</td>\n",
       "      <td>4.21</td>\n",
       "      <td>22004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42087</th>\n",
       "      <td>a dog's way home</td>\n",
       "      <td>W. Bruce Cameron</td>\n",
       "      <td>en</td>\n",
       "      <td>4.19</td>\n",
       "      <td>10640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21626</th>\n",
       "      <td>a dog's way home</td>\n",
       "      <td>Bobbie Pyron</td>\n",
       "      <td>en</td>\n",
       "      <td>4.21</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45259</th>\n",
       "      <td>a house without windows</td>\n",
       "      <td>Nadia Hashimi</td>\n",
       "      <td>en</td>\n",
       "      <td>3.98</td>\n",
       "      <td>9275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>a house without windows</td>\n",
       "      <td>Stevie Turner</td>\n",
       "      <td>en</td>\n",
       "      <td>3.62</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title_clean  \\\n",
       "3674                                        'salem's lot   \n",
       "385                                         'salem's lot   \n",
       "48062                                         'til death   \n",
       "27557                                         'til death   \n",
       "11569                                               1919   \n",
       "10578                                               1919   \n",
       "32466                                               1919   \n",
       "657                                                 1q84   \n",
       "4463                                                1q84   \n",
       "2876                                                1q84   \n",
       "24612  6 steps to forgiveness a live forgiveness sess...   \n",
       "28339  6 steps to forgiveness a live forgiveness sess...   \n",
       "34084                               a cottage by the sea   \n",
       "39633                               a cottage by the sea   \n",
       "29371                                  a death in vienna   \n",
       "16410                                  a death in vienna   \n",
       "42087                                   a dog's way home   \n",
       "21626                                   a dog's way home   \n",
       "45259                            a house without windows   \n",
       "2583                             a house without windows   \n",
       "\n",
       "                                            author_clean language_clean  \\\n",
       "3674                     Stephen King, Jerry N. Uelsmann             en   \n",
       "385                                         Stephen King             en   \n",
       "48062                                        Sharon Sala             en   \n",
       "27557                                             Miasha             en   \n",
       "11569                     John Dos Passos, E.L. Doctorow             en   \n",
       "10578                                          أحمد مراد             ar   \n",
       "32466                                    أحمد خالد توفيق             ar   \n",
       "657           Haruki Murakami, Jay Rubin, Philip Gabriel             en   \n",
       "4463                              Haruki Murakami, 村上 春樹             ja   \n",
       "2876                              Haruki Murakami, 村上 春樹             ja   \n",
       "24612  Josephine Hasan-Kerr, Elaine Earle MA, Carey K...             en   \n",
       "28339  Josephine D. Hasan-Kerr, Elaine Earle, Carey L...        unknown   \n",
       "34084                                          Ciji Ware             en   \n",
       "39633                                    Carole Matthews             en   \n",
       "29371                  Frank Tallis, Esti A. Budihabsari             id   \n",
       "16410                                       Daniel Silva             en   \n",
       "42087                                   W. Bruce Cameron             en   \n",
       "21626                                       Bobbie Pyron             en   \n",
       "45259                                      Nadia Hashimi             en   \n",
       "2583                                       Stevie Turner             en   \n",
       "\n",
       "       rating_clean  numRatings  \n",
       "3674           4.25       94874  \n",
       "385            4.03      334051  \n",
       "48062          4.23        1008  \n",
       "27557          4.20         183  \n",
       "11569          3.99        2536  \n",
       "10578          3.65       20860  \n",
       "32466          3.60         595  \n",
       "657            3.92      201096  \n",
       "4463           3.90       28602  \n",
       "2876           4.00       49121  \n",
       "24612          5.00           1  \n",
       "28339          4.80           5  \n",
       "34084          3.75        3291  \n",
       "39633          4.10        3531  \n",
       "29371          3.75        2381  \n",
       "16410          4.21       22004  \n",
       "42087          4.19       10640  \n",
       "21626          4.21        1887  \n",
       "45259          3.98        9275  \n",
       "2583           3.62         578  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find duplicated raw titles (case-insensitive)\n",
    "dup_raw = bbe_clean[bbe_clean.duplicated(subset='title_clean', keep=False)]\n",
    "\n",
    "# sort and preview a few\n",
    "dup_raw.sort_values('title_clean').head(20)[['title_clean', 'author_clean', 'language_clean', 'rating_clean', 'numRatings',]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2ae18f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books before filtering: 52478\n",
      "Books after filtering English only: 42663\n"
     ]
    }
   ],
   "source": [
    "bbe_en = bbe_clean[bbe_clean['language_clean'] == 'en'].copy()\n",
    "print(f\"Books before filtering: {len(bbe_clean)}\")\n",
    "print(f\"Books after filtering English only: {len(bbe_en)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f1174603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>rating_clean</th>\n",
       "      <th>numRatings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>a song of ice and fire</td>\n",
       "      <td>George R.R. Martin</td>\n",
       "      <td>4.56</td>\n",
       "      <td>53233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>a song of ice and fire</td>\n",
       "      <td>George R.R. Martin</td>\n",
       "      <td>4.63</td>\n",
       "      <td>44894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37383</th>\n",
       "      <td>always and forever</td>\n",
       "      <td>Harper Bentley</td>\n",
       "      <td>3.93</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37437</th>\n",
       "      <td>always and forever</td>\n",
       "      <td>Harper Bentley</td>\n",
       "      <td>3.93</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>artemis fowl</td>\n",
       "      <td>Eoin Colfer</td>\n",
       "      <td>3.85</td>\n",
       "      <td>495987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9391</th>\n",
       "      <td>artemis fowl</td>\n",
       "      <td>Eoin Colfer</td>\n",
       "      <td>4.41</td>\n",
       "      <td>3412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37426</th>\n",
       "      <td>beatlebone</td>\n",
       "      <td>Kevin Barry</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37480</th>\n",
       "      <td>beatlebone</td>\n",
       "      <td>Kevin Barry</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>beautiful creatures</td>\n",
       "      <td>Kami Garcia, Margaret Stohl</td>\n",
       "      <td>3.76</td>\n",
       "      <td>539336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33933</th>\n",
       "      <td>beautiful creatures</td>\n",
       "      <td>Kami Garcia, Margaret Stohl</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title_clean                 author_clean  rating_clean  \\\n",
       "1679   a song of ice and fire           George R.R. Martin          4.56   \n",
       "2899   a song of ice and fire           George R.R. Martin          4.63   \n",
       "37383      always and forever               Harper Bentley          3.93   \n",
       "37437      always and forever               Harper Bentley          3.93   \n",
       "444              artemis fowl                  Eoin Colfer          3.85   \n",
       "9391             artemis fowl                  Eoin Colfer          4.41   \n",
       "37426              beatlebone                  Kevin Barry          3.47   \n",
       "37480              beatlebone                  Kevin Barry          3.47   \n",
       "364       beautiful creatures  Kami Garcia, Margaret Stohl          3.76   \n",
       "33933     beautiful creatures  Kami Garcia, Margaret Stohl          4.30   \n",
       "\n",
       "       numRatings  \n",
       "1679        53233  \n",
       "2899        44894  \n",
       "37383         482  \n",
       "37437         482  \n",
       "444        495987  \n",
       "9391         3412  \n",
       "37426        2528  \n",
       "37480        2528  \n",
       "364        539336  \n",
       "33933        1875  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_en = bbe_en[bbe_en.duplicated(subset=['title_clean', 'author_clean'], keep=False)]\n",
    "dup_en.sort_values(['title_clean', 'author_clean']).head(10)[\n",
    "    ['title_clean', 'author_clean', 'rating_clean', 'numRatings']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4664d756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books after deduplication: 42585\n"
     ]
    }
   ],
   "source": [
    "# Keep highest numRatings version per (title_clean, author_clean)\n",
    "bbe_en_unique = (\n",
    "    bbe_en.sort_values('numRatings', ascending=False)\n",
    "          .drop_duplicates(subset=['title_clean', 'author_clean'], keep='first')\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Books after deduplication: {len(bbe_en_unique)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4d6b72bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check passed: No duplicate (title_clean, author_clean) pairs remain.\n"
     ]
    }
   ],
   "source": [
    "# sanity check: ensure unique title-author combinations\n",
    "duplicates_check = bbe_en_unique[bbe_en_unique.duplicated(subset=['title_clean', 'author_clean'], keep=False)]\n",
    "\n",
    "if duplicates_check.empty:\n",
    "    print(\"Sanity check passed: No duplicate (title_clean, author_clean) pairs remain.\")\n",
    "else:\n",
    "    print(\"Warning: Duplicates still exist — review these cases:\")\n",
    "    display(duplicates_check[['title_clean', 'author_clean', 'numRatings']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765527c",
   "metadata": {},
   "source": [
    "At this stage, the dataset has been standardized, language-filtered, and deduplicated, producing a clean and reproducible foundation for subsequent tasks such as page count retrieval, feature engineering, and recommendation modeling.\n",
    "This cleaned version (`bbe_en_unique`) represents the highest-quality English entries suitable for integration with the Google Books API and further predictive analytics workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7a889d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interim English-only deduplicated dataset saved successfully as bbe_clean_v11.csv in data/interim/bbe/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 11 \n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "interim_bbe_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bbe_en_unique.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim English-only deduplicated dataset saved successfully as bbe_clean_v{version}.csv in data/interim/bbe/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834cfb9f",
   "metadata": {},
   "source": [
    "**Pages**\n",
    "\n",
    "By cleaning the title, we reduced from **2.3K** items missing pages to **1.5k** items.  \n",
    "For a recommendation system, book length and edition type often correlate with engagement and retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "689963c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter and the Sorcerer's Stone</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To Kill a Mockingbird</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Fault in Our Stars</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1984</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Divergent</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hobbit, or There and Back Again</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title pages\n",
       "0  Harry Potter and the Sorcerer's Stone   309\n",
       "1                       The Hunger Games   374\n",
       "2                               Twilight   501\n",
       "3                  To Kill a Mockingbird   324\n",
       "4                       The Great Gatsby   200\n",
       "5                 The Fault in Our Stars   313\n",
       "6                                   1984   237\n",
       "7                    Pride and Prejudice   279\n",
       "8                              Divergent   487\n",
       "9    The Hobbit, or There and Back Again   366"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# retrieve the latest file\n",
    "bbe_clean = pd.read_csv('data/interim/bbe/bbe_clean_v11.csv', low_memory=False)\n",
    "\n",
    "bbe_clean[['title','pages']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5f61fd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pages\n",
       "NaN    1323\n",
       "320     910\n",
       "352     783\n",
       "288     755\n",
       "256     690\n",
       "304     683\n",
       "384     668\n",
       "336     665\n",
       "224     575\n",
       "368     563\n",
       "240     558\n",
       "192     516\n",
       "272     500\n",
       "400     492\n",
       "416     443\n",
       "208     443\n",
       "432     361\n",
       "160     337\n",
       "448     307\n",
       "144     303\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbe_clean['pages'].value_counts(dropna=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b2eafd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-numeric characters and convert\n",
    "bbe_clean['pages_clean'] = (\n",
    "    bbe_clean['pages']\n",
    "    .astype(str)\n",
    "    .str.extract(r'(\\d+)')  # raw string to avoid escape warnings\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# replace unrealistic values with NaN\n",
    "bbe_clean.loc[\n",
    "    (bbe_clean['pages_clean'] < 10) | (bbe_clean['pages_clean'] > 3000),\n",
    "    'pages_clean'\n",
    "] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "404178f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing pages: 1575\n"
     ]
    }
   ],
   "source": [
    "new_missing = bbe_clean['pages_clean'].isna().sum()\n",
    "print(f\"Remaining missing pages: {new_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e49239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 10.0\n",
      "Max: 3000.0\n",
      "0.01      29.0\n",
      "0.25     213.0\n",
      "0.50     304.0\n",
      "0.75     392.0\n",
      "0.99    1056.0\n",
      "Name: pages_clean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Min:\", bbe_clean['pages_clean'].min())\n",
    "print(\"Max:\", bbe_clean['pages_clean'].max())\n",
    "\n",
    "# quick quantile check\n",
    "print(bbe_clean['pages_clean'].quantile([0.01, 0.25, 0.5, 0.75, 0.99]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
