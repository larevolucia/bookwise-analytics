{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51fc4fb3",
   "metadata": {},
   "source": [
    "\n",
    "# Data Cleaning\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The purpose of this notebook is to **clean, standardize, and prepare the collected datasets** for subsequent exploratory analysis and modeling tasks.\n",
    "\n",
    "The goal is to transform raw inputs from multiple book datasets into a **reliable, consistent, and mergeable analytical base**, ensuring data integrity and comparability across platforms.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "| Dataset                    | Source                     | Description                                                               | Format |\n",
    "| -------------------------- | -------------------------- | ------------------------------------------------------------------------- | ------ |\n",
    "| `bbe_books.csv`            | Zenodo – *Best Books Ever* | Book metadata including title, author, rating, genres, and description.   | CSV    |\n",
    "| `books.csv`, `ratings.csv` | GitHub – *Goodbooks-10k*   | Book metadata and user–book interaction data for recommendation modeling. | CSV    |\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks in This Notebook\n",
    "\n",
    "This notebook will execute the following cleaning and preparation steps:\n",
    "\n",
    "1. **Standardize column formats:**\n",
    "   Ensure consistent data types and naming conventions across datasets (e.g., convert `isbn` to string, align `author`, `rating`, and `title` formats).\n",
    "\n",
    "2. **Clean and normalize missing values:**\n",
    "   Replace placeholder NaNs (`9999999999999`, empty lists, or `\"None\"`) with `np.nan`, then impute or drop based on analytical importance.\n",
    "\n",
    "3. **Detect and resolve duplicates:**\n",
    "   Identify duplicate records using key identifiers (`bookId`, `isbn`, `title + author`) and retain the most complete or relevant entries.\n",
    "\n",
    "4. **Validate and align categorical values:**\n",
    "   Standardize genre labels, language codes, and rating scales to ensure comparability between datasets.\n",
    "\n",
    "5. **Merge compatible datasets:**\n",
    "   Integrate *BestBooksEver* and *Goodbooks-10k_books* into a unified schema while maintaining referential integrity with the ratings dataset.\n",
    "\n",
    "6. **Outlier and consistency checks:**\n",
    "   Review numerical and date fields (e.g., `pages`, `price`, `publishDate`) for unrealistic or extreme values and adjust as needed.\n",
    "\n",
    "7. **Feature enrichment (optional):**\n",
    "   Derive or enhance fields such as `popularity_score`, `recency`, or missing genre information using external APIs where beneficial.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **Cleaned, schema-aligned datasets** ready for exploratory data analysis and modeling.\n",
    "* **Summary statistics** on completeness, duplicates, and outliers.\n",
    "* **Processed CSV files** saved for reproducibility in `data/processed/`.\n",
    "\n",
    "> **Note:** This notebook focuses on the *Data Cleaning and Preparation*. Further feature engineering and model-specific transformations will follow in later notebooks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383657df",
   "metadata": {},
   "source": [
    "## Navigate to the Parent Directory\n",
    "\n",
    "Before combining and saving datasets, it’s often helpful to move to a parent directory so that file operations (like loading or saving data) are easier and more organized. \n",
    "\n",
    "Before using the Python’s built-in os module to move one level up from the current working directory, it is advisable to inspect the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f'Current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f3220",
   "metadata": {},
   "source": [
    "To change to parent directory (root folder), run the code below. If you are already in the root folder, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c41cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to its parent\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print('Changed directory to parent.')\n",
    "\n",
    "# Get the new current working directory (the parent directory)\n",
    "current_dir = os.getcwd()\n",
    "print(f'New current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1120c",
   "metadata": {},
   "source": [
    "## Load and Inspect Books Datasets\n",
    "\n",
    "In this step, we load the previously collected datasets: **Goodbooks-10k** (books) and **Best Books Ever**. We will inspect their structure one more time before starting any merging or cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load datasets\n",
    "books_raw = pd.read_csv('data/raw/books.csv')\n",
    "bbe_raw = pd.read_csv('data/raw/bbe_books.csv')\n",
    "\n",
    "# create copies for cleaning\n",
    "books_clean = books_raw.copy()\n",
    "bbe_clean = bbe_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "display(bbe_clean.head(3))\n",
    "display(books_clean.head(3))\n",
    "\n",
    "# Check shape and missing values\n",
    "for name, df in {'BBE': bbe_clean, 'Books': books_clean,}.items():\n",
    "    print(f\"\\n{name} — Shape: {df.shape}\")\n",
    "    print(df.info())\n",
    "    print(df.isna().sum().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b59d8",
   "metadata": {},
   "source": [
    "We will check if the datasets share common identifiers and compatible data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4382d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_only_columns = set(bbe_clean.columns) - set(books_clean.columns)\n",
    "print(f'Columns only in BBE: {bbe_only_columns}')\n",
    "\n",
    "goodbooks_only_columns = set(books_clean.columns) - set(bbe_clean.columns)\n",
    "print(f'Columns only in Goodbooks: {goodbooks_only_columns}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66be32",
   "metadata": {},
   "source": [
    "Based on the initial inspection, we can create a mapping table to align columns from both datasets for merging and analysis.\n",
    "\n",
    "| **BestBooksEver (BBE)** | **Goodbooks10k_books (GB10k)** | **Notes / Alignment Rationale** |\n",
    "| --------------------------------- | ------------------------------------------------ | -------------------------------------------------------------------------- |\n",
    "| `bookId` | `book_id` | Main identifier; ensure both are numeric. |\n",
    "| `bookId_num` | `goodreads_book_id` | Goodreads identifier; ensure both are numeric for joining. |\n",
    "| `title` | `title` | Direct match. Used as secondary join key. |\n",
    "| `series` | — | Only in BBE; could enrich GB10k if available via API. |\n",
    "| `author` | `authors` | Same meaning. Normalize format. |\n",
    "| `rating` | `average_rating` | Equivalent — rename to unified `average_rating`. |\n",
    "| `numRatings` | `ratings_count` | Same measure of total user ratings. |\n",
    "| `ratingsByStars` | `ratings_1` … `ratings_5` | BBE has dict, GB10k has explicit columns. Expand or aggregate accordingly. |\n",
    "| `likedPercent` | — | BBE-only; optional metric of user sentiment. |\n",
    "| `isbn` | `isbn` / `isbn13` | Common linking key; keep both (string). Use for merges when present. |\n",
    "| `language` | `language_code` | Standardize to ISO 639-1 (lowercase). |\n",
    "| `description` | — | BBE-only; valuable for NLP features. |\n",
    "| `genres` | — | BBE-only; can enrich GB10k tags later. |\n",
    "| `characters` | — | bbe_clean-only; low modeling priority, but could add narrative metadata. |\n",
    "| `bookFormat` | — | BBE-only; possible categorical feature. |\n",
    "| `edition` | — | BBE-only. |\n",
    "| `pages` | — | BBE-only; numeric, may enrich GB10k metadata. |\n",
    "| `publisher` | — | bbe_clean_clean-only; possible future feature. |\n",
    "| `publishDate` | — | bbe_clean_clean-only; can approximate from GB10k’s `original_publication_year`. |\n",
    "| `firstPublishDate` | `original_publication_year` | Equivalent (date vs year). |\n",
    "| `coverImg` | `image_url` / `small_image_url` | Same function (cover link). |\n",
    "| `bbeScore` | — | BBE-only; internal popularity score. |\n",
    "| `bbeVotes` | `work_ratings_count` | Comparable as popularity proxy. |\n",
    "| `price` | — | BBE-only; likely non-essential for satisfaction prediction. |\n",
    "| `setting` | — | BBE-only; can support content enrichment. |\n",
    "| `awards` | — | BBE-only; categorical enrichment. |\n",
    "| — | `goodreads_book_id` / `best_book_id` / `work_id` | GB10k-only identifiers; may be used for deeper Goodreads linking. |\n",
    "| — | `books_count` | GB10k-only; number of editions per work. |\n",
    "| — | `work_text_reviews_count` | GB10k-only; can complement `numRatings` as engagement metric. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5069ac1",
   "metadata": {},
   "source": [
    "## Data Cleaning Steps\n",
    "\n",
    "### Best Books Ever\n",
    "\n",
    "- Handle identifier columns\n",
    "- Standardize key columns: `author`, `language`\n",
    "- Missing data handling strategies\n",
    "- Normalize genre and format\n",
    "- Validate for no nulls or duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e6ae8",
   "metadata": {},
   "source": [
    "#### 1. Handle identifier columns\n",
    "On the previous notebook, we created a new field `bookId_num` in the BBE dataset to align with `goodreads_book_id` in the Goodbooks10k dataset. We have also ensured that they were both converted to numeric types and that all `bookId` values generated a valid `bookId_num`. So we can skip the handle identifier columns, as it was already done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c6e5f",
   "metadata": {},
   "source": [
    "#### 2. Standardize key columns\n",
    "\n",
    "**Author**\n",
    "\n",
    "We will proceed with the standardization of key columns, starting with the `author` column. The author column in the BBE dataset often contains a qualifier such as \"(Goodreads Author)\". We will remove such qualifiers to standardize the format. We will also create an additional list column to store multiple authors as a list rather than a single string. This way, its is ready to use for feature engineering later on if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_split_authors(name):\n",
    "    \"\"\"\n",
    "    Cleans author names and returns a list of authors.\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "\n",
    "    # Remove role descriptors\n",
    "    cleaned = re.sub(r\"\\s*\\([^)]*\\)\", \"\", name)\n",
    "    \n",
    "    # Split into list if multiple authors exist\n",
    "    authors_list = [a.strip() for a in cleaned.split(\",\") if a.strip()]\n",
    "    \n",
    "    return authors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145894ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to BestBooksEver dataset\n",
    "bbe_clean[\"authors_list\"] = bbe_clean[\"author\"].apply(clean_and_split_authors)\n",
    "bbe_clean[\"author_clean\"] = bbe_clean[\"authors_list\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else None)\n",
    "\n",
    "# Quick check\n",
    "bbe_clean[[\"author\", \"author_clean\", \"authors_list\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a74c31",
   "metadata": {},
   "source": [
    "**Language**\n",
    "\n",
    "The `language` column in the Best Books Ever dataset used full names such as “English”, “German”, and “Arabic”.  Before transforming the values, we will check for all unique values to identify any unexpected entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique language values\n",
    "print(\"Unique language values in BBE dataset:\")\n",
    "bbe_clean['language'] = bbe_clean['language'].astype(str).str.strip()\n",
    "unique_languages = bbe_clean['language'].unique()\n",
    "\n",
    "print(f\"\\nTotal unique values: {len(unique_languages)}\\n\")\n",
    "print(unique_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d9853",
   "metadata": {},
   "source": [
    "We can see that there are some unexpected values such as:\n",
    "- _historical forms_ (“English, Middle (1100-1500)”, “French, Middle (ca.1400-1600)”)\n",
    "- _combined or semicolon-separated entries_ (“Filipino; Pilipino”, “Catalan; Valencian”)\n",
    "- _multi-language / uncertain cases_ (“Multiple languages”, “Undetermined”)\n",
    "- _rare or dialects_ (“Bokmål, Norwegian; Norwegian Bokmål”, “Aromanian; Arumanian; Macedo-Romanian”)\n",
    "\n",
    "We will clean the unusual entries by mapping them to the closest language present in the ISO 639-1 standard. Unrecognized values will be flagged and replaced with `\"unknown\"`. It was decided to distinguish the `\"unknown\"` from the `NaN` values to retain information about missingness versus unrecognized entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a83c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Standardize capitalization & spacing\n",
    "bbe_clean['language'] = bbe_clean['language'].astype(str).str.strip().str.title()\n",
    "\n",
    "# Handle NaNs that became strings\n",
    "bbe_clean['language'] = bbe_clean['language'].replace({'Nan': np.nan})\n",
    "\n",
    "# Simplify and unify multi-language / dialect forms\n",
    "replace_map = {\n",
    "    'Multiple Languages': 'Multilingual',\n",
    "    'Undetermined': 'Unknown',\n",
    "    'Iranian (Other)': 'Persian',\n",
    "    'Farsi': 'Persian',\n",
    "    'Filipino; Pilipino': 'Filipino',\n",
    "    'Catalan; Valencian': 'Catalan',\n",
    "    'Panjabi; Punjabi': 'Punjabi',\n",
    "    'Bokmål, Norwegian; Norwegian Bokmål': 'Norwegian',\n",
    "    'Norwegian Nynorsk; Nynorsk, Norwegian': 'Norwegian',\n",
    "    'Greek, Modern (1453-)': 'Greek',\n",
    "    'Greek, Ancient (To 1453)': 'Greek',\n",
    "    'French, Middle (Ca.1400-1600)': 'French',\n",
    "    'English, Middle (1100-1500)': 'English',\n",
    "    'Dutch, Middle (Ca.1050-1350)': 'Dutch',\n",
    "    'Aromanian; Arumanian; Macedo-Romanian': 'Romanian',\n",
    "    'Mayan Languages': 'Mayan',\n",
    "    'Australian Languages': 'English'\n",
    "}\n",
    "\n",
    "bbe_clean['language_clean'] = bbe_clean['language'].replace(replace_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5060ad13",
   "metadata": {},
   "source": [
    "After transforming the values, we apply a mapping to standardize the `language` column using **ISO 639-1 two-letter codes**.\n",
    "The mapping dictionaries are stored in the `src/cleaning/mappings/` folder to keep the notebooks cleaner and improve readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37289380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"src/cleaning/mappings/languages_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    languages_dict = json.load(f)\n",
    "\n",
    "# Apply dictionary\n",
    "bbe_clean['language_clean'] = bbe_clean['language'].str.lower().map(languages_dict)\n",
    "\n",
    "# Fill remaining NaNs\n",
    "bbe_clean['language_clean'] = bbe_clean['language_clean'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again for unique language values\n",
    "print(\"Unique language values in BBE dataset:\")\n",
    "unique_languages = bbe_clean['language'].unique()\n",
    "\n",
    "print(f\"\\nTotal unique values: {len(unique_languages)}\\n\")\n",
    "print(unique_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_breakdown = (\n",
    "    bbe_clean['language']\n",
    "    .value_counts()\n",
    "    .to_frame('count')\n",
    ")\n",
    "\n",
    "language_breakdown['percentage'] = (\n",
    "    language_breakdown['count'] / len(bbe_clean) * 100\n",
    ").round(2)\n",
    "\n",
    "print(language_breakdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6b8f9",
   "metadata": {},
   "source": [
    "**Dates**\n",
    "\n",
    "BBE dataset has two publication fields: `publishDate` and `firstPublishDate`. The `firstPublishDate` represents the original publication date, while `publishDate` refers to a more recent edition or reprint date. Publishing experts assumption is that the recency of the `firstPublishDate` is more relevant for modeling book satisfaction, as it reflects when the book was first introduced to readers. Therefore, we will focus on cleaning and standardizing the `firstPublishDate` column and use `publishDate` only if `firstPublishDate` is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2685c",
   "metadata": {},
   "source": [
    "While majority of the dates follow the 'MM/DD/YY' format, after a first attemp at cleaning, we noticed some dates do not conform to this format. Therefore, we will implement a more robust date parsing strategy, focusing first on transforming textual formats into 'MM/DD/YYYY' format before attempting to parse them into datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6d327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "def clean_date_string(date_str):\n",
    "    \"\"\"Remove ordinal suffixes and unwanted characters from a date string.\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return np.nan\n",
    "    # remove st, nd, rd, th (like 'April 27th 2010' → 'April 27 2010')\n",
    "    cleaned = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', str(date_str))\n",
    "    return cleaned.strip()\n",
    "\n",
    "def parse_mixed_date(date_str):\n",
    "    \"\"\"Try to parse a variety of date formats safely.\"\"\"\n",
    "    if pd.isna(date_str) or date_str == '':\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Use dateutil to parse most human-readable formats\n",
    "        return parser.parse(date_str, fuzzy=True)\n",
    "    except Exception:\n",
    "        # Try year-only fallback (e.g. '2003')\n",
    "        match = re.match(r'^\\d{4}$', str(date_str))\n",
    "        if match:\n",
    "            return pd.to_datetime(f\"{date_str}-01-01\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7cc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to both columns\n",
    "for col in ['firstPublishDate', 'publishDate']:\n",
    "       bbe_clean[f'{col}_clean'] = (\n",
    "        bbe_clean[col]\n",
    "        .astype(str)\n",
    "        .replace({'nan': np.nan, '': np.nan})\n",
    "        .apply(clean_date_string)\n",
    "        .apply(parse_mixed_date)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0493da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine using your logic: prefer firstPublishDate, else publishDate\n",
    "bbe_clean['publication_date_clean'] = (\n",
    "    bbe_clean['firstPublishDate_clean'].combine_first(bbe_clean['publishDate_clean'])\n",
    ")\n",
    "# Reconvert to datetime safely before using .dt\n",
    "bbe_clean['publication_date_clean'] = pd.to_datetime(bbe_clean['publication_date_clean'], errors='coerce')\n",
    "\n",
    "# Format as ISO standard\n",
    "bbe_clean['publication_date_clean'] = bbe_clean['publication_date_clean'].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Check a sample of remaining nulls\n",
    "bbe_clean[bbe_clean['publication_date_clean'].isna()][['title', 'firstPublishDate', 'publishDate', 'publication_date_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the unified publication date is missing\n",
    "total = len(bbe_clean)\n",
    "bbe_missing_dates = bbe_clean.loc[bbe_clean['publication_date_clean'].isna()]\n",
    "missing_count = len(bbe_missing_dates)\n",
    "\n",
    "print(f\"Missing publication dates: {missing_count} of {total} ({missing_count/total:.2%})\")\n",
    "\n",
    "# Preview key columns\n",
    "bbe_missing_dates[['title', 'author', 'firstPublishDate', 'publishDate', 'publication_date_clean']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f7a9c",
   "metadata": {},
   "source": [
    "**Publisher**\n",
    "\n",
    "Publisher names can vary significantly in formatting, including differences in capitalization, punctuation, and spacing. To standardize the `publisher` column, we will convert all entries to lowercase and strip any leading or trailing whitespace. This will help reduce variability and improve consistency across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample publishers:\")\n",
    "print(bbe_clean['publisher'].drop_duplicates().sample(30, random_state=42).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Strip, lowercase, remove extra spaces and punctuation\n",
    "bbe_clean['publisher'] = (\n",
    "    bbe_clean['publisher']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace('\"', '', regex=False)\n",
    "    .str.replace(\"'\", '', regex=False)\n",
    "    .str.replace(r'[.,]', '', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e51a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique publisher values \n",
    "bbe_clean['publisher'] = bbe_clean['publisher'].astype(str).str.strip() \n",
    "unique_publisher = bbe_clean['publisher'].unique() \n",
    "\n",
    "print(f\"\\nTotal unique publisher values: {len(unique_publisher)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize numeric publishers names:\n",
    "def clean_numeric_publishers(x):\n",
    "    if re.match(r'^\\d+$', x.strip()):\n",
    "        return 'unknown'\n",
    "    return x\n",
    "\n",
    "bbe_clean['publisher'] = bbe_clean['publisher'].apply(clean_numeric_publishers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34928226",
   "metadata": {},
   "source": [
    "This cleaning step reduced the number of unique publisher names from **11,111 to 10,764**.\n",
    "Since **English-language books represent 81% of the catalogue**, the analysis will focus on this segment.\n",
    "We will **standardize major English-language publishing groups**, consolidating their **imprints and subsidiaries**, and apply **fuzzy matching** to unify names with **minor variations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load publishers dictionary\n",
    "with open(\"src/cleaning/mappings/publishers_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    publishers_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83945720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Get top 10000 most common publishers\n",
    "top_n = 10000\n",
    "publisher_counts = bbe_clean['publisher'].value_counts()\n",
    "top_publishers = publisher_counts.head(top_n).index.tolist()\n",
    "\n",
    "# Create a mapping for top publishers only\n",
    "standardization_map = {}\n",
    "processed = set()\n",
    "\n",
    "for pub in top_publishers:\n",
    "    if pub in processed:\n",
    "        continue\n",
    "    \n",
    "    # Find similar publishers in the top list\n",
    "    matches = process.extract(pub, top_publishers, scorer=fuzz.ratio, limit=5)\n",
    "    \n",
    "    # Group similar ones (score > 90)\n",
    "    similar = [m[0] for m in matches if m[1] > 90]\n",
    "    canonical = similar[0]  # Use first as canonical\n",
    "    \n",
    "    for similar_pub in similar:\n",
    "        standardization_map[similar_pub] = canonical\n",
    "        processed.add(similar_pub)\n",
    "\n",
    "# Apply the mapping\n",
    "bbe_clean['publisher_standardized'] = bbe_clean['publisher'].replace(standardization_map)\n",
    "\n",
    "# Then apply manual mapping\n",
    "bbe_clean['publisher_standardized'] = bbe_clean['publisher_standardized'].replace(publishers_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c7a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_unique_publisher = bbe_clean['publisher_standardized'].unique() \n",
    "\n",
    "print(f\"\\nTotal unique publisher values: {len(standardized_unique_publisher)}\\n\") \n",
    "\n",
    "print(\"Sample publishers:\")\n",
    "print(bbe_clean['publisher_standardized'].drop_duplicates().sample(30, random_state=42).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ec14d",
   "metadata": {},
   "source": [
    "The cleaning process reduced the number of unique publisher names from **11,111 to 9993**, representing a **10% decrease**.\n",
    "Given that the dataset includes books in multiple languages and many small or independent publishers, this reduction is a **satisfactory outcome**.\n",
    "\n",
    "To further evaluate the effectiveness of the cleaning, we will analyze the **proportion of titles associated with the most common publishers**.\n",
    "This will help us assess how well the standardization process **consolidated the publisher catalog** and captured the main publishing groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b78810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your core publisher groups\n",
    "major_publishers = [\n",
    "    'penguin random house', 'harpercollins', 'macmillan',\n",
    "    'simon & schuster', 'hachette', 'bloomsbury',\n",
    "    'amazon publishing', 'scholastic'\n",
    "]\n",
    "\n",
    "# Create a flag\n",
    "bbe_clean['is_major_publisher'] = bbe_clean['publisher_standardized'].isin(major_publishers)\n",
    "\n",
    "# Count results\n",
    "total_books = len(bbe_clean)\n",
    "major_books = bbe_clean['is_major_publisher'].sum()\n",
    "share_major = major_books / total_books * 100\n",
    "\n",
    "print(f\"Books from mapped major publishers: {major_books} of {total_books} ({share_major:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0672c79",
   "metadata": {},
   "source": [
    "About 17% of all titles now belong to one of the standardized major publisher groups.\n",
    "The remaining publishers represent independent, regional, or self-published works.\n",
    "Further improvements (e.g., mapping academic and international publishers) could expand this coverage to 25–30%. But we'll leave it as is for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd851a",
   "metadata": {},
   "source": [
    "**Book Format**\n",
    "\n",
    "This step standardizes the `bookFormat` field across multiple languages and inconsistent label variations found in the dataset.  \n",
    "The goal is to translate all format names into English and consolidate equivalent values (e.g., *“Capa dura”*, *“Gebundene Ausgabe”*, *“Hard back”*) under unified categories such as **Hardcover**, **Paperback**, **Ebook**, and **Audiobook**.\n",
    "\n",
    "This cleaning ensures that:\n",
    "- Format values are consistent for analysis and visualization.  \n",
    "- Non-English or rare variants are translated and grouped appropriately.  \n",
    "- Missing or unrecognized entries are handled under a neutral category: **Other / Unknown**.  \n",
    "\n",
    "By applying a mapping dictionary, we make the variable suitable for aggregation, comparison, and predictive modeling. After transformation, we verify the result by inspecting the number of unique standardized values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique format values \n",
    "bbe_clean['bookFormat'] = bbe_clean['bookFormat'].astype(str).str.strip() \n",
    "unique_format = bbe_clean['bookFormat'].unique() \n",
    "\n",
    "print(f\"\\nTotal unique book format values: {len(unique_format)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load format dictionary\n",
    "with open(\"src/cleaning/mappings/format_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    format_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631890b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['bookFormat_clean'] = (\n",
    "    bbe_clean['bookFormat']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .replace(format_dict)\n",
    ")\n",
    "\n",
    "# Replace remaining unknowns or NaN with a unified label\n",
    "bbe_clean['bookFormat_clean'] = bbe_clean['bookFormat_clean'].replace(['nan', 'none', ''], np.nan)\n",
    "bbe_clean['bookFormat_clean'] = bbe_clean['bookFormat_clean'].fillna('Other / Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c61a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_format_clean = bbe_clean['bookFormat_clean'].unique() \n",
    "\n",
    "print(f\"\\nTotal unique book format values: {len(unique_format_clean)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3adabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_format_clean[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a412a95",
   "metadata": {},
   "source": [
    "After applying the standardization mapping, the number of unique book format values was reduced from **135** to **10**.  This represents a substantial improvement in data consistency and interpretability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b535cfd",
   "metadata": {},
   "source": [
    "**ISBN and ASIN Cleaning**\n",
    "\n",
    "The BBE dataset includes a single `isbn` column, which initially contained numerous missing or invalid entries (e.g. placeholder values such as `9999999999999`).\n",
    "\n",
    "Our initial cleaning flow focused solely on standardizing **ISBN** values, but upon further inspection, we identified additional patterns such as **Amazon ASINs** (10-character alphanumeric codes) and prefixed identifiers like `10:` or `13:`.\n",
    "\n",
    "These findings led to an adjustment to the cleaning logic and the order of operations in the pipeline.\n",
    "\n",
    "The final cleaning process:\n",
    "\n",
    "- Removes punctuation and non-digit characters to standardize ISBN formatting.\n",
    "- Detects and separates ASINs (`asin` column) to preserve them for potential cross-dataset enrichment.\n",
    "- Handles prefixed identifiers (e.g., `13:9780615700`) by removing prefixes before validation.\n",
    "- Filters out placeholder or invalid entries (`999…`, `000…`) and ensures consistent string representation.\n",
    "- Creates a new `isbn_clean` column containing only valid ISBN-10 or ISBN-13 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36afe4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect ISBN column\n",
    "bbe_clean[['title','isbn']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing and invalid patterns\n",
    "n_missing_isbn = bbe_clean['isbn'].isna().sum()\n",
    "print(f'Number of missing ISBN entries: {n_missing_isbn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify invalid placeholders (like 9999999999999)\n",
    "n_invalid_isbn = bbe_clean[bbe_clean['isbn'].astype(str).str.contains('9999999999')].shape[0]\n",
    "print(f'Number of placeholder ISBN entries: {n_invalid_isbn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_asin(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    x = str(x).strip()\n",
    "    if re.fullmatch(r'[A-Z0-9]{10}', x) and not x.isdigit():  # must have at least one letter\n",
    "        return x\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e2970",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['asin'] = bbe_clean['isbn'].apply(detect_asin)\n",
    "has_asin = bbe_clean[bbe_clean['asin'].notna()] \n",
    "print(f'Books with ASINs: {len(has_asin)}')\n",
    "has_asin[['title','isbn', 'asin']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fabfea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_isbn(x):\n",
    "    # handle missing\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "\n",
    "    # detect ASIN first\n",
    "    asin_val = detect_asin(x)\n",
    "    if pd.notna(asin_val):\n",
    "        # return NaN for ISBN cleaning, because it's an ASIN\n",
    "        return np.nan  \n",
    "\n",
    "    # clean numeric ISBNs\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r'^(10:|13:)', '', s)       # remove leading prefixes\n",
    "    s = re.sub(r'\\D', '', s)               # keep only digits\n",
    "\n",
    "    # handle placeholders\n",
    "    if re.fullmatch(r'(9{10}|9{13}|0{10}|0{13})', s):\n",
    "        return np.nan\n",
    "\n",
    "    # keep valid ISBN-10 or ISBN-13\n",
    "    if len(s) in [10, 13]:\n",
    "        return s\n",
    "\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffc1c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['isbn_clean'] = bbe_clean['isbn'].apply(clean_isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b056cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect ISBN columns after cleaning\n",
    "bbe_clean[['title','isbn', 'isbn_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9baf8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_remaining = bbe_clean[bbe_clean['isbn_clean'].astype(str).str.fullmatch(r'(9{10}|9{13}|0{10}|0{13})', na=False)]\n",
    "print(f\"Remaining placeholder ISBNs: {len(placeholder_remaining)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where isbn_clean is NaN\n",
    "missing_isbn_clean = bbe_clean[bbe_clean['isbn_clean'].isna()]\n",
    "\n",
    "# Print the number of missing and show the first few examples\n",
    "print(f\"Missing isbn_clean: {missing_isbn_clean.shape[0]}\")\n",
    "missing_isbn_clean[['title', 'bookFormat', 'isbn', 'asin','isbn_clean']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31f098",
   "metadata": {},
   "source": [
    "To inspect if there are other cases of invalid ISBNs, we will filter the rows where the `isbn_type` is either `'wrong_length'` or `'missing'`. This will help us identify any additional issues with the ISBN data that may need to be addressed. For that a custom function `isbn_type` was created to classify the reason for invalidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isbn_type(x):\n",
    "    if pd.isna(x):\n",
    "        return 'missing'\n",
    "\n",
    "    s = str(x).strip()\n",
    "\n",
    "    # Detect ASIN (10-char alphanumeric, must have at least one letter)\n",
    "    if re.fullmatch(r'[A-Z0-9]{10}', s.upper()) and not s.isdigit():\n",
    "        return 'asin'\n",
    "\n",
    "    # Remove non-digits for numeric checks\n",
    "    x = re.sub(r'\\D', '', s)\n",
    "\n",
    "    # Placeholder patterns\n",
    "    if re.fullmatch(r'9{10}|9{13}', x):\n",
    "        return 'placeholder_9'\n",
    "    if re.fullmatch(r'0{10}|0{13}', x):\n",
    "        return 'placeholder_0'\n",
    "\n",
    "    # Length checks\n",
    "    if len(x) in [10, 13]:\n",
    "        return 'valid'\n",
    "    if len(x) > 0:\n",
    "        return 'wrong_length'\n",
    "\n",
    "    return 'missing'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeec353",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['isbn_type'] = bbe_clean['isbn'].apply(isbn_type)\n",
    "bbe_clean['isbn_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with type either 'wrong_length' or 'missing'\n",
    "invalid_isbn = bbe_clean[bbe_clean['isbn_type'].isin(['wrong_length', 'missing'])]\n",
    "\n",
    "# Show total count\n",
    "print(f\"Total invalid (wrong_length + missing): {invalid_isbn.shape[0]}\")\n",
    "\n",
    "# Preview relevant columns\n",
    "invalid_isbn[['title', 'author_clean', 'bookFormat', 'isbn', 'asin', 'isbn_type']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676aef5e",
   "metadata": {},
   "source": [
    "Out of all records, **9,081 entries (≈18%)** were identified as invalid ISBNs, leaving roughly **82%** valid.\n",
    "Only **34** cases were tagged as `'wrong_length'` and **1** as `'missing'`.\n",
    "These mostly represent truncated or prefixed identifiers, while the `isbn_type` function accurately distinguished valid ISBNs, ASINs, and placeholders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
