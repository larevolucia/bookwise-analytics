{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51fc4fb3",
   "metadata": {},
   "source": [
    "\n",
    "# Data Cleaning\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The purpose of this notebook is to **clean, standardize, and prepare the collected datasets** for subsequent exploratory analysis and modeling tasks.\n",
    "\n",
    "The goal is to transform raw inputs from multiple book datasets into a **reliable, consistent, and mergeable analytical base**, ensuring data integrity and comparability across platforms.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "| Dataset                    | Source                     | Description                                                               | Format |\n",
    "| -------------------------- | -------------------------- | ------------------------------------------------------------------------- | ------ |\n",
    "| `bbe_books.csv`            | Zenodo – *Best Books Ever* | Book metadata including title, author, rating, genres, and description.   | CSV    |\n",
    "| `books.csv`, `ratings.csv` | GitHub – *Goodbooks-10k*   | Book metadata and user–book interaction data for recommendation modeling. | CSV    |\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks in This Notebook\n",
    "\n",
    "This notebook will execute the following cleaning and preparation steps:\n",
    "\n",
    "1. **Standardize column formats:**\n",
    "   Ensure consistent data types and naming conventions across datasets (e.g., convert `isbn` to string, align `author`, `rating`, and `title` formats).\n",
    "\n",
    "2. **Clean and normalize missing values:**\n",
    "   Replace placeholder NaNs (`9999999999999`, empty lists, or `\"None\"`) with `np.nan`, then impute or drop based on analytical importance.\n",
    "\n",
    "3. **Detect and resolve duplicates:**\n",
    "   Identify duplicate records using key identifiers (`bookId`, `isbn`, `title + author`) and retain the most complete or relevant entries.\n",
    "\n",
    "4. **Validate and align categorical values:**\n",
    "   Standardize genre labels, language codes, and rating scales to ensure comparability between datasets.\n",
    "\n",
    "5. **Merge compatible datasets:**\n",
    "   Integrate *BestBooksEver* and *Goodbooks-10k_books* into a unified schema while maintaining referential integrity with the ratings dataset.\n",
    "\n",
    "6. **Outlier and consistency checks:**\n",
    "   Review numerical and date fields (e.g., `pages`, `price`, `publishDate`) for unrealistic or extreme values and adjust as needed.\n",
    "\n",
    "7. **Feature enrichment (optional):**\n",
    "   Derive or enhance fields such as `popularity_score`, `recency`, or missing genre information using external APIs where beneficial.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **Cleaned, schema-aligned datasets** ready for exploratory data analysis and modeling.\n",
    "* **Summary statistics** on completeness, duplicates, and outliers.\n",
    "* **Processed CSV files** saved for reproducibility in `data/processed/`.\n",
    "\n",
    "> **Note:** This notebook focuses on the *Data Cleaning and Preparation*. Further feature engineering and model-specific transformations will follow in later notebooks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383657df",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "## Navigate to the Parent Directory\n",
    "\n",
    "Before combining and saving datasets, it’s often helpful to move to a parent directory so that file operations (like loading or saving data) are easier and more organized. \n",
    "\n",
    "Before using the Python’s built-in os module to move one level up from the current working directory, it is advisable to inspect the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f'Current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f3220",
   "metadata": {},
   "source": [
    "To change to parent directory (root folder), run the code below. If you are already in the root folder, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c41cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to its parent\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print('Changed directory to parent.')\n",
    "\n",
    "# Get the new current working directory (the parent directory)\n",
    "current_dir = os.getcwd()\n",
    "print(f'New current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1120c",
   "metadata": {},
   "source": [
    "## Load and Inspect Books Datasets\n",
    "\n",
    "In this step, we load the previously collected datasets: **Goodbooks-10k** (books) and **Best Books Ever**. We will inspect their structure one more time before starting any merging or cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load datasets\n",
    "books_raw = pd.read_csv('data/raw/books.csv')\n",
    "bbe_raw = pd.read_csv('data/raw/bbe_books.csv')\n",
    "\n",
    "# create copies for cleaning\n",
    "books_clean = books_raw.copy()\n",
    "bbe_clean = bbe_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "interim_bbe_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "interim_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 0\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "books_clean.to_csv(interim_gb_path / f\"books_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "display(bbe_clean.head(3))\n",
    "display(books_clean.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape and missing values\n",
    "for name, df in {'BBE': bbe_clean, 'Books': books_clean,}.items():\n",
    "    print(f\"\\n{name} — Shape: {df.shape}\")\n",
    "    print(df.info())\n",
    "    print(df.isna().sum().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b59d8",
   "metadata": {},
   "source": [
    "We will check if the datasets share common identifiers and compatible data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4382d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_only_columns = set(bbe_clean.columns) - set(books_clean.columns)\n",
    "print(f'Columns only in BBE: {bbe_only_columns}')\n",
    "\n",
    "goodbooks_only_columns = set(books_clean.columns) - set(bbe_clean.columns)\n",
    "print(f'Columns only in Goodbooks: {goodbooks_only_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66be32",
   "metadata": {},
   "source": [
    "Based on the initial inspection, we can create a mapping table to align columns from both datasets for merging and analysis.\n",
    "\n",
    "| **BestBooksEver (BBE)** | **Goodbooks10k_books (GB10k)** | **Notes / Alignment Rationale** |\n",
    "| --------------------------------- | ------------------------------------------------ | -------------------------------------------------------------------------- |\n",
    "| `bookId` | `book_id` | Identifiers in eachlist. |\n",
    "| `goodreads_id_clean` | `goodreads_id_clean` | Goodreads identifier; ensure both are numeric for joining. |\n",
    "| `title` | `title` | Direct match. Used as secondary join key. |\n",
    "| `series` | — | Only in BBE; could enrich GB10k if available via API. |\n",
    "| `author` | `authors` | Same meaning. Normalize format. |\n",
    "| `rating` | `average_rating` | Equivalent — rename to unified `average_rating`. |\n",
    "| `numRatings` | `ratings_count` | Same measure of total user ratings. |\n",
    "| `ratingsByStars` | `ratings_1` … `ratings_5` | BBE has dict, GB10k has explicit columns. Expand or aggregate accordingly. |\n",
    "| `likedPercent` | — | BBE-only; optional metric of user sentiment. |\n",
    "| `isbn` | `isbn` / `isbn13` | Common linking key; keep both (string). Use for merges when present. |\n",
    "| `language` | `language_code` | Standardize to ISO 639-1 (lowercase). |\n",
    "| `description` | — | BBE-only; valuable for NLP features. |\n",
    "| `genres` | — | BBE-only; can enrich GB10k tags later. |\n",
    "| `characters` | — | BBE-only; low modeling priority, but could add narrative metadata. |\n",
    "| `bookFormat` | — | BBE-only; possible categorical feature. |\n",
    "| `edition` | — | BBE-only; low modeling priority. |\n",
    "| `pages` | — | BBE-only; numeric, may enrich GB10k metadata. |\n",
    "| `publisher` | — | BBE-only; possible future feature. |\n",
    "| `publishDate` | — | BBE-only; can approximate from GB10k’s `original_publication_year`. |\n",
    "| `firstPublishDate` | `original_publication_year` | Equivalent (date vs year). |\n",
    "| `coverImg` | `image_url` / `small_image_url` | Same function (cover link), to be dropped. |\n",
    "| `bbeScore` | — | BBE-only; internal popularity score. |\n",
    "| `bbeVotes` | `work_ratings_count` | Comparable as popularity proxy. |\n",
    "| `price` | — | BBE-only; likely non-essential for satisfaction prediction. |\n",
    "| `setting` | — | BBE-only; can support content enrichment. |\n",
    "| `awards` | — | BBE-only; categorical enrichment. |\n",
    "| — | `best_book_id` / `work_id` | GB10k-only identifiers; may be used for deeper Goodreads linking. |\n",
    "| — | `books_count` | GB10k-only; number of editions per work. |\n",
    "| — | `work_text_reviews_count` | GB10k-only; can complement `numRatings` as engagement metric. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5069ac1",
   "metadata": {},
   "source": [
    "# Data Cleaning Steps\n",
    "\n",
    "### Best Books Ever\n",
    "\n",
    "- Handle identifier columns\n",
    "- Standardize key columns: `author`, `language`\n",
    "- Missing data handling strategies\n",
    "- Normalize genre and format\n",
    "- Validate for no nulls or duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c6e5f",
   "metadata": {},
   "source": [
    "#### **bookId / goodreads_id_clean**\n",
    "\n",
    "As part of the **Data Collection** phase, we extracted numeric identifiers from the `bookId` field in the **Best Books Ever (BBE)** dataset to create a new column, `goodreads_id_clean`, ensuring full compatibility with the `goodreads_id_clean` field from the **Goodbooks-10k** dataset. Both columns were standardized to numeric types and validated for completeness and cross-dataset alignment, enabling accurate merging and identifier matching in subsequent analyses.\n",
    "\n",
    "In this **Data Preparation** stage, we revalidated the identifier field to confirm data integrity. The following checks were performed:\n",
    "\n",
    "* **Completeness:** No missing (`NaN`) values were found.\n",
    "* **Uniqueness:** Only **54 duplicates** out of **52,478 records** (0.1%) were detected, likely due to multi-edition entries or duplicate titles referencing the same Goodreads ID.\n",
    "* **Data type:** All values are stored as `float64` and represent valid numeric identifiers.\n",
    "* **Range validation:** IDs span from **1** to **55,726,130**, consistent with Goodreads’ typical numeric ID structure.\n",
    "\n",
    "These verifications confirm that the identifier cleaning from the Data Collection phase remains consistent and reliable. No further transformation is required at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0096f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify existing identifier cleaning\n",
    "print(\"Missing values:\", bbe_clean['goodreads_id_clean'].isna().sum())\n",
    "print(\"Duplicates:\", bbe_clean['goodreads_id_clean'].duplicated().sum())\n",
    "print(\"Unique IDs:\", bbe_clean['goodreads_id_clean'].nunique())\n",
    "bbe_clean['goodreads_id_clean'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ae569",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['goodreads_id_clean'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45a6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['title','goodreads_id_clean']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da358f9",
   "metadata": {},
   "source": [
    "The small number of duplicate `goodreads_id_clean` entries might reflect legitimate cases where multiple editions or formats (e.g., paperback, e-book, reprint) share the same Goodreads identifier. We will check if the rows are identical comparing the values in checking key fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many of the duplicate rows are fully identical across key metadata fields\n",
    "key_fields = ['goodreads_id_clean', 'title', 'author', 'language', 'bookFormat', 'isbn', 'publishDate', 'firstPublishDate']\n",
    "\n",
    "# Create a mask for perfect duplicates\n",
    "exact_dupes_mask = bbe_clean.duplicated(subset=key_fields, keep=False)\n",
    "\n",
    "# Filter and view them\n",
    "exact_dupes = bbe_clean[exact_dupes_mask].sort_values(by='goodreads_id_clean')\n",
    "\n",
    "print(f\"Exact duplicates found: {exact_dupes.shape[0]}\")\n",
    "print(f\"Unique duplicated IDs (fully identical records): {exact_dupes['goodreads_id_clean'].nunique()}\")\n",
    "\n",
    "# Preview a few to verify\n",
    "exact_dupes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e004023",
   "metadata": {},
   "source": [
    "After inspecting the duplicate `goodreads_id_clean` entries, we confirmed that a small subset of rows were fully identical across all key metadata fields (title, author, language, format, ISBN, and publish date). These represented true redundancies rather than alternate editions. To preserve dataset integrity and avoid bias in downstream analyses, we removed these exact duplicates while retaining one instance per unique record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ddfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean = bbe_clean.drop_duplicates(subset=key_fields, keep='first').reset_index(drop=True)\n",
    "print(\"Remaining records after removing exact duplicates:\", len(bbe_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to drop columns\n",
    "drop_bbe_cols = ['bookId', 'goodreads_book_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150b46b",
   "metadata": {},
   "source": [
    "#### **Author**\n",
    "\n",
    "We will proceed with the standardization of key columns, starting with the `author` column. The author column in the BBE dataset often contains a qualifier such as \"(Goodreads Author)\". We will remove such qualifiers to standardize the format. We will also create an additional list column to store multiple authors as a list rather than a single string. This way, its is ready to use for feature engineering later on if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_split_authors(name):\n",
    "    \"\"\"\n",
    "    Cleans author names and returns a list of authors.\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "\n",
    "    # Remove role descriptors\n",
    "    cleaned = re.sub(r\"\\s*\\([^)]*\\)\", \"\", name)\n",
    "    \n",
    "    # split on commas and lowercase each name\n",
    "    authors_list = [a.strip().lower() for a in cleaned.split(\",\") if a.strip()]\n",
    "        \n",
    "    return authors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77187ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to BestBooksEver dataset\n",
    "bbe_clean[\"authors_list\"] = bbe_clean[\"author\"].apply(clean_and_split_authors)\n",
    "bbe_clean[\"author_clean\"] = bbe_clean[\"authors_list\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else None)\n",
    "\n",
    "# Quick check\n",
    "bbe_clean[[\"author\", \"author_clean\", \"authors_list\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.append('author')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 1\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f77069",
   "metadata": {},
   "source": [
    "#### **Title**\n",
    "\n",
    "To ensure consistent and reproducible page-count retrieval from the Google Books API, we cleaned the `title` column into a new feature: `title_clean`. After cleaning we will identify any missing title and deduplicate entries, if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['title']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb312f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean book titles for Google Books API queries.\n",
    "    \n",
    "    Steps:\n",
    "    - Remove bracketed or parenthetical notes (e.g. '(Box Set)', '[Hardcover]')\n",
    "    - Remove common edition/format words\n",
    "    - Remove ellipsis or truncated markers\n",
    "    - Normalize spaces and lowercase\n",
    "    \"\"\"\n",
    "    if not isinstance(title, str):\n",
    "        return np.nan\n",
    "\n",
    "    \n",
    "    text = title.strip()\n",
    "    \n",
    "    # remove bracketed or parenthetical content\n",
    "    text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    \n",
    "    # remove common edition/format terms\n",
    "    text = re.sub(r\"\\b(box set|collection|illustrated|edition|volume|vol\\.|book\\s*\\d+)\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # remove ellipsis or truncation markers\n",
    "    text = text.replace(\"...\", \"\")\n",
    "    \n",
    "    # remove extra punctuation and multiple spaces\n",
    "    text = re.sub(r\"[^\\w\\s'\\-]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # lowercase for API consistency\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['title_clean'] = bbe_clean['title'].apply(clean_title)\n",
    "bbe_clean[['title_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask for missing titles\n",
    "mask_no_title = bbe_clean['title_clean'].isna() | (bbe_clean['title_clean'] == \"\")\n",
    "\n",
    "# filter those rows\n",
    "no_title_books = bbe_clean[mask_no_title]\n",
    "\n",
    "# display summary\n",
    "print(f\"Books without title: {no_title_books.shape[0]}\")\n",
    "\n",
    "# inspect identifier columns if they exist\n",
    "identifier_cols = ['isbn', 'goodreads_id_clean'] \n",
    "print(\"\\nAvailable identifiers for missing-title rows:\")\n",
    "print(no_title_books[identifier_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6b1eb",
   "metadata": {},
   "source": [
    "After cleaning the title field into `title_clean`, we initially thought of applying filtering by English-language books at this stage. We decided to postpone this step after imputation, to ensure we don't exclude titles that could be maintained after handling 'unknown' language values.\n",
    "\n",
    "We moved to the next step, keeping only one version of each title per author, prioritizing the record with the highest `numRatings`. This ensures that the retained record represents the most widely rated (and therefore most validated) version of the book, avoiding redundancy while preserving author attribution. By checking both `title_clean` and `author_clean`, we safeguard against incorrectly dropping books with similar titles but different authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98417fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicated raw titles (case-insensitive)\n",
    "dup_raw = bbe_clean[bbe_clean.duplicated(subset=['title_clean'], keep=False)]\n",
    "\n",
    "# Sort for quick inspection\n",
    "dup_raw.sort_values('title_clean').head(20)[\n",
    "    ['title_clean', 'author_clean', 'language', 'rating', 'numRatings']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5961c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates within (title, author, language) groups\n",
    "bbe_dedup = (\n",
    "    bbe_clean.sort_values('numRatings', ascending=False)\n",
    "             .drop_duplicates(subset=['title_clean', 'author_clean', 'language'], keep='first')\n",
    "             .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Books before deduplication: {len(bbe_clean)}\")\n",
    "print(f\"Books after deduplication: {len(bbe_dedup)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75358e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few examples of previously duplicated titles\n",
    "bbe_dedup.sort_values(['title_clean', 'author_clean']).head(10)[\n",
    "    ['title_clean', 'author_clean', 'language', 'rating', 'numRatings']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a04e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.append('title')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 2\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe51531",
   "metadata": {},
   "source": [
    "#### **ISBN and ASIN Cleaning**\n",
    "\n",
    "The BBE dataset includes a single `isbn` column, which initially contained numerous missing or invalid entries (e.g. placeholder values such as `9999999999999`).\n",
    "\n",
    "Our initial cleaning flow focused solely on standardizing **ISBN** values, but upon further inspection, we identified additional patterns such as **Amazon ASINs** (10-character alphanumeric codes) and prefixed identifiers like `10:` or `13:`.\n",
    "\n",
    "These findings led to an adjustment to the cleaning logic and the order of operations in the pipeline.\n",
    "\n",
    "The final cleaning process:\n",
    "\n",
    "- Removes punctuation and non-digit characters to standardize ISBN formatting.\n",
    "- Detects and separates ASINs (`asin` column) to preserve them for potential cross-dataset enrichment.\n",
    "- Handles prefixed identifiers (e.g., `13:9780615700`) by removing prefixes before validation.\n",
    "- Filters out placeholder or invalid entries (`999…`, `000…`) and ensures consistent string representation.\n",
    "- Creates a new `isbn_clean` column containing only valid ISBN-10 or ISBN-13 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb26af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect ISBN column\n",
    "bbe_clean[['title_clean','isbn']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad32dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing and invalid patterns\n",
    "n_missing_isbn = bbe_clean['isbn'].isna().sum()\n",
    "print(f'Number of missing ISBN entries: {n_missing_isbn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify invalid placeholders (like 9999999999999)\n",
    "n_invalid_isbn = bbe_clean[bbe_clean['isbn'].astype(str).str.contains('9999999999')].shape[0]\n",
    "print(f'Number of placeholder ISBN entries: {n_invalid_isbn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b64e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def detect_asin(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    x = str(x).strip()\n",
    "    if re.fullmatch(r'[A-Z0-9]{10}', x) and not x.isdigit():  # must have at least one letter\n",
    "        return x\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['asin'] = bbe_clean['isbn'].apply(detect_asin)\n",
    "has_asin = bbe_clean[bbe_clean['asin'].notna()] \n",
    "print(f'Books with ASINs: {len(has_asin)}')\n",
    "has_asin[['title_clean','isbn', 'asin']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_isbn(x):\n",
    "    # handle missing\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "\n",
    "    # detect ASIN first\n",
    "    asin_val = detect_asin(x)\n",
    "    if pd.notna(asin_val):\n",
    "        # return NaN for ISBN cleaning, because it's an ASIN\n",
    "        return np.nan  \n",
    "\n",
    "    # clean numeric ISBNs\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r'^(10:|13:)', '', s)       # remove leading prefixes\n",
    "    s = re.sub(r'\\D', '', s)               # keep only digits\n",
    "\n",
    "    # handle placeholders\n",
    "    if re.fullmatch(r'(9{10}|9{13}|0{10}|0{13})', s):\n",
    "        return np.nan\n",
    "\n",
    "    # keep valid ISBN-10 or ISBN-13\n",
    "    if len(s) in [10, 13]:\n",
    "        return s\n",
    "\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f965a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['isbn_clean'] = bbe_clean['isbn'].apply(clean_isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318caa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect ISBN columns after cleaning\n",
    "bbe_clean[['title_clean','isbn', 'isbn_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_remaining = bbe_clean[bbe_clean['isbn_clean'].astype(str).str.fullmatch(r'(9{10}|9{13}|0{10}|0{13})', na=False)]\n",
    "print(f\"Remaining placeholder ISBNs: {len(placeholder_remaining)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1996ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where isbn_clean is NaN\n",
    "missing_isbn_clean = bbe_clean[bbe_clean['isbn_clean'].isna()]\n",
    "\n",
    "# Print the number of missing and show the first few examples\n",
    "print(f\"Missing isbn_clean: {missing_isbn_clean.shape[0]}\")\n",
    "missing_isbn_clean[['title', 'bookFormat', 'isbn', 'asin','isbn_clean']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c9fe3",
   "metadata": {},
   "source": [
    "To inspect if there are other cases of invalid ISBNs, we will filter the rows where the `isbn_type` is either `'wrong_length'` or `'missing'`. This will help us identify any additional issues with the ISBN data that may need to be addressed. For that a custom function `isbn_type` was created to classify the reason for invalidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe9505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isbn_type(x):\n",
    "    if pd.isna(x):\n",
    "        return 'missing'\n",
    "\n",
    "    s = str(x).strip()\n",
    "\n",
    "    # Detect ASIN (10-char alphanumeric, must have at least one letter)\n",
    "    if re.fullmatch(r'[A-Z0-9]{10}', s.upper()) and not s.isdigit():\n",
    "        return 'asin'\n",
    "\n",
    "    # Remove non-digits for numeric checks\n",
    "    x = re.sub(r'\\D', '', s)\n",
    "\n",
    "    # Placeholder patterns\n",
    "    if re.fullmatch(r'9{10}|9{13}', x):\n",
    "        return 'placeholder_9'\n",
    "    if re.fullmatch(r'0{10}|0{13}', x):\n",
    "        return 'placeholder_0'\n",
    "\n",
    "    # Length checks\n",
    "    if len(x) in [10, 13]:\n",
    "        return 'valid'\n",
    "    if len(x) > 0:\n",
    "        return 'wrong_length'\n",
    "\n",
    "    return 'missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faabbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['isbn_type'] = bbe_clean['isbn'].apply(isbn_type)\n",
    "bbe_clean['isbn_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ebc47",
   "metadata": {},
   "source": [
    "The `isbn_type` function accurately distinguished valid ISBNs, ASINs, and placeholders and can be used to validate the data cleaning pipeline. We can see that when applied to the original `isbn` column, we have **4,684** ASIN, **4,350** placeholders, **34** entries in the wrong lenght due to preffixes and **1** missing value. After cleaning, we see that there are **9,066** NaN values (missing) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['isbn_type'] = bbe_clean['isbn_clean'].apply(isbn_type)\n",
    "bbe_clean['isbn_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with type either 'missing'\n",
    "invalid_isbn = bbe_clean[bbe_clean['isbn_type'].isin(['missing'])]\n",
    "\n",
    "# Show total count\n",
    "print(f\"Total invalid (missing): {invalid_isbn.shape[0]}\")\n",
    "\n",
    "# Preview relevant columns\n",
    "invalid_isbn[['title', 'author', 'bookFormat', 'isbn', 'asin', 'isbn_type']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96efa8ac",
   "metadata": {},
   "source": [
    "Out of all records, **9,066 entries (≈18%)** were identified as invalid ISBNs, leaving roughly **82%** valid.\n",
    "Invalid cases, after cleaning are limited to the `'missing'` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.extend(['isbn', 'isbn_type'])\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 3\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef2553",
   "metadata": {},
   "source": [
    "#### **Language**\n",
    "\n",
    "The `language` column in the Best Books Ever dataset used full names such as “English”, “German”, and “Arabic”.  Before transforming the values, we will check for all unique values to identify any unexpected entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique language values\n",
    "print(\"Unique language values in BBE dataset:\")\n",
    "bbe_clean['language'] = bbe_clean['language'].astype(str).str.strip()\n",
    "unique_languages = bbe_clean['language'].unique()\n",
    "\n",
    "print(f\"\\nTotal unique values: {len(unique_languages)}\\n\")\n",
    "print(unique_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d9853",
   "metadata": {},
   "source": [
    "We can see that there are some unexpected values such as:\n",
    "- _historical forms_ (“English, Middle (1100-1500)”, “French, Middle (ca.1400-1600)”)\n",
    "- _combined or semicolon-separated entries_ (“Filipino; Pilipino”, “Catalan; Valencian”)\n",
    "- _multi-language / uncertain cases_ (“Multiple languages”, “Undetermined”)\n",
    "- _rare or dialects_ (“Bokmål, Norwegian; Norwegian Bokmål”, “Aromanian; Arumanian; Macedo-Romanian”)\n",
    "\n",
    "We will clean the unusual entries by mapping them to the closest language present in the ISO 639-1 standard. Unrecognized values will be flagged and replaced with `\"unknown\"`. It was decided to distinguish the `\"unknown\"` from the `NaN` values to retain information about missingness versus unrecognized entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a83c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Standardize capitalization & spacing\n",
    "bbe_clean['language'] = bbe_clean['language'].astype(str).str.strip().str.title()\n",
    "\n",
    "# Handle NaNs that became strings\n",
    "bbe_clean['language'] = bbe_clean['language'].replace({'Nan': np.nan})\n",
    "\n",
    "# Simplify and unify multi-language / dialect forms\n",
    "replace_map = {\n",
    "    'Multiple Languages': 'Multilingual',\n",
    "    'Undetermined': 'Unknown',\n",
    "    'Iranian (Other)': 'Persian',\n",
    "    'Farsi': 'Persian',\n",
    "    'Filipino; Pilipino': 'Filipino',\n",
    "    'Catalan; Valencian': 'Catalan',\n",
    "    'Panjabi; Punjabi': 'Punjabi',\n",
    "    'Bokmål, Norwegian; Norwegian Bokmål': 'Norwegian',\n",
    "    'Norwegian Nynorsk; Nynorsk, Norwegian': 'Norwegian',\n",
    "    'Greek, Modern (1453-)': 'Greek',\n",
    "    'Greek, Ancient (To 1453)': 'Greek',\n",
    "    'French, Middle (Ca.1400-1600)': 'French',\n",
    "    'English, Middle (1100-1500)': 'English',\n",
    "    'Dutch, Middle (Ca.1050-1350)': 'Dutch',\n",
    "    'Aromanian; Arumanian; Macedo-Romanian': 'Romanian',\n",
    "    'Mayan Languages': 'Mayan',\n",
    "    'Australian Languages': 'English'\n",
    "}\n",
    "\n",
    "bbe_clean['language'] = bbe_clean['language'].replace(replace_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5060ad13",
   "metadata": {},
   "source": [
    "After transforming the values, we apply a mapping to standardize the `language` column using **ISO 639-1 two-letter codes**.\n",
    "The mapping dictionaries are stored in the `src/cleaning/mappings/` folder to keep the notebooks cleaner and improve readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37289380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"src/cleaning/mappings/languages_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    languages_dict = json.load(f)\n",
    "\n",
    "# Apply dictionary\n",
    "bbe_clean['language_clean'] = bbe_clean['language'].str.lower().map(languages_dict)\n",
    "\n",
    "# Fill remaining NaNs\n",
    "bbe_clean['language_clean'] = bbe_clean['language_clean'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again for unique language values\n",
    "print(\"Unique language values in BBE dataset:\")\n",
    "unique_languages = bbe_clean['language_clean'].unique()\n",
    "\n",
    "print(f\"\\nTotal unique values: {len(unique_languages)}\\n\")\n",
    "print(unique_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_breakdown = (\n",
    "    bbe_clean['language_clean']\n",
    "    .value_counts()\n",
    "    .to_frame('count')\n",
    ")\n",
    "\n",
    "language_breakdown['percentage'] = (\n",
    "    language_breakdown['count'] / len(bbe_clean) * 100\n",
    ").round(2)\n",
    "\n",
    "print(language_breakdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99afa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.append('language')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 4\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c6b8f9",
   "metadata": {},
   "source": [
    "#### **Dates**\n",
    "\n",
    "BBE dataset has two publication fields: `publishDate` and `firstPublishDate`. The `firstPublishDate` represents the original publication date, while `publishDate` refers to a more recent edition or reprint date. Publishing experts assumption is that the recency of the `firstPublishDate` is more relevant for modeling book satisfaction, as it reflects when the book was first introduced to readers. Therefore, we will focus on cleaning and standardizing the `firstPublishDate` column and use `publishDate` only if `firstPublishDate` is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2685c",
   "metadata": {},
   "source": [
    "While majority of the dates follow the 'MM/DD/YY' format, after a first attemp at cleaning, we noticed some dates do not conform to this format. Therefore, we will implement a more robust date parsing strategy, focusing first on transforming textual formats into 'MM/DD/YYYY' format before attempting to parse them into datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6d327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "def clean_date_string(date_str):\n",
    "    \"\"\"Remove ordinal suffixes and unwanted characters from a date string.\"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return np.nan\n",
    "    # remove st, nd, rd, th (like 'April 27th 2010' → 'April 27 2010')\n",
    "    cleaned = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', str(date_str))\n",
    "    return cleaned.strip()\n",
    "\n",
    "def parse_mixed_date(date_str):\n",
    "    \"\"\"Try to parse a variety of date formats safely.\"\"\"\n",
    "    if pd.isna(date_str) or date_str == '':\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Use dateutil to parse most human-readable formats\n",
    "        return parser.parse(date_str, fuzzy=True)\n",
    "    except Exception:\n",
    "        # Try year-only fallback (e.g. '2003')\n",
    "        match = re.match(r'^\\d{4}$', str(date_str))\n",
    "        if match:\n",
    "            return pd.to_datetime(f\"{date_str}-01-01\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7cc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to both columns\n",
    "for col in ['firstPublishDate', 'publishDate']:\n",
    "       bbe_clean[f'{col}_clean'] = (\n",
    "        bbe_clean[col]\n",
    "        .astype(str)\n",
    "        .replace({'nan': np.nan, '': np.nan})\n",
    "        .apply(clean_date_string)\n",
    "        .apply(parse_mixed_date)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0493da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine using your logic: prefer firstPublishDate, else publishDate\n",
    "bbe_clean['publication_date_clean'] = (\n",
    "    bbe_clean['firstPublishDate_clean'].combine_first(bbe_clean['publishDate_clean'])\n",
    ")\n",
    "# Reconvert to datetime safely before using .dt\n",
    "bbe_clean['publication_date_clean'] = pd.to_datetime(bbe_clean['publication_date_clean'], errors='coerce')\n",
    "\n",
    "# Format as ISO standard\n",
    "bbe_clean['publication_date_clean'] = bbe_clean['publication_date_clean'].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Check a sample of remaining nulls\n",
    "bbe_clean[bbe_clean['publication_date_clean'].isna()][['title_clean', 'firstPublishDate', 'publishDate', 'publication_date_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the unified publication date is missing\n",
    "total = len(bbe_clean)\n",
    "bbe_missing_dates = bbe_clean.loc[bbe_clean['publication_date_clean'].isna()]\n",
    "missing_count = len(bbe_missing_dates)\n",
    "\n",
    "print(f\"Missing publication dates: {missing_count} of {total} ({missing_count/total:.2%})\")\n",
    "\n",
    "# Preview key columns\n",
    "bbe_missing_dates[['title_clean', 'author_clean', 'firstPublishDate', 'publishDate', 'publication_date_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.extend(['firstPublishDate', 'publishDate', 'firstPublishDate_clean', 'publishDate_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 5\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f7a9c",
   "metadata": {},
   "source": [
    "#### **Publisher**\n",
    "\n",
    "Publisher names vary widely in formatting, including punctuation, spacing, and imprint-level differences. We normalized the `publisher` column by lowercasing, trimming whitespace, removing noise characters, and filtering numeric placeholders.\n",
    "\n",
    "This initial cleaning reduced unique publisher names from **11,110 to 10,724**, providing a cleaner base for further standardization. Because **81% of the catalogue is English**, subsequent steps focus on major English-language publishers. Fuzzy matching was applied to merge near-duplicate names before imprint consolidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw unique publishers:\", bbe_clean[\"publisher\"].nunique())\n",
    "bbe_clean[\"publisher_clean\"] = (\n",
    "    bbe_clean[\"publisher\"]\n",
    "    .replace([\"\", \"nan\", \"NaN\"], np.nan)\n",
    "    .astype(\"string\")\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(r'[\\\"\\'.]', '', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop numeric-only publishers\n",
    "bbe_clean[\"publisher_clean\"] = bbe_clean[\"publisher_clean\"].apply(\n",
    "    lambda x: np.nan if pd.notna(x) and re.fullmatch(r\"\\d+\", x) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1df11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "# fuzzy dedupe top N publishers\n",
    "top_publishers = (\n",
    "    bbe_clean[\"publisher_clean\"]\n",
    "    .dropna()\n",
    "    .value_counts()\n",
    "    .head(800)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "fuzzy_map = {}\n",
    "\n",
    "for p in top_publishers:\n",
    "    if p in fuzzy_map:\n",
    "        continue\n",
    "    matches = process.extract(\n",
    "        p, top_publishers, scorer=fuzz.WRatio, limit=8\n",
    "    )\n",
    "    sims = [m[0] for m in matches if m[1] >= 92]\n",
    "    for s in sims:\n",
    "        fuzzy_map[s] = p  # p becomes canonical form\n",
    "\n",
    "bbe_clean[\"publisher_clean\"] = bbe_clean[\"publisher_clean\"].replace(fuzzy_map)\n",
    "print(\"Cleaned unique publishers:\", bbe_clean[\"publisher_clean\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb66fab",
   "metadata": {},
   "source": [
    "\n",
    "To unify imprints and naming variants under major publishing groups, we applied a keyword-based mapping that replaces matched entries with a single parent label (e.g., *penguin random house*, *harpercollins*, *macmillan*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f895da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load manual mapping\n",
    "with open(\"src/cleaning/mappings/publishers_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    manual_map = json.load(f)\n",
    "\n",
    "bbe_clean[\"publisher_clean\"] = bbe_clean[\"publisher_clean\"].replace(manual_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afbf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main publishers and their keyword patterns\n",
    "publisher_patterns = {\n",
    "    # --- Trade publishers ---\n",
    "    \"harpercollins\": [\n",
    "        \"harpercollins\", \"harlequin\", \"harper\", \"william morrow\",\n",
    "        \"avon\", \"balzer\", \"bray\", \"ecco\", \"mariner\", \"harper\"\n",
    "        \"hmh\", \"katherine tegen\", \"harperteen\", \"broadside books\"\n",
    "    ],\n",
    "    \"penguin random house\": [\n",
    "        \"penguin\", \"random house\", \"knopf\", \"vintage\", \"bantam\",\n",
    "        \"crown\", \"puffin\", \"dorling kindersley\", \"dk\", \"ballantine\",\n",
    "        \"doubleday\", \"del rey\", \"berkley\", \"signet\", \"ace\", \"delacorte\",\n",
    "        \"anchor\", \"daw\", \"jove\", \"viking\", \"modern library\", \"riverhead\",\n",
    "        \"spectra\", \"razorbill\", \"philomel\",\"putnam\", \"dial books\"\n",
    "    ],\n",
    "    \"hachette\": [\n",
    "        \"hachette\", \"little brown\", \"orbit\", \"grand central\",\n",
    "        \"mulholland\", \"headline\", \"little, brown\", \"basic books\",\n",
    "        \"publicaffairs\", \"faithwords\", \"hodder\", \"yen press\",\n",
    "        \"quercus\"\n",
    "    ],\n",
    "    \"macmillan\": [\n",
    "        \"macmillan\", \"picador\", \"tor\", \"farrar straus\", \"farrar, straus\",\n",
    "        \"fsg\", \"st. martin\", \"holt\", \"metropolitan books\", \"griffin\",\n",
    "        \"starscape\", \"roaring brook\", \"square fish\", \"flatiron\",\n",
    "        \"feiwel and friends\", \"minotaur books\", \"forge\", \"tom doherty\"\n",
    "    ],\n",
    "    \"simon & schuster\": [\n",
    "        \"simon schuster\", \"simon & schuster\", \"atria\", \"gallery books\",\n",
    "        \"pocket books\", \"scribner\", \"touchstone\", \"simon pulse\",\n",
    "        \"aladdin\", \"west gallery\", \"atheneum\", \n",
    "        \"gallery/scout press\", \"free press\"\n",
    "    ],\n",
    "    \"bloomsbury\": [\n",
    "        \"bloomsbury\", \"walker\", \"walker books\",\n",
    "    ],\n",
    "    \"scholastic\": [\n",
    "        \"scholastic\", \"arthur a. levine\", \"graphix\", \"levine\", \"blue sky\"\n",
    "    ],\n",
    "    \"amazon publishing\": [\n",
    "        \"amazoncrossing\",\"amazon crossing\", \"little a\", \"lake union\",\n",
    "        \"montlake\", \"skyscape\", \"two lions\", \"47north\",\n",
    "        \"topple\", \"mindy's\", \"amazonencore\", \"amazon encore\",\n",
    "        \"amazon original stories\"\n",
    "    ],\n",
    "\n",
    "    # --- academic publishers ---\n",
    "    \"oxford university press\": [\n",
    "        \"oxford university press\", \"oxford up\", \"oxford\", \n",
    "    ],\n",
    "    \"cambridge university press\": [\n",
    "        \"cambridge university press\", \"cambridge up\"\n",
    "    ],\n",
    "    \"wiley\": [\n",
    "        \"wiley\", \"john wiley\", \"john wiley & sons\", \"wiley-blackwell\"\n",
    "    ],\n",
    "    \"taylor & francis\": [\n",
    "        \"taylor & francis\", \"taylor and francis\", \"t&f\", \"crc press\",\n",
    "        \"routledge\", \"psychology press\", \"crc\"\n",
    "    ],\n",
    "    \"springer nature\": [\n",
    "        \"springer\", \"springer nature\", \"nature publishing\", \"palgrave\",\n",
    "        \"palgrave macmillan\", \"apl\", \"springer-verlag\", \"birkhäuser\",\n",
    "    ],\n",
    "    \"elsevier\": [\n",
    "        \"elsevier\", \"butterworth heinemann\",\n",
    "        \"morgan kaufmann\", \"north holland\",\n",
    "        \"academic press\" \n",
    "    ],\n",
    "    \"sage\": [\n",
    "        \"sage\", \"sage publishing\", \"sage publications\"\n",
    "    ],\n",
    "    \"mit press\": [\n",
    "        \"mit press\", \"massachusetts institute of technology press\"\n",
    "    ],\n",
    "    \"princeton university press\": [\n",
    "        \"princeton university press\", \"pup\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_parent(pub):\n",
    "    if pd.isna(pub):\n",
    "        return pub\n",
    "    for parent, kws in publisher_patterns.items():\n",
    "        for kw in kws:\n",
    "            if kw in pub:\n",
    "                return parent\n",
    "    return pub\n",
    "\n",
    "bbe_clean[\"publisher_clean\"] = bbe_clean[\"publisher_clean\"].apply(assign_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute final coverage\n",
    "major_publishers = list(publisher_patterns.keys())\n",
    "\n",
    "bbe_clean[\"is_major_publisher\"] = bbe_clean[\"publisher_clean\"].isin(major_publishers)\n",
    "\n",
    "total = len(bbe_clean)\n",
    "major = bbe_clean[\"is_major_publisher\"].sum()\n",
    "english = (bbe_clean[\"language_clean\"] == \"en\").sum()\n",
    "\n",
    "print(f\"Major-publisher books (all languages): \"\n",
    "      f\"{major}/{total} ({major/total*100:.2f}%)\")\n",
    "\n",
    "print(f\"Major-publisher books (English only): \"\n",
    "      f\"{major}/{english} ({major/english*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nRemaining unique publishers:\", bbe_clean[\"publisher_clean\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0672c79",
   "metadata": {},
   "source": [
    "This consolidation reduced the number of unique publishers from **10,724 to 9,248**, capturing the main trade and academic groups while retaining smaller independent publishers.\n",
    "\n",
    "After standardization, **40.24% of all titles** and **49.47% of English-language titles** map to a major publisher. This indicates strong coverage of the dominant publishing groups in the dataset.\n",
    "\n",
    "Remaining publishers, such as *the writers coffee shop*, *buccaneer books*, and *america star books*, represent the expected long tail of small, regional, or self-publishing presses. We will plot the data to ensure that there is little value to extend the mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d13962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "publisher_col = 'publisher_clean' \n",
    "\n",
    "# Count frequencies\n",
    "counts = (\n",
    "    bbe_clean[publisher_col]\n",
    "    .dropna()\n",
    "    .value_counts()\n",
    ")\n",
    "\n",
    "# long‑tail: drop top 30 publishers to show heavy‑tail structure\n",
    "tail = counts[30:]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(tail.values)\n",
    "plt.title('Long-Tail Distribution of Publisher Frequencies')\n",
    "plt.xlabel('Publisher Rank (beyond top 30)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8646b91",
   "metadata": {},
   "source": [
    "The long-tail distribution shows that after the major publishers, the remaining ~9,000 publishers contribute extremely few titles, mostly only one each. This means additional mapping yields diminishing returns, since there is insufficient frequency or consistency to reliably assign these small publishers to meaningful parent groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8151781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.extend([\n",
    "    'publisher'\n",
    "    ])\n",
    "\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 6\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd851a",
   "metadata": {},
   "source": [
    "#### **Book Format**\n",
    "\n",
    "This step standardizes the `bookFormat` field across multiple languages and inconsistent label variations found in the dataset.  \n",
    "The goal is to translate all format names into English and consolidate equivalent values (e.g., *“Capa dura”*, *“Gebundene Ausgabe”*, *“Hard back”*) under unified categories such as **Hardcover**, **Paperback**, **Ebook**, and **Audiobook**.\n",
    "\n",
    "This cleaning ensures that:\n",
    "- Format values are consistent for analysis and visualization.  \n",
    "- Non-English or rare variants are translated and grouped appropriately.  \n",
    "- Missing or unrecognized entries are handled under a neutral category: **Other / Unknown**.  \n",
    "\n",
    "By applying a mapping dictionary, we make the variable suitable for aggregation, comparison, and predictive modeling. After transformation, we verify the result by inspecting the number of unique standardized values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique format values \n",
    "bbe_clean['bookFormat'] = bbe_clean['bookFormat'].astype(str).str.strip() \n",
    "unique_format = bbe_clean['bookFormat'].unique() \n",
    "\n",
    "print(f\"\\nTotal unique book format values: {len(unique_format)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load format dictionary\n",
    "with open(\"src/cleaning/mappings/format_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    format_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631890b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['bookFormat_clean'] = (\n",
    "    bbe_clean['bookFormat']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .replace(format_dict)\n",
    ")\n",
    "\n",
    "# Replace remaining unknowns or NaN with a unified label\n",
    "bbe_clean['bookFormat_clean'] = bbe_clean['bookFormat_clean'].replace(['nan', 'none', ''], np.nan)\n",
    "bbe_clean['bookFormat_clean'] = bbe_clean['bookFormat_clean'].fillna('Other / Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c61a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_format_clean = bbe_clean['bookFormat_clean'].unique() \n",
    "\n",
    "print(f\"\\nTotal unique book format values: {len(unique_format_clean)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3adabd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_format_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca631e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.append('bookFormat')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 7\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a412a95",
   "metadata": {},
   "source": [
    "After applying the standardization mapping, the number of unique book format values was reduced from **135** to **10**.  This represents a substantial improvement in data consistency and interpretability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e35f4",
   "metadata": {},
   "source": [
    "#### **Ratings**\n",
    "\n",
    "In this step, we will first evaluate the quality and consistency of the `rating` field.\n",
    "We first check for missing or invalid values and calculate the percentage of available ratings to assess data completeness. Then, we use the `describe()` method to verify whether the ratings follow the expected 1–5 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7651de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where rating is not NaN\n",
    "total_books = len(bbe_clean)\n",
    "has_ratings = bbe_clean[bbe_clean['rating'].notna()]\n",
    "has_ratings_num = has_ratings.shape[0]\n",
    "share_ratings = has_ratings_num / total_books * 100\n",
    "\n",
    "# Print the number of titles with ratings and show the first few examples\n",
    "print(f\"Books with ratings: {has_ratings_num} of {total_books} ({share_ratings:.2f}%)\")\n",
    "has_ratings[['title_clean', 'rating', 'numRatings','ratingsByStars']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['rating'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['rating'].unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bff8e3",
   "metadata": {},
   "source": [
    "The inspection confirms that the dataset is generally clean; however, a small number of entries have a value of `0`, which represents missing evaluations. These will be replaced with `NaN` to ensure the ratings remain within the valid range (1–5). Since all valid values already follow the standard Goodreads scale, no normalization is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f90c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (bbe_clean['rating'] == 0)\n",
    "print(f'Items with value equal 0: {bbe_clean[mask].shape[0]}')\n",
    "bbe_clean[mask][['title_clean', 'author_clean','rating','numRatings','ratingsByStars']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['rating_clean'] = bbe_clean['rating'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332efe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['rating_clean'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bbe_clean['rating_clean'].hist(bins=20)\n",
    "plt.title('Distribution of Average Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3563f",
   "metadata": {},
   "source": [
    "#### **NumRating**\n",
    "\n",
    "Next we will handle `numRatings`. The `numRatings` feature represents the total count of user ratings per book. We seen know from the mask we created that where `ratings` equals `0`, `numRatings` tends to be `0` too. We will check if that is always the case by checking for invalid values and using the `.describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4254c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_numRatings = bbe_clean[bbe_clean['numRatings'].isna()]\n",
    "na_numRatings_num = na_numRatings.shape[0]\n",
    "share_numRatings = na_numRatings_num / total_books * 100\n",
    "\n",
    "# Print the number of titles with ratings and show the first few examples\n",
    "print(f\"Books with no numRatings values: {na_numRatings_num} of {total_books} ({share_numRatings}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c234c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['numRatings'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc52bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['numRatings_clean'] = (\n",
    "    pd.to_numeric(bbe_clean['numRatings'], errors='coerce') \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0689cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['numRatings_clean'].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bc87b",
   "metadata": {},
   "source": [
    "Since it’s a valid count metric (0 values indicate unrated books), no replacement with NaN is required. However, because most books have relatively few ratings while a few very popular titles have millions, the distribution is heavily right-skewed. To better visualize and later analyze relationships with other variables, we apply a logarithmic transformation (`log1p`) to smooth out the long tail and reveal underlying patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(bbe_clean['numRatings'], bins=20)\n",
    "plt.title(\"Distribution of Log(Number of Ratings)\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(np.log1p(bbe_clean['numRatings']), bins=50)\n",
    "plt.title(\"Distribution of Log(Number of Ratings)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74e109",
   "metadata": {},
   "source": [
    "The log transformation reveals a near-normal distribution centered around books with moderate popularity.\n",
    "This confirms that `numRatings` is a valid and informative feature, and no normalization or imputation is needed at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the count to reduce skew\n",
    "bbe_clean['numRatings_log'] = np.log1p(bbe_clean['numRatings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee88d05",
   "metadata": {},
   "source": [
    "#### **ratingsByStars**\n",
    "\n",
    "In this step, we examine how complete the `ratingsByStars` field is across all books.  \n",
    "This feature represents the 1–5 star breakdown of user ratings and is essential for modelling engagement quality and satisfaction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e520ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_ratings_stars = bbe_clean[bbe_clean['ratingsByStars'].isna()]\n",
    "na_ratings_stars_num = na_ratings_stars.shape[0]\n",
    "share_na_ratings_stars = na_ratings_stars_num / total_books * 100\n",
    "\n",
    "# Print the number of titles with ratingsByStars and show the first few examples\n",
    "print(f\"Books with ratingsByStars: {na_ratings_stars_num} of {total_books} ({share_na_ratings_stars}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff53e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['ratingsByStars'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_ratings_stars_mask = (bbe_clean['ratingsByStars'] == '[]')\n",
    "empty_ratings_stars = bbe_clean[empty_ratings_stars_mask].shape[0]\n",
    "print(f'Items with empty values: {empty_ratings_stars}')\n",
    "bbe_clean[empty_ratings_stars_mask][['title_clean', 'author_clean','rating','numRatings','ratingsByStars']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fbbc6",
   "metadata": {},
   "source": [
    "By quantifying missing or empty values, we identify potential inconsistencies between overall ratings (`rating`, `numRatings`) and their detailed distribution.\n",
    "After counting missing and empty entries, we compare these against books that *do* have `rating` and `numRatings` values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d49164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask for books with ratings\n",
    "mask_has_ratings = (bbe_clean['numRatings'] > 0) & (bbe_clean['rating_clean'] > 0)\n",
    "# mask for books without rating distributions\n",
    "mask_no_distribution = (bbe_clean['ratingsByStars'] == '[]')\n",
    "# combine masks: have ratings but no distribution:\n",
    "mask_rated_no_distribution = mask_has_ratings & mask_no_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a51959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and inspect missing star distributions among rated books\n",
    "\n",
    "# count how many books have ratings but no ratingsByStars distribution\n",
    "num_missing_dist = mask_rated_no_distribution.sum()\n",
    "share_missing_dist = num_missing_dist / len(bbe_clean) * 100\n",
    "\n",
    "# count total books with empty or missing distributions (regardless of ratings)\n",
    "total_empty_dist = empty_ratings_stars_mask.sum()\n",
    "\n",
    "# compute what share of those empty distributions actually have valid ratings\n",
    "share_with_ratings = (num_missing_dist / total_empty_dist) * 100\n",
    "share_without_ratings = 100 - share_with_ratings\n",
    "\n",
    "# print results\n",
    "print(f\"Total books: {len(bbe_clean):,}\")\n",
    "print(f\"Books with ratings but missing distribution: {num_missing_dist:,} ({share_missing_dist:.2f}%)\")\n",
    "print(f\"  ↳ Of all empty distributions ({total_empty_dist:,} total):\")\n",
    "print(f\"      • With ratings: {share_with_ratings:.2f}%\")\n",
    "print(f\"      • Without ratings: {share_without_ratings:.2f}%\")\n",
    "\n",
    "# inspect a few examples\n",
    "bbe_clean.loc[mask_rated_no_distribution, ['title_clean', 'author_clean', 'rating_clean', 'numRatings_clean', 'ratingsByStars']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb30c75",
   "metadata": {},
   "source": [
    "This highlights a critical gap: books with ratings but without a distribution breakdown.  \n",
    "\n",
    "Such gaps likely stem from export limitations or missing historical data from Goodreads, and must be addressed before feature engineering or predictive tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5d1fe",
   "metadata": {},
   "source": [
    "To handle missing `ratingsByStars` while preserving analytical completeness, we implement a probabilistic estimation function.  \n",
    "The approach assumes a normal distribution around the book’s average rating, proportionally allocating counts across 1–5 stars.  \n",
    "\n",
    "This preserves both the **total number of ratings** and the **shape of expected user sentiment**, ensuring downstream models can use these reconstructed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb063abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def estimate_star_distribution(avg_rating, num_ratings):\n",
    "    # define 1–5 star levels\n",
    "    stars = np.arange(1, 6)\n",
    "\n",
    "    # normal distribution around avg_rating\n",
    "    # - (stars - avg_rating): distance of each star value from the mean\n",
    "    # - **2: squares the distance to emphasize larger deviations\n",
    "    # - -0.5 * (...): converts distance into a negative exponent (closer = less negative)\n",
    "    # - np.exp(...): applies exponential decay, giving higher weights to values near the mean\n",
    "    # - 0.5 controls the curve’s spread (smaller = narrower, larger = wider)\n",
    "    weights = np.exp(-0.5 * ((stars - avg_rating) ** 2) / 0.5**2)\n",
    "    weights /= weights.sum()  # normalize to 1\n",
    "    \n",
    "    # Scale to total ratings\n",
    "    estimated_counts = np.round(weights * num_ratings).astype(int)\n",
    "\n",
    "    # Adjust rounding error so sum matches exactly\n",
    "    diff = num_ratings - estimated_counts.sum()\n",
    "    estimated_counts[np.argmax(weights)] += diff\n",
    "\n",
    "    return estimated_counts.tolist()\n",
    "\n",
    "# code inspiration: \n",
    "# https://www.geeksforgeeks.org/machine-learning/gaussian-distribution-in-machine-learning/\n",
    "# https://www.geeksforgeeks.org/python/python-normal-distribution-in-statistics/\n",
    "# https://www.geeksforgeeks.org/numpy/binning-data-in-python-with-scipy-numpy/\n",
    "# https://blog.quantinsti.com/gaussian-distribution/\n",
    "# https://www.freecodecamp.org/news/how-to-explain-data-using-gaussian-distribution-and-summary-statistics-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e24a4",
   "metadata": {},
   "source": [
    "The function is applied to all titles with valid ratings but missing distributions.  \n",
    "We then validate that each reconstructed list of star counts sums to its corresponding `numRatings`, ensuring internal consistency.  \n",
    "A high proportion of valid totals indicates that the imputation strategy worked as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['ratingsByStars_clean'] = bbe_clean['ratingsByStars']\n",
    "# For books missing star distributions but with valid ratings,\n",
    "# estimate a plausible 1–5 star breakdown using avg_rating and numRatings,\n",
    "# and store the result in 'ratingsByStars_clean'.\n",
    "bbe_clean.loc[mask_rated_no_distribution, 'ratingsByStars_clean'] = (\n",
    "    bbe_clean.loc[mask_rated_no_distribution]\n",
    "    .apply(lambda x: estimate_star_distribution(x['rating'], int(x['numRatings'])), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many were filled\n",
    "filled_count = bbe_clean.loc[mask_rated_no_distribution, 'ratingsByStars_clean'].notna().sum()\n",
    "print(f\"Filled distributions: {filled_count:,} (of {mask_rated_no_distribution.sum():,} missing)\")\n",
    "\n",
    "# preview examples\n",
    "print(\"\\nSample of estimated distributions:\")\n",
    "display(\n",
    "    bbe_clean.loc[mask_rated_no_distribution, \n",
    "                  ['title', 'author_clean', 'rating', 'numRatings', 'ratingsByStars_clean']\n",
    "                 ].head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5582ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that each estimated list sums correctly\n",
    "check_sum = bbe_clean.loc[mask_rated_no_distribution].apply(\n",
    "    lambda x: sum(x['ratingsByStars_clean']) == int(x['numRatings']), axis=1\n",
    ")\n",
    "valid_share = check_sum.mean() * 100\n",
    "print(f\"\\nDistributions matching numRatings total: {valid_share:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3658b5ae",
   "metadata": {},
   "source": [
    "We extend the validation to the full dataset, verifying that all `ratingsByStars_clean` entries, both original and estimated, correctly sum to `numRatings`. This serves as a final data integrity checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def safe_sum_ratings(row):\n",
    "    val = row['ratingsByStars_clean']\n",
    "    # Convert stringified lists into Python lists\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            val = ast.literal_eval(val)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    # If it's a list, make sure elements are integers\n",
    "    if isinstance(val, list):\n",
    "        try:\n",
    "            val = [int(v) for v in val]  # convert each element to int\n",
    "            return sum(val) == int(row['numRatings'])\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# https://dev.to/mstuttgart/using-literal-eval-for-string-to-object-conversion-in-python-46i\n",
    "# https://www.educative.io/answers/what-is-astliteralevalnodeorstring-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sum_all = bbe_clean.loc[bbe_clean['ratingsByStars_clean'].notna()].apply(safe_sum_ratings, axis=1)\n",
    "valid_share_all = check_sum_all.mean() * 100\n",
    "\n",
    "print(f\"All distributions matching numRatings total: {valid_share_all:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b43433",
   "metadata": {},
   "source": [
    "Next, we replace remaining `'[]'` values using `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d36f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_empty_ratings_stars_mask = (bbe_clean['ratingsByStars_clean'] == '[]')\n",
    "remaining_empty_ratings_stars = bbe_clean[remaining_empty_ratings_stars_mask].shape[0]\n",
    "print(f'Remaining empty values: {remaining_empty_ratings_stars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6696a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['ratingsByStars_clean'] = bbe_clean['ratingsByStars_clean'].replace('[]', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad63b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['ratingsByStars_clean'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1741b4",
   "metadata": {},
   "source": [
    "We decided to split the list into individual features as it makes it easier to query and filter, better for statistical operations, it is more intuitive for visualization and standard format for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def parse_ratings_to_list(val):\n",
    "    \"\"\"Parse and return ratings in correct order\"\"\"\n",
    "    \n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return [np.nan] * 5\n",
    "    \n",
    "    if isinstance(val, list):\n",
    "        try:\n",
    "            clean_list = [int(v) for v in val]\n",
    "            if len(clean_list) == 5:\n",
    "                # REVERSE the list if it's stored as [5★, 4★, 3★, 2★, 1★]\n",
    "                return clean_list[::-1]  # This reverses to [1★, 2★, 3★, 4★, 5★]\n",
    "            else:\n",
    "                return [np.nan] * 5\n",
    "        except (ValueError, TypeError):\n",
    "            return [np.nan] * 5\n",
    "    \n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            if isinstance(parsed, list) and len(parsed) == 5:\n",
    "                return [int(v) for v in parsed][::-1]  # Reverse here too\n",
    "            else:\n",
    "                return [np.nan] * 5\n",
    "        except Exception:\n",
    "            return [np.nan] * 5\n",
    "    \n",
    "    return [np.nan] * 5\n",
    "\n",
    "# apply the function\n",
    "bbe_clean['ratings_parsed'] = bbe_clean['ratingsByStars_clean'].apply(parse_ratings_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b680f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand into separate columns\n",
    "ratings_expanded = bbe_clean['ratings_parsed'].apply(pd.Series)\n",
    "ratings_expanded.columns = ['ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5']\n",
    "# add to dataframe\n",
    "bbe_clean = pd.concat([bbe_clean, ratings_expanded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76837650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "print(\"\\nNew rating columns created:\")\n",
    "print(bbe_clean[['title_clean', 'rating_clean', 'numRatings', 'ratings_1', 'ratings_2', \n",
    "                  'ratings_3', 'ratings_4', 'ratings_5']].head(10))\n",
    "\n",
    "# verify sums match numRatings\n",
    "bbe_clean['ratings_sum_check'] = bbe_clean[['ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5']].sum(axis=1)\n",
    "matches = (bbe_clean['ratings_sum_check'] == bbe_clean['numRatings']).sum()\n",
    "print(f\"\\nRatings that sum correctly: {matches} / {len(bbe_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28118d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# calculate mean shares for BBE dataset\n",
    "# first, create share columns if you haven't already\n",
    "for i in range(1, 6):\n",
    "    bbe_clean[f'ratings_{i}_share'] = bbe_clean[f'ratings_{i}'] / bbe_clean['numRatings']\n",
    "\n",
    "# calculate average shares\n",
    "mean_shares_bbe = [bbe_clean[f\"ratings_{i}_share\"].mean() for i in range(1, 6)]\n",
    "\n",
    "# create the bar chart\n",
    "plt.bar(range(1, 6), mean_shares_bbe)\n",
    "plt.title(\"Average Star Rating Distribution (Best Books Ever)\")\n",
    "plt.xlabel(\"Star Rating\")\n",
    "plt.ylabel(\"Average Share of Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d873ec2",
   "metadata": {},
   "source": [
    "Lastly, we computed mean star-share distributions and plotted comparative bar charts to visualize global reader sentiment patterns.\n",
    "This revealed the datasets follow a similar right-skewed pattern, where 4- and 5-star ratings dominate—typical of user-generated book reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16398e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.extend(['ratingsByStars', 'rating', 'numRatings', 'ratingsByStars_clean', 'ratings_parsed','ratings_sum_check'])\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 8\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081084ba",
   "metadata": {},
   "source": [
    "#### **Awards**\n",
    "\n",
    "In this step, we will use some custom functions created to process both the genres and the awards columns, as they both represent textual data in a list. After a first analysis we can see that the data has a temporal component that adds noise and inconsistency to the feature. Since temporality is already being captured in the `firstPublicationDate` we will strip this to clean the data and, therefore, creating a clean function specific for awards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d0148",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['title_clean','awards']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c61a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and handle missing awards values\n",
    "empty_awards_mask = (bbe_clean['awards'] == '[]')\n",
    "empty_awards = bbe_clean[empty_awards_mask].shape[0]\n",
    "share_missing_awards = (empty_awards / len(bbe_clean)) * 100\n",
    "print(f'Books with empty awards values: {empty_awards} ({share_missing_awards:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# GENERIC PARSING AND CLEANING FUNCTIONS\n",
    "\n",
    "def parse_list_field(val):\n",
    "    \"\"\"\n",
    "    Safely parse a stringified list (e.g. '[\"x\", \"y\"]') into a Python list.\n",
    "    Returns np.nan for missing, invalid, or empty values.\n",
    "    \"\"\"\n",
    "    if pd.isna(val) or val in ['[]', '', None]:\n",
    "        return np.nan\n",
    "    try:\n",
    "        parsed = ast.literal_eval(val)\n",
    "        if isinstance(parsed, list) and len(parsed) > 0:\n",
    "            return parsed\n",
    "        else:\n",
    "            return np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def clean_text_item(text, keep_pattern=r'[^a-z0-9\\s-]'):\n",
    "    \"\"\"\n",
    "    Lowercase and remove noise, keeping only letters, digits, hyphens and spaces.\n",
    "    Can be reused for genres, awards, etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(keep_pattern, '', text)  # clean unwanted chars\n",
    "    text = re.sub(r'\\s+', ' ', text)       # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def clean_list_field(lst, keep_pattern=r'[^a-z0-9\\s-]'):\n",
    "    \"\"\"\n",
    "    Clean and deduplicate elements from a list of strings.\n",
    "    Returns np.nan for invalid or empty lists.\n",
    "    \"\"\"\n",
    "    if not isinstance(lst, list):\n",
    "        return np.nan\n",
    "    cleaned = [clean_text_item(item, keep_pattern) for item in lst if isinstance(item, str) and item.strip()]\n",
    "    cleaned = [c for c in cleaned if c]\n",
    "    return list(set(cleaned)) if cleaned else np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce59c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_awards_list(lst):\n",
    "    \"\"\"\n",
    "    Clean and normalize awards list:\n",
    "    - Lowercase and remove punctuation noise\n",
    "    - Remove year patterns like (2009) or (2010)\n",
    "    - Deduplicate entries\n",
    "    \"\"\"\n",
    "    if not isinstance(lst, list):\n",
    "        return np.nan\n",
    "\n",
    "    cleaned = []\n",
    "    for a in lst:\n",
    "        if not isinstance(a, str) or not a.strip():\n",
    "            continue\n",
    "        a = a.lower().strip()\n",
    "        # remove (YYYY) patterns\n",
    "        a = re.sub(r'\\(\\s*\\d{4}\\s*\\)', '', a)\n",
    "        # remove leftover punctuation and extra spaces\n",
    "        a = re.sub(r'[^a-z0-9\\s\\-\\&\\'\"]', '', a)\n",
    "        a = re.sub(r'\\s+', ' ', a).strip()\n",
    "        cleaned.append(a)\n",
    "\n",
    "    cleaned = list(set(cleaned))\n",
    "    return cleaned if cleaned else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['awards_parsed'] = bbe_clean['awards'].apply(parse_list_field)\n",
    "bbe_clean['awards_clean'] = bbe_clean['awards_parsed'].apply(clean_awards_list)\n",
    "bbe_clean[['title_clean','awards','awards_parsed','awards_clean']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9370b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusable dictionary counts\n",
    "def count_unique_items(df, column):\n",
    "    \"\"\"\n",
    "    Count the frequency of each unique element in a list-type column.\n",
    "    Returns a dictionary {item: count}.\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for lst in df[column].dropna():\n",
    "        if isinstance(lst, list):\n",
    "            for item in lst:\n",
    "                counts[item] = counts.get(item, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_awards_dict = count_unique_items(bbe_clean, 'awards_clean')\n",
    "num_unique_awards = len(unique_awards_dict)\n",
    "print(f\"Number of unique awards: {num_unique_awards}\")\n",
    "\n",
    "sorted_awards = sorted(unique_awards_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_awards[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be534c",
   "metadata": {},
   "source": [
    "Analysis shows that only a small subset, around **20%** includes award data, with **5,616 unique award names**. The high number of unique labels, combined with low frequency counts per award, makes this feature extremely **sparse and fragmented**. Even the most common award (“Dorothy Canfield Fisher Children's Book Award Nominee”) appears only **324 times** in a dataset of over **52,000 books**, representing less than **1%** of the records.\n",
    "\n",
    "Such high-cardinality categorical data introduces:\n",
    "\n",
    "* **Noise:** because similar awards appear under slightly different names or languages\n",
    "* **Inefficiency:** since one-hot or text encoding would explode feature dimensions\n",
    "* **Weak signal strength:** as most awards occur too infrequently to influence model patterns\n",
    "\n",
    "By simplifying this column into a **binary indicator (`has_award`)**, we preserve the meaningful information; whether a book has received any recognition — while removing the sparsity and variability that would degrade model performance.\n",
    "This boolean variable captures the *prestige signal* without the complexity, improving both interpretability and computational efficiency for downstream tasks such as **recommendation and clustering**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean flag for books with any valid awards\n",
    "bbe_clean['has_award'] = bbe_clean['awards_clean'].apply(\n",
    "    lambda x: isinstance(x, list) and len(x) > 0\n",
    ")\n",
    "bbe_clean[['title','awards_clean','has_award']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2af8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.extend(['awards_parsed','awards_clean', 'awards'])\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 9\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4546c",
   "metadata": {},
   "source": [
    "#### **Genres**\n",
    "\n",
    "In memory-based recommender systems, categorical attributes such as genre serve as key features for similarity computation. In this step, we first identify and handle missing values, then we use the generic functions created on the **Awards** section and parse the genre lists using `ast.literal_eval` to ensure proper data structure representation, and finally quantify the unique genres and analyze their distribution across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd91499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and handle missing genre values\n",
    "empty_genres_mask = (bbe_clean['genres'] == '[]')\n",
    "empty_genres = bbe_clean[empty_genres_mask].shape[0]\n",
    "share_missing_genres = (empty_genres / len(bbe_clean)) * 100\n",
    "print(f'Books with empty genre values: {empty_genres} ({share_missing_genres:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['genres_parsed'] = bbe_clean['genres'].apply(parse_list_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['genres_parsed'] = bbe_clean['genres_parsed'].apply(clean_list_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(bbe_clean['genres_parsed'].apply(type).value_counts())\n",
    "print(bbe_clean['genres_parsed'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_nan_genres = bbe_clean['genres_parsed'].apply(lambda x: isinstance(x, float))\n",
    "bbe_clean[mask_nan_genres][['title_clean', 'author_clean', 'genres', 'genres_parsed']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres_dict = dict(\n",
    "    sorted(count_unique_items(bbe_clean, 'genres_parsed').items(), key=lambda x: x[1], reverse=True)\n",
    ")\n",
    "unique_genres_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90970a",
   "metadata": {},
   "source": [
    "After analyzing the list of top genres, we identified several tags that are problematic because they describe edition or format attributes rather than actual genres. Other values are relevant but belong in different fields, such as *publisher* or *awards*. To address this, we will create separate lists and handle each case individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define known non-genre tags (format types)\n",
    "non_genres = [\n",
    "    'audiobook',  'book club', 'chapter books', 'unfinished', 'media tie in',\n",
    "     'manga', 'amazon', 'comics manga', 'cartoon', 'comix', 'textbooks',\n",
    "    'own', 'mine'\n",
    "]\n",
    "awards = ['hugo awards', 'nobel prize']\n",
    "\n",
    "harlequin = ['harlequin teen','harlequin', 'harlequin romance', \n",
    "              'harlequin historical', 'harlequin desire', 'harlequin presents', 'harlequin nocturne']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9a9c28",
   "metadata": {},
   "source": [
    "For award-related tags, we will cross-reference them with the existing `has_awards` field and update it to `True` where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ensure column exists\n",
    "if 'has_award' not in bbe_clean.columns:\n",
    "    bbe_clean['has_award'] = False\n",
    "\n",
    "# log before update\n",
    "before_true = bbe_clean['has_award'].sum()\n",
    "print(f'Total books with awards: {before_true}.')\n",
    "\n",
    "# detect award-related genres and update flag\n",
    "bbe_clean['has_award'] = np.where(\n",
    "    bbe_clean['genres_parsed'].apply(\n",
    "        lambda g: any(a in [x.lower() for x in g] for a in awards) if isinstance(g, list) else False\n",
    "    ),\n",
    "    True,\n",
    "    bbe_clean['has_award']\n",
    ")\n",
    "\n",
    "# log after update\n",
    "after_true = bbe_clean['has_award'].sum()\n",
    "new_awards = after_true - before_true\n",
    "\n",
    "print(f\"Updated 'has_award' for {new_awards} additional books (total now: {after_true}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d462d72c",
   "metadata": {},
   "source": [
    "Finally, for cases involving *Harlequin*, we will create a mask to determine whether these entries should be dropped or whether they can help further refine the `publisher_clean` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Harlequin flag\n",
    "bbe_clean['is_harlequin'] = bbe_clean['genres_parsed'].apply(\n",
    "    lambda genres: any(g in harlequin for g in genres) if isinstance(genres, list) else False\n",
    ")\n",
    "\n",
    "# log\n",
    "total_books = len(bbe_clean)\n",
    "harlequin_true = bbe_clean['is_harlequin'].sum()\n",
    "harlequin_share = harlequin_true / total_books * 100 if total_books > 0 else 0\n",
    "\n",
    "print(f\"Found {harlequin_true:,} Harlequin-tagged titles \"\n",
    "      f\"({harlequin_share:.2f}% of total {total_books:,} books).\")\n",
    "\n",
    "# show Harlequin-flagged rows where publisher_clean isn't 'harpercollins'\n",
    "harlequin_mismatch = bbe_clean.loc[\n",
    "    (bbe_clean['is_harlequin']) &\n",
    "    (bbe_clean['publisher_clean'] != 'harpercollins'),\n",
    "    ['title_clean', 'publisher_clean', 'genres_parsed']\n",
    "]\n",
    "\n",
    "harlequin_mismatch_count = harlequin_mismatch.shape[0]\n",
    "harlequin_mismatch_head = harlequin_mismatch.head(15)\n",
    "\n",
    "print(f\"\\nThere are {harlequin_mismatch_count} Harlequin matches with publisher variations.\")\n",
    "\n",
    "if not harlequin_mismatch_head.empty:\n",
    "    print(\"\\nSample of Harlequin-related publishers (not exactly 'harlequin'):\\n\")\n",
    "    print(harlequin_mismatch_head.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nAll flagged publishers are exactly 'harlequin'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f5e51",
   "metadata": {},
   "source": [
    "From the summary above, we can see that **80 entries** from the **96** marked as Harlequin are not matching the HarperCollins publisher name. By analysing the values, we can see that they are valid imprints from HarperCollins, such as _mira_ and _silhouette_ .We will use the previously created flag to standardize these entries and improve overall data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c440aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many publisher_clean values will be changed\n",
    "num_changed = bbe_clean.loc[bbe_clean['is_harlequin'], 'publisher_clean'].ne('harpercollins').sum()\n",
    "\n",
    "# standardize publisher_clean for all Harlequin-flagged rows\n",
    "bbe_clean.loc[bbe_clean['is_harlequin'], 'publisher_clean'] = 'harpercollins'\n",
    "\n",
    "print(f\"\\nStandardized {num_changed:,} publisher entries to 'harpercollins'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b0fd0",
   "metadata": {},
   "source": [
    "\n",
    "Tags that represent format attributes will be removed, as they likely reflect a single user's specific edition rather than general information about the book ID. Therefore, we will remove them from the genres list.\n",
    "Since we already extract the information needed from the other two categories, we will create a single function to clean all these values from the genres column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4449995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_post_parsing(genres, *lists_to_remove):\n",
    "    \"\"\"\n",
    "    Remove known non-genre or auxiliary tags safely.\n",
    "    Accepts any number of lists/sets to remove.\n",
    "    \"\"\"\n",
    "    if not isinstance(genres, list):\n",
    "        return genres\n",
    "\n",
    "    # Combine all removal terms into one flat set\n",
    "    removal_terms = set().union(*lists_to_remove)\n",
    "    \n",
    "    return [g for g in genres if g.lower().strip() not in removal_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb26cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['genres_clean'] = bbe_clean['genres_parsed'].apply(\n",
    "    lambda g: clean_post_parsing(g, non_genres, awards, harlequin)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_unique_genres_dict = dict(\n",
    "    sorted(count_unique_items(bbe_clean, 'genres_clean').items(), key=lambda x: x[1], reverse=True)\n",
    ")\n",
    "final_unique_genres_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33fa8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# convert to Series for easy analysis\n",
    "genre_counts = pd.Series(final_unique_genres_dict).sort_values(ascending=False)\n",
    "top_n = 30\n",
    "\n",
    "#plot top N genres\n",
    "top_n = 30\n",
    "plt.figure(figsize=(10,6))\n",
    "genre_counts.head(top_n).plot(kind='bar', color='slateblue')\n",
    "plt.title(f\"Top {top_n} Genres by Frequency\")\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.ylabel(\"Book Count\")\n",
    "plt.xticks(rotation=75)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7ca10",
   "metadata": {},
   "source": [
    "The genre frequency plot reveals a highly skewed distribution typical of book markets, dominated by broad categories like _Fiction_ and _Romance_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['title_clean','author_clean', 'genres_parsed', 'bookFormat_clean', 'has_award']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0393517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate genre-level and book-level coverage\n",
    "total_genres = genre_counts.sum()\n",
    "top10 = set(genre_counts.head(10).index)\n",
    "\n",
    "top10_share = (genre_counts.head(10).sum() / total_genres) * 100\n",
    "mask_top10 = bbe_clean['genres_parsed'].apply(\n",
    "    lambda lst: any(g in top10 for g in lst) if isinstance(lst, list) else False\n",
    ")\n",
    "book_share_top10 = (mask_top10.sum() / len(bbe_clean)) * 100\n",
    "\n",
    "print(f\"Top 10 genres account for {top10_share:.2f}% of all genre occurrences.\")\n",
    "print(f\"Books with at least one of the top 10 genres: {book_share_top10:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124fe4e2",
   "metadata": {},
   "source": [
    "Broad, mainstream genres dominate both in tag volume and book coverage. Nearly one-third of all genre tags (**31.41%**) in the dataset come from the same 10 genres, showing strong catalog concentration. Nearly nine out of ten books (**86.54%**) fall within those top categories, confirming their dominance at the book level. This distribution supports the design of **segment-based recommendation strategies** for mainstream readers while maintaining a “long-tail” of niche genres to personalize discovery.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33edb942",
   "metadata": {},
   "source": [
    "Since genre tags are extremely long-tail: a few popular genres dominate while hundreds of niche labels appear rarely. Collapsing the tail into a single bucket (_other_) reduces feature sparsity, speeds up modeling, and keeps the vectors interpretable for the dashboard. This aligns with CRISP-DM Data Preparation and the assessment’s requirement to collect, arrange, and process data before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0826e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplification rules\n",
    "TOP_K = 100          # keep the 100 most frequent genres\n",
    "TAIL_LABEL = 'other' # name of the long-tail bucket\n",
    "FILL_MISSING = True  # set to False if you prefer to leave NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a58833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "valid_lists = bbe_clean['genres_clean'].dropna()\n",
    "genre_counts = Counter(g for lst in valid_lists for g in lst)\n",
    "genre_counts = pd.Series(genre_counts).sort_values(ascending=False)\n",
    "top_genres = set(genre_counts.head(TOP_K).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc80f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_genre_list(lst, *, keep=top_genres, tail=TAIL_LABEL):\n",
    "    if not isinstance(lst, list):  # NaN / missing\n",
    "        return ['unknown'] if FILL_MISSING else pd.NA\n",
    "    kept = [g if g in keep else tail for g in lst]\n",
    "    # dedupe while preserving order\n",
    "    seen = set()\n",
    "    simplified = [x for x in kept if not (x in seen or seen.add(x))]\n",
    "    # if everything was mapped to tail and list became ['other'] it's fine; if it became empty, fill fallback\n",
    "    return simplified or (['unknown'] if FILL_MISSING else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf59c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['genres_simplified'] = bbe_clean['genres_clean'].apply(simplify_genre_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee35f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of rows simplified successfully\n",
    "ok_share = bbe_clean['genres_simplified'].notna().mean() * 100\n",
    "print(f\"Simplified rows available: {ok_share:.2f}%\")\n",
    "\n",
    "# How many rows include the tail label\n",
    "has_tail = bbe_clean['genres_simplified'].apply(lambda lst: isinstance(lst, list) and TAIL_LABEL in lst).sum()\n",
    "print(f\"Rows containing '{TAIL_LABEL}': {has_tail}\")\n",
    "\n",
    "# Coverage of the head vs tail (by occurrences)\n",
    "from collections import Counter\n",
    "simp_counts = Counter(g for lst in bbe_clean['genres_simplified'].dropna() for g in lst)\n",
    "head_occ = sum(simp_counts[g] for g in simp_counts if g in top_genres)\n",
    "tail_occ = simp_counts.get(TAIL_LABEL, 0)\n",
    "total_occ = head_occ + tail_occ\n",
    "print(f\"Head coverage: {head_occ/total_occ*100:.2f}% | Tail coverage: {tail_occ/total_occ*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(bbe_clean['genres_simplified'].apply(type).value_counts())\n",
    "print(bbe_clean['genres_simplified'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def genre_completeness_report(df, columns):\n",
    "    \"\"\"\n",
    "    Check completeness (non-empty lists) for given genre columns.\n",
    "    Returns a DataFrame with counts and percentages.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    summary = []\n",
    "    \n",
    "    for col in columns:\n",
    "        mask_valid = df[col].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "        valid = mask_valid.sum()\n",
    "        missing = total - valid\n",
    "        summary.append({\n",
    "            \"column\": col,\n",
    "            \"total_books\": total,\n",
    "            \"valid_genres\": valid,\n",
    "            \"missing_genres\": missing,\n",
    "            \"valid_%\": round((valid / total) * 100, 2),\n",
    "            \"missing_%\": round((missing / total) * 100, 2)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "# completeness check\n",
    "completeness = genre_completeness_report(\n",
    "    bbe_clean, \n",
    "    ['genres_parsed', 'genres_simplified']\n",
    ")\n",
    "print(\"Genre Completeness Summary\\n\")\n",
    "display(completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55569a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.extend(['genres','genres_parsed', 'is_harlequin'])\n",
    "\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 10\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56168233",
   "metadata": {},
   "source": [
    "#### **Description**\n",
    "\n",
    "Before cleaning, we inspected several book descriptions to identify common issues such as editorial notes, missing spaces after punctuation, escaped characters, and residual metadata (e.g., “Librarian’s note”, ISBN mentions, or “(Note: this title…)”).\n",
    "These observations informed the creation of a custom regex-based cleaning function.\n",
    "\n",
    "We then implemented a `clean_description()` function to remove noise, normalize spacing, and prepare text for analysis.\n",
    "Although NLP is currently a stretch goal, we decided to clean the text field proactively to ensure it’s ready for both readability and potential future feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bb597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and handle missing awards values\n",
    "no_description_mask = (bbe_clean['description'] == ' ')\n",
    "no_description = bbe_clean[no_description_mask].shape[0]\n",
    "share_no_description = (no_description / len(bbe_clean)) * 100\n",
    "print(f'Books with no description: {no_description} ({share_no_description:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fef9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['title_clean','description']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check a few different descriptions for inspection of things to be cleaned\n",
    "n = 6\n",
    "bbe_clean.loc[n, 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def clean_description(text):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # add a missing period and space between sentences when a lowercase letter \n",
    "    # that is immediately followed by an uppercase letter\n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1. \\2\", text)\n",
    "\n",
    "    # remove librarian/editorial notes and metadata (Note:, Alternate cover, ISBN refs)\n",
    "    text = re.sub(r\"\\(Note:.*?\\)\", \"\", text, flags=re.IGNORECASE)    \n",
    "    text = re.sub(r\"Librarian's note:.*?(?:\\.)\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r\"Alternate cover edition of ISBN \\d+\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"edition of ISBN \\d+\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "    # fix missing space after punctuation (.,;!?) when followed by letter/number\n",
    "    text = re.sub(r'([.,;!?])(?=[A-Za-z0-9])', r'\\1 ', text)\n",
    "\n",
    "    # fix missing space after ISBN numbers (e.g., \"9780679783268Since\")\n",
    "    text = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', text)\n",
    "\n",
    "    # normalize escaped quotes\n",
    "    text = text.replace(\"\\\\'\", \"'\").replace('\\\\\"', '\"')\n",
    "\n",
    "    # keep readable characters, include # for \"#1\"\n",
    "    text = re.sub(r\"[^A-Za-zÀ-ÖØ-öø-ÿ0-9#\\s.,;!?\\'\\\"-]\", \" \", text)\n",
    "\n",
    "    # collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8220a7ea",
   "metadata": {},
   "source": [
    "Before applying the cleaning function to the full dataset, we tested it on **synthetic example** representing all major issues (missing punctuation, uppercase continuity, notes, etc.). After validation, we split the results into two versions:\n",
    "\n",
    "* `description_clean`: preserves original casing — ideal for the **Streamlit dashboard** or display.\n",
    "* `description_nlp`: lowercase version — ready for **NLP or feature engineering** if needed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on synthetic description\n",
    "raw_description = \"\"\"\n",
    "Alternate cover edition of ISBN 9780679783268Since today.\n",
    "Librarian's note: An alternate cover edition can be found hereIt was 1939.Nazi Germany.The country is holding its breath.Death has never been busier!\n",
    "By her brother's graveside,Liesel's life changes when she picks up The Gravedigger's Handbook.\n",
    "(Note: this title was not published as YA fiction)\n",
    "\"WINNING MEANS FAME AND FORTUNE.LOSING MEANS CERTAIN DEATH.\"\n",
    "Jeune astronome convaincue de l'existence d'une vie extraterrestre intelligente...\n",
    "From #1 New York Times bestselling author Brandon Sanderson, The Way of Kings, book one of The Stormlight Archive begins an incredible new saga of epic proportion.Roshar is a world of stone and storms.\n",
    "\"\"\"\n",
    "print(clean_description(raw_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep readable version for UI\n",
    "bbe_clean['description_clean'] = bbe_clean['description'].apply(clean_description)\n",
    "\n",
    "# Lowercase version for NLP\n",
    "bbe_clean['description_nlp'] = bbe_clean['description_clean'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c92777",
   "metadata": {},
   "source": [
    "Finally, we verified both `description_clean` and `description_nlp` for completeness and formatting consistency to confirm that the cleaning pipeline performed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4844985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['title_clean','description_clean','description_nlp']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f40f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.append('description')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 11\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} datasets saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a2078",
   "metadata": {},
   "source": [
    "#### **Series**\n",
    "\n",
    "In this step, I reused the existing `clean_title()` function logic and adapted it into a new `clean_series()` function to clean the `series` feature. The function focuses on improving text consistency by removing unnecessary elements while keeping the core series name intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# retrieve the latest file\n",
    "bbe_clean = pd.read_csv('data/interim/bbe/bbe_clean_v11.csv', low_memory=False)\n",
    "\n",
    "bbe_clean[['title_clean','series']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5560836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_series(series: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean book series names for consistency.\n",
    "\n",
    "    Steps:\n",
    "    - Remove bracketed or parenthetical notes (e.g. '(Book 3)', '[Series]')\n",
    "    - Remove numeric indicators (#1, Book 2) while preserving base name\n",
    "    - Normalize spaces and lowercase\n",
    "    \"\"\"\n",
    "    if not isinstance(series, str) or series.strip() == \"\":\n",
    "        return np.nan\n",
    "    \n",
    "    text = series.strip()\n",
    "    \n",
    "    # remove bracketed or parenthetical content\n",
    "    text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    \n",
    "    # remove series order markers (e.g. \"#1\", \"Book 2\", \"Vol. 3\")\n",
    "    text = re.sub(r\"(#\\d+|book\\s*\\d+|volume\\s*\\d+|vol\\.\\s*\\d+)\", \"\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # clean punctuation and normalize spacing\n",
    "    text = re.sub(r\"[^\\w\\s'\\-]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # remove leftover \"-3\" or \"–3\"\n",
    "    text = re.sub(r\"[-–]\\s*\\d+\\b\", \"\", text)\n",
    "\n",
    "    # remove trailing standalone numbers (e.g., \"narnia 7\")\n",
    "    text = re.sub(r\"\\b\\d+\\b$\", \"\", text)\n",
    "    # ---------------------------------\n",
    "\n",
    "\n",
    "    return text.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f698099",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['series_clean'] = bbe_clean['series'].apply(clean_series)\n",
    "bbe_clean[['title_clean','series','series_clean']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ddb9d",
   "metadata": {},
   "source": [
    "After cleaning, I validated the completeness of the feature. The analysis shows that **approximately 50% of the books belong to a series**, indicating that the `series` field carries significant information that should be retained for downstream modeling and recommendation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b4784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask for missing titles\n",
    "total_books = len(bbe_clean)\n",
    "mask_no_series = bbe_clean['series_clean'].isna() | (bbe_clean['series_clean'] == \"\")\n",
    "# filter those rows\n",
    "no_series_books = bbe_clean[mask_no_series]\n",
    "share_no_series = ( no_series_books.shape[0] / total_books) * 100\n",
    "\n",
    "# display summary\n",
    "print(f\"Books without series: {no_series_books.shape[0]} ({share_no_series:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973744c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.append('series')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834cfb9f",
   "metadata": {},
   "source": [
    "#### **Pages**\n",
    "\n",
    "Cleaning the title field reduced missing page data from **2.3K** to **1.5K**.\n",
    "Since book length often correlates with **reader engagement** and **retention**, improving this feature strengthens the reliability of our recommendation inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689963c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['title_clean','pages']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['pages'].value_counts(dropna=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eafd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-numeric characters and convert\n",
    "bbe_clean['pages_clean'] = (\n",
    "    bbe_clean['pages']\n",
    "    .astype(str)\n",
    "    .str.extract(r'(\\d+)')  # raw string to avoid escape warnings\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# replace unrealistic values with NaN\n",
    "bbe_clean.loc[\n",
    "    (bbe_clean['pages_clean'] < 10) | (bbe_clean['pages_clean'] > 3000),\n",
    "    'pages_clean'\n",
    "] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404178f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_missing = bbe_clean['pages_clean'].isna().sum()\n",
    "print(f\"Remaining missing pages: {new_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e49239",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min:\", bbe_clean['pages_clean'].min())\n",
    "print(\"Max:\", bbe_clean['pages_clean'].max())\n",
    "\n",
    "# quick quantile check\n",
    "print(bbe_clean['pages_clean'].quantile([0.01, 0.25, 0.5, 0.75, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c25874",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_books = bbe_clean[bbe_clean['pages_clean'] < 100]\n",
    "short_books[['title_clean', 'author_clean', 'pages_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9387b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bbe_clean[bbe_clean['pages_clean'] < 500]['pages_clean'].hist(bins=50)\n",
    "plt.axvline(100, color='red', linestyle='--', label='100 pages')\n",
    "plt.title('Distribution of Book Lengths (up to 500 pages)')\n",
    "plt.xlabel('Pages')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_books['author_clean'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_books['genres_parsed'].explode().value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d042e5ef",
   "metadata": {},
   "source": [
    "Books with fewer than **100 pages** were inspected to ensure data validity.\n",
    "Titles like *The Little Prince*, *The Giving Tree*, and *The Old Man and the Sea* confirm that short works are genuine entries rather than data errors.\n",
    "The most frequent authors include **Dr. Seuss**, **Walt Disney Company**, and **Francine Pascal**, while dominant genres such as *Children’s*, *Poetry*, and *Picture Books* further validate these as authentic short-format books.\n",
    "Therefore, page counts below 100 are **considered valid** and retained for subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.append('pages')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31887a0",
   "metadata": {},
   "source": [
    "#### **Edition**\n",
    "\n",
    "To evaluate whether the `edition` column added value to the dataset, we performed a simple descriptive analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc2732",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_books = bbe_clean.shape[0]\n",
    "num_unique_editions = len(bbe_clean['edition'].unique())\n",
    "num_with_editions = bbe_clean['edition'].notna().sum()\n",
    "share_with_edition = (num_with_editions / total_books) * 100\n",
    "print(f'Books with Edition info: {num_with_editions} ({share_with_edition:.2f}%)')\n",
    "print(f'Edition values: {num_unique_editions}')\n",
    "bbe_clean['edition'].unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f0b327",
   "metadata": {},
   "source": [
    "Findings:\n",
    "\n",
    "* Only **9.03%** of books contained edition information (`3,846` out of all records).\n",
    "* There were **1,204 unique edition values**, many describing print or regional variations (e.g., *“First Edition”*, *“US/CAN Edition”*, *“25th Anniversary Edition”*).\n",
    "* These labels were highly inconsistent and offered **little predictive or business relevance** for engagement or recommendation tasks.\n",
    "\n",
    "**Decision:**\n",
    "Given the low completeness, high cardinality, and weak relationship to the business problem (member engagement and retention), the column will be dropped at the end steps of the cleaning process. Removing `edition` simplified the dataset without losing meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.append('edition')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5df342",
   "metadata": {},
   "source": [
    "#### **bbeVotes / bbeScore**\n",
    "\n",
    "Both `bbeVotes` and `bbeScore` were fully complete (100%) but required validation to confirm numeric consistency and detect potential anomalies.\n",
    "We will remove any formatting (commas, string types) and convert both columns to integers for accurate statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['bbeVotes'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['title_clean', 'bbeVotes', 'bbeScore', 'rating_clean', 'numRatings_log', 'likedPercent']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove formatting and convert to integers\n",
    "bbe_clean['bbeVotes_clean'] = (\n",
    "    bbe_clean['bbeVotes'].astype(str).str.replace(',', '').astype(int)\n",
    ")\n",
    "bbe_clean['bbeScore_clean'] = (\n",
    "    bbe_clean['bbeScore'].astype(str).str.replace(',', '').astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ab426",
   "metadata": {},
   "source": [
    "Next, we will verify if the data follows a long-tailed distribution typical of popularity metrics (few books accumulate thousands of votes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bbe_clean[['bbeVotes_clean', 'bbeScore_clean']].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b966d1",
   "metadata": {},
   "source": [
    " We identified one data quality issue: negative minimum in `bbeVotes` (likely an entry error). Marked for correction (`abs()` or `NaN` replacement depending on context). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count negative values\n",
    "bbe_clean[bbe_clean['bbeVotes'] < 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e984839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect negative values\n",
    "bbe_clean[bbe_clean['bbeVotes_clean'] < 0][['title_clean', 'bbeVotes_clean', 'bbeScore_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89509ad",
   "metadata": {},
   "source": [
    "Detected 41 invalid negative vote counts across valid book entries.\n",
    "Given that magnitudes ranged from –1 to –4 and titles were legitimate (e.g., _Men Explain Things to Me_, _Battle Angel Alita_), these were treated as sign errors. The correction applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the absolute value correction.\n",
    "bbe_clean['bbeVotes_clean'] = bbe_clean['bbeVotes_clean'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count negative values again to validate transformation\n",
    "bbe_clean[bbe_clean['bbeVotes_clean'] < 0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55079b7",
   "metadata": {},
   "source": [
    "Next, we will use boxplotting to inspect for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['bbeVotes_clean', 'bbeScore_clean']].plot(kind='box', subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = bbe_clean.nlargest(10, 'bbeVotes_clean')[['title_clean', 'bbeVotes_clean', 'bbeScore_clean']]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53eec3",
   "metadata": {},
   "source": [
    " Boxplots reveal extreme but valid outliers for high-performing books (e.g. *The Hunger Games*, *Harry Potter*). Outliers were retained since they represent meaningful popularity signals rather than noise.\n",
    "\n",
    " After cleaning and validating, both variables are ready for downstream analysis (correlation, scaling, or feature engineering). No transformation beyond this stage is required during the cleaning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3305fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.extend(['bbeVotes', 'bbeScore'])\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12081e8e",
   "metadata": {},
   "source": [
    "#### **likedPercent**\n",
    "\n",
    "The `likedPercent` feature represents the proportion of users who marked a book as “liked” on Goodreads and serves as a key engagement indicator within the dataset. Before using it in modeling or analytics, we evaluated its completeness, distribution, and relationship to other popularity metrics (`bbeVotes`, `bbeScore`, and `rating`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['likedPercent'].head(10)\n",
    "bbe_clean['likedPercent'].value_counts(dropna=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a0088",
   "metadata": {},
   "source": [
    "The output implies that the data is already standardized. Still, we'll make sure the column is fully clean, consistent, and ready for use in analytics or ML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['likedPercent'].info()\n",
    "bbe_clean['likedPercent'].describe()\n",
    "bbe_clean['likedPercent'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dbb6f2",
   "metadata": {},
   "source": [
    "The `likedPercent` column contains a total of **42,585 entries**, of which **42,220 are non-null**, leaving only **365 missing values**, a very small proportion of the dataset. It is stored as a **`float64`** data type, meaning the values are numeric and ready for analysis without further type conversion. The column’s **memory usage of approximately 332.8 KB** confirms that it is highly efficient and lightweight to process.\n",
    "\n",
    "We will now compare the `likedPercent` against other available engagement signals, before deciding if data imputation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13590ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate missing and available likedPercent\n",
    "missing_likes = bbe_clean[bbe_clean['likedPercent'].isna()]\n",
    "present_likes = bbe_clean[bbe_clean['likedPercent'].notna()]\n",
    "\n",
    "print(f\"Missing likedPercent entries: {missing_likes.shape[0]}\")\n",
    "print(f\"Available likedPercent entries: {present_likes.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f1b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_compare = ['bbeVotes_clean', 'bbeScore_clean', 'rating_clean']\n",
    "\n",
    "missing_summary = missing_likes[cols_to_compare].describe()\n",
    "present_summary = present_likes[cols_to_compare].describe()\n",
    "\n",
    "display(missing_summary, present_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03588ca4",
   "metadata": {},
   "source": [
    "After comparing the 365 entries with missing `likedPercent` values against those with available data, we found a clear pattern:\n",
    "\n",
    "* Books without `likedPercent` have **extremely low engagement** (median `bbeVotes = 1`, mean `bbeScore ≈ 78`), while books with data show **much higher popularity metrics**.\n",
    "* **Average ratings** between the two groups were nearly identical (~4.0), confirming that missingness is **not linked to book quality** but rather to **limited user interaction**.\n",
    "* This indicates that `likedPercent` is **missing not at random**, it’s absent because the book doesn’t have enough user votes to compute the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3094ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep NaN values (do not impute)\n",
    "bbe_clean['likedPercent_clean'] = bbe_clean['likedPercent']\n",
    "\n",
    "# add binary engagement flag\n",
    "bbe_clean['has_likedPercent'] = bbe_clean['likedPercent_clean'].notna().astype(int)\n",
    "\n",
    "# verify the transformation\n",
    "print(bbe_clean['has_likedPercent'].value_counts(dropna=False))\n",
    "bbe_clean[['likedPercent', 'likedPercent_clean', 'has_likedPercent']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df9148",
   "metadata": {},
   "source": [
    "Instead of imputing artificial values, I chose to:\n",
    "\n",
    "1. **Retain NaN values** in `likedPercent_clean` to reflect the absence of sufficient engagement data.\n",
    "2. **Add a binary flag** `has_likedPercent` indicating whether the book has a valid engagement percentage (1:`True`/0:`False`).\n",
    "\n",
    "This preserves the semantic meaning of missing data, distinguishing **unrated or low-visibility titles** from those with measurable popularity, while avoiding bias that could be introduced through imputation.\n",
    "\n",
    "In later modeling stages, `has_likedPercent` can be used as a **predictor of audience visibility or engagement depth**, helping the system differentiate between *popular*, *niche*, and *newly added* books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8da169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.append('likedPercent')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "version = 12\n",
    "\n",
    "interim_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(interim_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} dataset saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f55f1",
   "metadata": {},
   "source": [
    "#### **Setting & Characters**\n",
    "\n",
    "\n",
    "After examining both columns, it became evident that they contain a large proportion of invalid or empty values (`[]`), accounting for approximately **70%** of entries in `characters` and **75%** in `setting`. Although technically non-null, these values hold no meaningful information. The remaining valid entries are highly sparse and mostly redundant with information already present in other features such as **`author`**, **`series`**, or **`genres`** (e.g. “Hogwarts School” or “Harry Potter” are already captured in the title). Given their poor density and low predictive potential, both columns were excluded from further modeling and visualization tasks. This decision simplifies the dataset while preserving its semantic richness, ensuring data integrity and relevance for feature engineering and downstream ML tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4944c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['setting'].info()\n",
    "bbe_clean['setting'].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['setting'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc472b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['setting'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211071dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_setting_mask = bbe_clean['setting'].str.strip() == '[]'\n",
    "empty_setting_count = empty_setting_mask.sum()\n",
    "empty_setting_share = (empty_setting_count / len(bbe_clean)) * 100\n",
    "\n",
    "print(f\"Empty strings: {empty_setting_count} ({empty_setting_share:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d930a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['setting'].unique()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['characters'].info()\n",
    "bbe_clean['characters'].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2491cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['characters'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca26ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['characters'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_characters_mask = bbe_clean['characters'].str.strip() == '[]'\n",
    "empty_characters_count = empty_characters_mask.sum()\n",
    "empty_characters_share = (empty_characters_count / len(bbe_clean)) * 100\n",
    "\n",
    "print(f\"Empty strings: {empty_characters_count} ({empty_characters_share:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d989a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['characters'].unique()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce76767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.extend(['setting', 'characters'])\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53b39e",
   "metadata": {},
   "source": [
    "#### **Price**\n",
    "\n",
    "Next, we examined the structure of the dataset to identify any irregularities in the `price` feature. The analysis revealed **9,076 missing (`NaN`) values**, with no other non-numeric entries present. This confirms that all invalid records stem solely from missing data rather than formatting errors or unexpected symbols. \n",
    "We will then create a generic price cleaning function that can be re-used after data imputation, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['price'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36311080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count NaN values\n",
    "is_nan = bbe_clean['price'].isna().sum()\n",
    "print(f\"NaN values: {is_nan}\")\n",
    "\n",
    "# Identify non-numeric characters\n",
    "non_numeric = (\n",
    "    bbe_clean['price']\n",
    "    .astype(str)\n",
    "    .str.extractall(r'([^\\d.,\\-])')[0]  # extract all non-numeric symbols and select first column\n",
    "    .value_counts()\n",
    ")\n",
    "\n",
    "print(f\"Non-numeric values:\\n{non_numeric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0101f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['price'].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_price(value):\n",
    "    \"\"\"Standardize price values by handling missing, textual, and range inputs.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "\n",
    "    s = str(value).strip().lower()\n",
    "\n",
    "    # handle common invalid or missing cases\n",
    "    if s in {'free', 'none', 'nan', '', 'n/a', '-', '—'}:\n",
    "        return np.nan\n",
    "\n",
    "    # remove non-numeric symbols\n",
    "    s = re.sub(r'[^\\d.,\\-]', '', s)\n",
    "\n",
    "    # handle ranges like '12-15' -> mean of both\n",
    "    if '-' in s:\n",
    "        try:\n",
    "            nums = [float(x) for x in re.split(r'[-–]', s) if x.strip()]\n",
    "            return np.mean(nums) if nums else np.nan\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    # convert to float, replacing commas with dots\n",
    "    try:\n",
    "        return float(s.replace(',', '.'))\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    \n",
    "# apply cleaning\n",
    "bbe_clean['price_clean'] = bbe_clean['price'].apply(clean_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8602ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean[['title_clean','price_clean', 'price']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Missing share: {bbe_clean['price_clean'].isna().mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_clean['price_clean'].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f212e",
   "metadata": {},
   "source": [
    "After cleaning, we generated a descriptive summary of the **`price_clean`** feature. The results revealed a **minimum price of 0.84** and a **maximum of 898.64**, both of which appear to be unrealistic given the median value of approximately **5.16**. These extreme observations likely represent data entry errors or atypical cases that could distort the overall distribution.\n",
    "\n",
    "Initially, we considered using the **Interquartile Range (IQR)** method to cap these outliers and stabilize the distribution. However, upon review, some of the high values correspond to legitimate specialized titles (e.g., academic or collector editions), while others seem erroneous. To balance statistical consistency with contextual accuracy, we decided to **flag suspicious or missing prices for external validation** and **impute them using verified data from the Google Books API**, ensuring a more reliable and context-aware price feature for subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa19c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_price_anomalies(df, col='price_clean', low=1.0, high=50.0):\n",
    "    \"\"\"\n",
    "    Flags books with missing or extreme prices for external validation.\n",
    "    \"\"\"\n",
    "    return df[col].isna() | (df[col] < low) | (df[col] > high)\n",
    "\n",
    "bbe_clean['price_flag'] = flag_price_anomalies(bbe_clean)\n",
    "print(bbe_clean['price_flag'].sum(), \"books flagged for external imputation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_bbe_cols.append('price')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd359bda",
   "metadata": {},
   "source": [
    "#### **cover**\n",
    "\n",
    "Although the datasets include book cover image URLs, these variables were excluded from the current analysis because the project’s focus is on **tabular predictive modelling**, not **computer vision**. According to the project scope and CRISP-DM methodology, the objective at this stage is to develop explainable, data-driven insights and prediction pipelines based on structured metadata (e.g. ratings, genres, publication dates, and author features). Image analysis would require a separate preprocessing workflow (e.g. feature extraction with CNNs) and computational resources beyond the needs of this business problem. Since cover images do not contribute directly to the numerical or categorical predictors in the current modelling pipeline, they were dropped to streamline the dataset and maintain alignment with the **tabular data preparation and ML evaluation criteria** outlined in the assessment guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_bbe_cols.append('coverImg')\n",
    "drop_bbe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725db71",
   "metadata": {},
   "source": [
    "#### Feature selection and Column Preservation\n",
    "\n",
    "Before removing non-predictive fields, we save the relevant BBE publisher columns for future imputation.\n",
    "After exporting these originals, the cleaned dataset retains only the features needed for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure folder exists\n",
    "bbe_save_path = Path(\"data/interim/bbe\")\n",
    "os.makedirs(bbe_save_path, exist_ok=True)\n",
    "\n",
    "# find which columns actually exist in the dataframe\n",
    "existing_drops = [c for c in drop_bbe_cols if c in bbe_clean.columns]\n",
    "print(f\"Will save {len(existing_drops)} columns: {existing_drops}\")\n",
    "\n",
    "# extract the columns before dropping\n",
    "cols_to_save = bbe_clean[existing_drops]\n",
    "\n",
    "# save to CSV\n",
    "cols_to_save.to_csv(os.path.join(bbe_save_path, \"publisher_columns_for_imputation.csv\"), index=False)\n",
    "\n",
    "# save as pickle\n",
    "cols_to_save.to_pickle(os.path.join(bbe_save_path, \"publisher_columns_for_imputation.pkl\"))\n",
    "\n",
    "print(\"Saved original columns.\")\n",
    "\n",
    "# now drop from the main dataframe\n",
    "bbe_clean = bbe_clean.drop(columns=existing_drops, errors='ignore')\n",
    "print(f\"Remaining columns: {len(bbe_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4905d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "clean_bbe_path = Path(\"data/interim/bbe\")\n",
    "clean_bbe_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 13\n",
    "\n",
    "clean_bbe_path = Path(\"data/interim/bbe\")\n",
    "\n",
    "bbe_clean.to_csv(clean_bbe_path / f\"bbe_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Cleaned v{version} dataset saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fdbc1",
   "metadata": {},
   "source": [
    "### Goodbooks-10k\n",
    "\n",
    "- Handle identifier columns\n",
    "- Standardize key columns: `author`, `language`\n",
    "- Missing data handling strategies\n",
    "- Normalize genre and format\n",
    "- Validate for no nulls or duplicates\n",
    "\n",
    "#### **Identifiers**\n",
    "\n",
    "We will validate and clean all identifier columns in the Goodbooks-10k dataset (`goodreads_id_clean`, `best_book_id`, `work_id`).\n",
    "These identifiers are crucial for merging datasets (Goodbooks and BBE) and for potential API or web-scraping enrichment.\n",
    "We aim to ensure consistency, detect duplicates, handle missing or inconsistent IDs, and prepare for cross-dataset linking.\n",
    "\n",
    "#####  **goodreads_id_clean**\n",
    "\n",
    "On the previous notebook (**01_Data_Collection**), we ensured that `goodreads_id_clean` was correctly extracted and formatted as a numeric identifier. In this step, we confirmed that:\n",
    "\n",
    "- The column is **complete** (10,000 non-null entries)\n",
    "- It has **no duplicates**\n",
    "- All values are **numeric** and within the expected Goodreads range (1 → 33 million)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify existing identifier cleaning\n",
    "print(\"Missing values:\", books_clean['goodreads_id_clean'].isna().sum())\n",
    "print(\"Duplicates:\", books_clean['goodreads_id_clean'].duplicated().sum())\n",
    "print(\"Unique IDs:\", books_clean['goodreads_id_clean'].nunique())\n",
    "books_clean['goodreads_id_clean'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate best_book_id\n",
    "books_clean['best_book_id_clean'] = pd.to_numeric(books_clean['best_book_id'], errors='coerce')\n",
    "\n",
    "print(\"Missing values:\", books_clean['best_book_id'].isna().sum())\n",
    "print(\"Duplicates:\", books_clean['best_book_id'].duplicated().sum())\n",
    "print(\"Unique IDs:\", books_clean['best_book_id'].nunique())\n",
    "\n",
    "books_clean[['goodreads_id_clean', 'best_book_id_clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify one-to-one relationship between IDs\n",
    "id_match = (books_clean['goodreads_id_clean'] == books_clean['best_book_id_clean']).mean()\n",
    "print(f\"Share of matching IDs: {id_match:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify mismatched IDs\n",
    "id_mismatch = books_clean[books_clean['goodreads_id_clean'] != books_clean['best_book_id_clean']]\n",
    "print(f\"Mismatched rows: {len(id_mismatch)} ({len(id_mismatch)/len(books_clean)*100:.2f}%)\")\n",
    "\n",
    "# Preview examples\n",
    "id_mismatch[['title', 'authors', 'goodreads_id_clean', 'best_book_id_clean']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f47f24",
   "metadata": {},
   "source": [
    "##### **Variance Between goodreads_id_clean and best_book_id**\n",
    "\n",
    "We identified **241 mismatched rows (2.41%)** between `goodreads_id_clean` and `best_book_id`.  \n",
    "Inspection of these entries (e.g., *Gone Girl*, *Memoirs of a Geisha*, *American Gods*) confirms that they represent **different editions or translations** of the same literary work.\n",
    "\n",
    "This behaviour is consistent with Goodreads’ internal structure:\n",
    "- **`goodreads_id_clean`** refers to a specific *book edition*.\n",
    "- **`best_book_id`** points to the edition Goodreads considers the *canonical or most-rated version* of that work.\n",
    "\n",
    "For consistency across datasets, we will:\n",
    "- Keep **`goodreads_id_clean`** as the **primary identifier** for joins and merges.  \n",
    "- Retain **`best_book_id`** for reference or edition-level enrichment if needed later.  \n",
    "- Handle potential edition groupings through the `work_id` field in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2af4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate work_id\n",
    "books_clean['work_id_clean'] = pd.to_numeric(books_clean['work_id'], errors='coerce')\n",
    "\n",
    "print(\"Missing values:\", books_clean['work_id'].isna().sum())\n",
    "print(\"Duplicates:\", books_clean['work_id'].duplicated().sum())\n",
    "print(\"Unique IDs:\", books_clean['work_id'].nunique())\n",
    "\n",
    "# check relation between work_id and other IDs\n",
    "mapping_check = (\n",
    "    books_clean.groupby('work_id_clean')['goodreads_id_clean']\n",
    "    .nunique()\n",
    "    .reset_index(name='books_per_work')\n",
    ")\n",
    "\n",
    "print(mapping_check['books_per_work'].describe())\n",
    "mapping_check.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee0f46",
   "metadata": {},
   "source": [
    "##### **work_id**\n",
    "\n",
    "After converting to numeric and checking relationships, we found that all `work_id` values are **unique (10,000)**. This indicates a **one-to-one mapping** between `work_id`, `goodreads_id_clean`, and `best_book_id`, unlike the original Goodreads structure where multiple editions share the same work ID.\n",
    "\n",
    "This suggests that the Goodbooks-10k dataset used here has already been normalized to a single representative edition per work.  \n",
    "Therefore:\n",
    "- `work_id` behaves effectively as another unique identifier, not a grouping key.  \n",
    "- All three ID columns (`goodreads_id_clean`, `best_book_id`, and `work_id`) can be used interchangeably for joins.  \n",
    "- We’ll still retain `work_id` for completeness and potential metadata lookups, but we don’t need to aggregate or deduplicate by it.\n",
    "\n",
    "Since all identifiers are clean, unique, and consistent, the identifier-cleaning phase is complete.  \n",
    "Next, we will consolidate them into a single reference table for future merges and cross-dataset integrations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba265ec",
   "metadata": {},
   "source": [
    "#### **author**\n",
    "\n",
    "We reused the same **`clean_and_split_authors()`** function previously developed for the BBE dataset to ensure consistent author formatting across datasets. This function standardizes delimiters, removes unwanted characters, and splits multi-author strings into clean, lowercase lists. We then generated two derived fields: **`authors_list`**, a Python list of clean author names, and **`author_clean`**, a readable comma-separated string for display and grouping. This process harmonizes author data, enabling reliable joins and comparisons between datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to clean function to goodbooks-10k dataset\n",
    "books_clean[\"authors_list\"] = books_clean[\"authors\"].apply(clean_and_split_authors)\n",
    "books_clean[\"author_clean\"] = books_clean[\"authors_list\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else None)\n",
    "\n",
    "# Quick check\n",
    "books_clean[[\"authors\", \"author_clean\", \"authors_list\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814299f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_gb_cols = ['authors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb861622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "interim_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 1\n",
    "\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "\n",
    "books_clean.to_csv(interim_gb_path / f\"books_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} dataset saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc912a",
   "metadata": {},
   "source": [
    "#### **language_code**\n",
    "\n",
    "This step focused on standardizing the `language_code` feature to ensure consistency across datasets. Using a predefined mapping dictionary, all language identifiers were normalized to two-letter ISO codes, and regional variants (e.g., `\"en-US\"`, `\"en-GB\"`, `\"en-CA\"`) were unified under a single `\"en\"` label. This standardization facilitates accurate language-based analysis and future merging with external datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d903b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique language values\n",
    "print(\"Unique language values in GB dataset:\")\n",
    "books_clean['language_code'] = books_clean['language_code'].astype(str).str.strip()\n",
    "gb_unique_languages = books_clean['language_code'].unique()\n",
    "\n",
    "print(f\"\\nTotal unique values: {len(gb_unique_languages)}\\n\")\n",
    "print(gb_unique_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"src/cleaning/mappings/languages_dict.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    languages_dict = json.load(f)\n",
    "\n",
    "# Apply dictionary\n",
    "books_clean['language_clean'] = books_clean['language_code'].str.lower().map(languages_dict)\n",
    "\n",
    "# Fill remaining NaNs\n",
    "books_clean['language_clean'] = books_clean['language_clean'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d319e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again for unique language values\n",
    "print(\"Unique language values in BBE dataset:\")\n",
    "gb_unique_languages = books_clean['language_clean'].unique()\n",
    "\n",
    "print(f\"\\nTotal unique values: {len(gb_unique_languages)}\\n\")\n",
    "print(gb_unique_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_breakdown = (\n",
    "    books_clean['language_clean']\n",
    "    .value_counts()\n",
    "    .to_frame('count')\n",
    ")\n",
    "\n",
    "language_breakdown['percentage'] = (\n",
    "    language_breakdown['count'] / len(books_clean) * 100\n",
    ").round(2)\n",
    "\n",
    "print(language_breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a506b2d",
   "metadata": {},
   "source": [
    "After applying the language mapping from `languages_dict.json`, around **35%** of the entries were initially labeled as `\"unknown\"`. Upon inspection, most of these corresponded to **regional English codes** such as `\"en-US\"`, `\"en-GB\"`, and `\"en-CA\"`, which were not included in the initial mapping. These variants were standardized to `\"en\"` to ensure consistency across datasets. After this correction, only **1,084 records (≈10%)** remain genuinely missing and are retained as `\"unknown\"` for transparency in downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect raw codes that became 'unknown'\n",
    "unknown_mask = books_clean['language_clean'] == 'unknown'\n",
    "books_clean.loc[unknown_mask, 'language_code'].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1dbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean.loc[books_clean['language_clean'].isin(['unknown']), ['title', 'author_clean','language_code']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9083b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean.loc[books_clean['language_clean'].isin(['unknown']), 'author_clean'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean.loc[books_clean['language_clean'].isin(['unknown']), 'author_clean'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e4a8d6",
   "metadata": {},
   "source": [
    "After standardizing regional language variants, only **1,084 records** remained with `language_clean = \"unknown\"`. Inspection revealed these books are mostly written by well-known English-language authors (e.g., *Darren Shan, Dean Koontz, John Grisham*), indicating missing metadata rather than true ambiguity. To preserve data integrity, these values were **kept as `\"unknown\"`** for now. Imputation will be addressed **after merging with the BBE dataset**, leveraging cross-dataset matches to infer missing languages where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_gb_cols.append('language_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35549a2e",
   "metadata": {},
   "source": [
    "#### **original publication year**\n",
    "\n",
    "The `original_publication_year` field was converted into a standardized datetime format to enable consistent temporal analysis and easier merging with BBE dataset. Since the source only contained year values, a default placeholder date (`YYYY-01-01`) was applied. Missing or invalid years were safely handled and converted using `np.nan`, preserving them for potential imputation later in the workflow. This transformation ensures all publication dates are now numeric and compatible with pandas’ datetime operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b74ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean[['title','original_publication_year']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean[['original_publication_year']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean.loc[books_clean['original_publication_year'].isna(), ['title','original_publication_year']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "books_clean['publication_date_clean'] = pd.to_datetime(\n",
    "    books_clean['original_publication_year']\n",
    "        .apply(lambda x: f\"{int(x)}-01-01\" if pd.notna(x) else np.nan),\n",
    "    errors='coerce'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eea6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean[['title', 'publication_date_clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945eb87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['publication_date_clean'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc0aa9",
   "metadata": {},
   "source": [
    "After converting `original_publication_year` to a datetime format, 98.85% of records successfully generated a valid `publication_date_clean`. Only 115 entries (≈1.15%) contained missing or invalid year values and were preserved as `NaN` for future imputation or exclusion during analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ebc65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_gb_cols.append('original_publication_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1487c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "interim_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 2\n",
    "\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "\n",
    "books_clean.to_csv(interim_gb_path / f\"books_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} dataset saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0dffb",
   "metadata": {},
   "source": [
    "#### **ISBN / ISBN13**\n",
    "\n",
    "Both `isbn` and `isbn13` fields contained overlapping but inconsistent identifiers. To ensure compatibility across datasets and prepare for downstream transformations, we standardized all ISBNs into a single `isbn_standard` field:\n",
    "\n",
    "- Convert valid `isbn` and `isbn13` values to strings (preserving leading zeros).\n",
    "- Clean `isbn` and `isbn13` values using a custom function (`clean_isbn`) that removes noise, validates format, and converts ISBNs into standardized format.\n",
    "- Unify both sources under `isbn_standard`, ensuring a consistent identifier format for analysis and integration tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676cdc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect ISBN column\n",
    "books_clean[['title','isbn', 'isbn13']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['isbn'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['isbn13'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure string type for isbn13\n",
    "books_clean['isbn13_str'] = (\n",
    "    books_clean['isbn13']\n",
    "    .apply(lambda x: f\"{int(x):013d}\" if pd.notna(x) else np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f10792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing and invalid patterns\n",
    "gb_n_missing_isbn = books_clean['isbn'].isna().sum()\n",
    "gb_n_missing_isbn13 = books_clean['isbn13_str'].isna().sum()\n",
    "print(f'Number of missing ISBN entries: {gb_n_missing_isbn}')\n",
    "print(f'Number of missing ISBN-13 entries: {gb_n_missing_isbn}13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273200ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify invalid placeholders (like 9999999999999)\n",
    "gb_n_invalid_isbn = books_clean[books_clean['isbn'].astype(str).str.contains('9999999999')].shape[0]\n",
    "gb_n_invalid_isbn13 = books_clean[books_clean['isbn13_str'].astype(str).str.contains('9999999999')].shape[0]\n",
    "print(f'Number of placeholder ISBN entries: {gb_n_invalid_isbn}')\n",
    "print(f'Number of placeholder ISBN-13 entries: {gb_n_invalid_isbn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['asin'] = books_clean['isbn'].apply(detect_asin)\n",
    "books_clean['asin'] = books_clean['isbn13_str'].apply(detect_asin)\n",
    "gb_has_asin = books_clean[books_clean['asin'].notna()] \n",
    "print(f'Books with ASINs: {len(gb_has_asin)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e593c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and merge in one go\n",
    "books_clean['isbn_clean'] = books_clean.apply(\n",
    "    lambda row: (\n",
    "        clean_isbn(row['isbn13_str'])\n",
    "        if pd.notna(row['isbn13_str'])\n",
    "        else clean_isbn(row['isbn'])\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5858cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean[['title','isbn', 'isbn13_str', 'isbn_clean']].sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1812847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_gb_cols.extend(['isbn', 'isbn13', 'isbn13_str'])\n",
    "drop_gb_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af603f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "interim_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 3\n",
    "\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "\n",
    "books_clean.to_csv(interim_gb_path / f\"books_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} dataset saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a92d2",
   "metadata": {},
   "source": [
    "#### **average_rating**\n",
    "\n",
    "In this step, we evaluate the **`average_rating`** feature from the *Goodbooks-10k* dataset to ensure it is complete, valid, and ready for analysis. \n",
    "We will verify completeness and data type of the `average_rating` feature using `.info()` method, confirm that values fall within the valid rating range (1-0) using `.describe()` method, examine rating distribution for anomalies or skew by plotting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f47af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where rating is not NaN\n",
    "gb_total_books = len(books_clean)\n",
    "gb_has_ratings = books_clean[books_clean['average_rating'].notna()]\n",
    "gb_has_ratings_num = gb_has_ratings.shape[0]\n",
    "gb_share_ratings = gb_has_ratings_num / gb_total_books * 100\n",
    "\n",
    "# Print the number of titles with ratings and show the first few examples\n",
    "print(f\"Books with ratings: {gb_has_ratings_num} of {gb_total_books} ({gb_share_ratings:.2f}%)\")\n",
    "gb_has_ratings[['title', 'average_rating', 'ratings_count', 'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04101995",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['average_rating'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99174054",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['average_rating'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "books_clean['average_rating'].hist(bins=20)\n",
    "plt.title('Distribution of Average Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e891b2",
   "metadata": {},
   "source": [
    "The `average_rating` feature was fully populated (100% completeness) with no missing values.  \n",
    "Additional validation confirmed all values fell within the expected 1–5 range, consistent with Goodreads rating conventions.  \n",
    "The mean rating (≈4.00) and relatively small standard deviation (0.25) suggest a well-distributed dataset without rating bias or truncation issues.  \n",
    "No further cleaning was required beyond range validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0271ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['rating_clean'] = books_clean['average_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_gb_cols.append('average_rating')\n",
    "drop_gb_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24f55d",
   "metadata": {},
   "source": [
    "#### **work_ratings_count**\n",
    "\n",
    "We analysed `ratings_count`, `average_ratings` and rating-level and identified a high mismatches (**99.86%**). After analysing `work_ratings_count` we concluded that the averages and rating-level data referred to the work data (`work_ratings_count`) rather than the edition level (`ratings_count`). We then decided to proceed with marking `ratings_count` to be dropped and using `work_ratings_count` instead, as it represents better the reception of the entire work, and not a particular edition.\n",
    "\n",
    " We will use `.describe()` method to check data type and a descriptive distribution analysis. Similar to what we did in the BBE database, we will also apply a logarithmic transformation (log1p) to smooth out the long tail and reveal underlying patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6874dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if ratings match rating-level data\n",
    "books_clean['ratings_sum'] = books_clean[[f'ratings_{i}' for i in range(1,6)]].sum(axis=1)\n",
    "print((books_clean['ratings_sum'] == books_clean['work_ratings_count']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c71fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# calculate average rating using the work-level breakdown\n",
    "books_clean['avg_from_work'] = (\n",
    "    books_clean.apply(lambda x: sum((i+1) * x[f'ratings_{i+1}'] for i in range(5)), axis=1)\n",
    "    / books_clean['work_ratings_count']\n",
    ")\n",
    "\n",
    "# compare to the original average_rating\n",
    "comparison = (books_clean['average_rating'].round(2) == books_clean['avg_from_work'].round(2))\n",
    "match_rate = comparison.mean() * 100\n",
    "\n",
    "print(f\"Matching average ratings (based on work_ratings_count): {match_rate:.2f}%\")\n",
    "\n",
    "# inspect any mismatches\n",
    "mismatched = books_clean.loc[~comparison, ['average_rating', 'avg_from_work', 'work_ratings_count'] + [f'ratings_{i}' for i in range(1,6)]]\n",
    "print(f\"Mismatched rows: {len(mismatched)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_na_ratings_count = books_clean[books_clean['work_ratings_count'].isna()]\n",
    "gb_na_ratings_count_num = gb_na_ratings_count.shape[0]\n",
    "gb_share_ratings_count = gb_na_ratings_count_num / gb_total_books * 100\n",
    "\n",
    "# Print the number of titles with no ratings\n",
    "print(f\"Books with no ratings_count values: {gb_na_ratings_count_num} of {gb_total_books} ({gb_share_ratings_count}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd261a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['work_ratings_count'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(np.log1p(books_clean['work_ratings_count']), bins=50)\n",
    "plt.title(\"Distribution of Log(Number of Ratings)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfcb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['numRatings_clean'] = books_clean['work_ratings_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the count to reduce skew\n",
    "books_clean['numRatings_log'] = np.log1p(books_clean['work_ratings_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d57d9",
   "metadata": {},
   "source": [
    "- The `work_ratings_count` field was fully populated, **no missing or invalid values** were detected among 10,000 entries.  \n",
    "- The counts range from **5,510 to 4,942,365**, confirming a highly skewed popularity distribution where a few titles dominate in visibility.  \n",
    "- After applying the **logarithmic transformation (`log1p`)**, the distribution became approximately **normal**, revealing clearer clustering patterns of moderately and highly rated books.  \n",
    "- This confirms that the feature is both valid and **ready for downstream modeling**, without requiring imputation or capping.  \n",
    "- The transformed field `numRatings_log` will be used in correlation and feature-engineering steps to reduce skew impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_gb_cols.extend(['ratings_count', 'ratings_sum', 'avg_from_work', 'work_ratings_count'])\n",
    "drop_gb_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b219bb4",
   "metadata": {},
   "source": [
    "#### **ratings_1–ratings_5**\n",
    "\n",
    "The GoodBooks-10k dataset includes five columns, `ratings_1` through `ratings_5`, representing the number of users who assigned each star level to a book. This distribution provides valuable insights into rating sentiment and variance beyond the mean score.\n",
    "\n",
    "In this step, we will verify for completeness and consistency across all five rating-level columns; ensure each value is numeric and non-negative; check that their sum matches `ratings_count`; normalize the distribution and create a combined `ratings_distribution` for easier analysis.\n",
    "\n",
    "The result will be clean, normalized representation of each book’s rating shape, showing the share of 1–5-star ratings.\n",
    "This feature will support later comparisons with the BBE dataset and correlation analysis between rating structure and popularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1edb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_cols = ['ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5']\n",
    "\n",
    "# check missing or invalid values\n",
    "missing_star_counts = books_clean[rating_cols].isna().sum()\n",
    "print(\"Missing values per rating column:\\n\", missing_star_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure numeric and non-negative\n",
    "books_clean[rating_cols] = books_clean[rating_cols].apply(pd.to_numeric, errors='coerce')\n",
    "for col in rating_cols:\n",
    "    invalid = (books_clean[col] < 0).sum()\n",
    "    if invalid > 0:\n",
    "        print(f\"Negative values detected in {col}: {invalid}\")\n",
    "        books_clean.loc[books_clean[col] < 0, col] = np.nan\n",
    "    else:\n",
    "        print(f'No negative values detected in {col}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rating_cols = [f'ratings_{i}' for i in range(1,6)]\n",
    "\n",
    "#  star-level totals vs work total\n",
    "books_clean['ratings_sum'] = books_clean[rating_cols].sum(axis=1)\n",
    "eq_rate = (books_clean['ratings_sum'] == books_clean['numRatings_clean']).mean()\n",
    "print(f\"ratings_sum == numRatings_clean proportion: {eq_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcccb51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_cols = [f'ratings_{i}' for i in range(1,6)]\n",
    "\n",
    "# normalize each star-level count into a share of total work ratings\n",
    "for i in range(1, 6):\n",
    "    books_clean[f'ratings_{i}_share'] = books_clean[f'ratings_{i}'] / books_clean['numRatings_clean']\n",
    "\n",
    "# check all be close to 1.0\n",
    "books_clean['ratings_share_sum'] = books_clean[[f'ratings_{i}_share' for i in range(1,6)]].sum(axis=1)\n",
    "print(books_clean['ratings_share_sum'].describe().round(3))\n",
    "\n",
    "# calculate mean share per star level for plotting\n",
    "mean_shares = [books_clean[f'ratings_{i}_share'].mean() for i in range(1,6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(1,6), mean_shares)\n",
    "plt.title(\"Average Star Rating Distribution (GoodBooks-10k)\")\n",
    "plt.xlabel(\"Star Rating\")\n",
    "plt.ylabel(\"Average Share of Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c793124",
   "metadata": {},
   "source": [
    "Lastly, we computed mean star-share distributions and plotted comparative bar charts to visualize global reader sentiment patterns.\n",
    "Goodbooks-10k dataset also revealed a similar right-skewed pattern, where 4- and 5-star ratings dominate, typical of user-generated book reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff68018",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_gb_cols.extend(['ratings_share_sum', 'ratings_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259cd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "interim_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 4\n",
    "\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "\n",
    "books_clean.to_csv(interim_gb_path / f\"books_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} dataset saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c398dd",
   "metadata": {},
   "source": [
    "#### **work_text_reviews_count**\n",
    "\n",
    "This feature represents the total number of written reviews per book work in the Goodreads dataset. Since reviews are strong engagement indicators, ensuring data completeness and consistency is essential before using this variable for correlation or predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean[['title', 'work_text_reviews_count', 'rating_clean']].sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6001c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['work_text_reviews_count'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['work_text_reviews_count'].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5edfa4",
   "metadata": {},
   "source": [
    "After initial inspection, we conclude that the column is fully complete (10000 non-null int64), meaning no missing data.The range varies from 3 to 155,254, with a highly right-skewed distribution, reflecting that a small number of books accumulate a disproportionately high number of reviews. \n",
    "\n",
    "The mean review count (**~2,920**) is notably higher than the median (**~1,402**), further confirming positive skewness.\n",
    "\n",
    "This field is ready for normalization or log transformation before feature correlation analysis, particularly if we plan to integrate it into engagement or popularity metrics (e.g. combining with `work_ratings_count`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear histogram\n",
    "books_clean['work_text_reviews_count'].hist(bins=50)\n",
    "plt.title('Distribution of Text Reviews')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# log histogram\n",
    "books_clean['work_text_reviews_count'].hist(bins=50)\n",
    "plt.yscale('log')\n",
    "plt.title('Distribution of Text Reviews (log scale)')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eaa47b",
   "metadata": {},
   "source": [
    "The histogram (both linear and log-scaled) clearly illustrated a long-tail distribution, typical of digital engagement data, where a few highly popular titles dominate user activity. To correct this imbalance and make the feature more analytically useful, a log transformation was applied.\n",
    "\n",
    "**_work_text_reviews_log=log(1+work_text_reviews_count)_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ab39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['work_text_reviews_log'] = np.log1p(books_clean['work_text_reviews_count'])\n",
    "books_clean['work_text_reviews_log'].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1abb693",
   "metadata": {},
   "source": [
    "Next, we will inspect outliers to confirm assumptions of top 1% reviews reflecting popular books with high ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold as top 1% of review counts\n",
    "threshold = books_clean['work_text_reviews_count'].quantile(0.99)\n",
    "\n",
    "# mask for high-review books\n",
    "mask_high_reviews = books_clean['work_text_reviews_count'] > threshold\n",
    "\n",
    "# inspect books\n",
    "books_clean.loc[mask_high_reviews, ['title', 'author_clean', 'work_text_reviews_count', 'rating_clean']].sort_values(by='work_text_reviews_count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dcf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(\n",
    "    books_clean['work_text_reviews_count'],\n",
    "    books_clean['rating_clean'],\n",
    "    alpha=0.5, edgecolors='none'\n",
    ")\n",
    "\n",
    "plt.xscale('log')  # log scale to handle skew\n",
    "plt.title('Review Count vs. Average Rating')\n",
    "plt.xlabel('Number of Text Reviews (log scale)')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a6aa48",
   "metadata": {},
   "source": [
    "A scatter plot between the number of text reviews (log scale) and average rating reveals that:\n",
    "\n",
    "- Most high-engagement books (right side of the plot) cluster around ratings between 3.8 and 4.5, suggesting consistent audience approval.\n",
    "\n",
    "- The most reviewed books include globally popular titles such as The Hunger Games, The Fault in Our Stars, Gone Girl, and The Book Thief, all of which are known for strong reader engagement.\n",
    "\n",
    "- Outliers with extremely high review counts but moderate ratings (e.g., Twilight at 3.57) indicate divisive titles that still attract massive attention, a useful insight for understanding popularity vs. sentiment dynamics.\n",
    "\n",
    "This analysis confirms that `work_text_reviews_count` is a reliable engagement proxy, strongly correlated with book popularity and visibility. The transformation and inspection validate its readiness for inclusion in subsequent modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "interim_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 5\n",
    "\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "\n",
    "books_clean.to_csv(interim_gb_path / f\"books_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} dataset saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean = pd.read_csv(\"data/interim/goodbooks/books_clean_v5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18915d8e",
   "metadata": {},
   "source": [
    "#### **title**\n",
    "\n",
    "To ensure consistency, we cleaned the `title` column into a new feature `title_clean` using the previously created `clean_title` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_clean['title_clean'] = books_clean['title'].apply(clean_title)\n",
    "books_clean['title_clean'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1dd55",
   "metadata": {},
   "source": [
    "We examined potential duplicates based on the cleaned title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_dup_raw = books_clean[books_clean.duplicated(subset='title_clean', keep=False)]\n",
    "gb_dup_raw.sort_values('title_clean').head(20)[\n",
    "    ['title_clean', 'author_clean', 'language_clean', 'rating_clean', 'numRatings_clean']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89be9f",
   "metadata": {},
   "source": [
    "Some duplicate groups could contain missing or empty titles. We identified these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mask_no_title = gb_dup_raw['title_clean'].isna() | (gb_dup_raw['title_clean'] == \"\")\n",
    "gb_no_title_books = gb_dup_raw[gb_mask_no_title]\n",
    "\n",
    "print(f\"Books without title: {gb_no_title_books.shape[0]}\")\n",
    "\n",
    "gb_identifier_cols = ['isbn_clean', 'goodreads_id_clean']\n",
    "print(\"\\nAvailable identifiers for missing-title rows:\")\n",
    "print(gb_no_title_books[gb_identifier_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3213d",
   "metadata": {},
   "source": [
    "As done previously in the BBE dataset, we kept only one version of each (`title_clean`, `author_clean`) pair, keeping the record with the highest `numRatings_clean`.\n",
    "This ensures that highly validated editions are retained, while preventing unintended removal of books that share titles but have different authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep highest numRatings version per (title_clean, author_clean)\n",
    "books_unique = (\n",
    "    books_clean.sort_values('numRatings_clean', ascending=False)\n",
    "          .drop_duplicates(subset=['title_clean', 'author_clean'], keep='first')\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Books after deduplication: {len(books_unique)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: ensure unique title-author combinations\n",
    "gb_duplicates_check = books_unique[\n",
    "    books_unique.duplicated(subset=['title_clean', 'author_clean'], keep=False)\n",
    "]\n",
    "\n",
    "if gb_duplicates_check.empty:\n",
    "    print(\"Sanity check passed: No duplicate (title_clean, author_clean) pairs remain.\")\n",
    "else:\n",
    "    print(\"Warning: Duplicates still exist — review these cases:\")\n",
    "    display(gb_duplicates_check[['title_clean', 'author_clean', 'numRatings_clean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b01fcf",
   "metadata": {},
   "source": [
    "At this stage, the dataset has been:\n",
    "\n",
    "- cleaned and standardized,\n",
    "- titles normalized,\n",
    "- structural duplicates removed,\n",
    "- identifier gaps highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c200fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_gb_cols.append('title')\n",
    "drop_gb_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "interim_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 6\n",
    "\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "\n",
    "books_clean.to_csv(interim_gb_path / f\"books_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"Interim v{version} dataset saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56871bf",
   "metadata": {},
   "source": [
    "#### **books_count**\n",
    "\n",
    "The `books_count` variable in the Goodbooks-10k dataset represents the total number of printed or edition-level records for each book.  \n",
    "During expert consultations, we learned that in real-world publishing, print volume can influence selection decisions. However, this feature is unavailable in the Best Books Ever (BBE) dataset, which represents our *current candidate books* for recommendation.\n",
    "\n",
    "**Rationale for Removal:**  \n",
    "- **Dataset alignment:** `books_count` is missing from BBE, preventing consistent feature comparison across both datasets.  \n",
    "- **Predictive value:** The variable reflects publishing logistics rather than reader engagement or satisfaction—key factors driving subscription retention.  \n",
    "- **Model generalization:** Retaining this feature could bias predictions toward historically overprinted books, reducing the system’s ability to identify emerging, high-quality titles.  \n",
    "\n",
    "**Validation Step:**  \n",
    "Before dropping, a quick check of feature relevance via correlatoin showed minimal association (**<0.5**) with engagement proxies (`work_ratings_count`, `average_rating`). This confirms that excluding it will not reduce predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c040151",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['books_count', 'work_ratings_count', 'average_rating']\n",
    "\n",
    "subset = books_clean[cols].dropna()\n",
    "\n",
    "corr = subset.corr(method='pearson')\n",
    "\n",
    "print(corr)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation between books_count and Engagement Metrics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add drop column\n",
    "drop_gb_cols.append('books_count')\n",
    "drop_gb_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b50539",
   "metadata": {},
   "source": [
    "#### **image_url / small_image_url**\n",
    "\n",
    "As explained in the BBE `coverImg` feature, `image_url` and `small_image_url` variables were excluded from the current analysis because the project’s focus is on tabular predictive modelling, not computer vision. Since cover images do not contribute directly to the numerical or categorical predictors in the current modelling pipeline, they will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_gb_cols.extend(['image_url', 'small_image_url'])\n",
    "drop_gb_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd266ee0",
   "metadata": {},
   "source": [
    "### Final Feature Selection and Dataset Export\n",
    "\n",
    "After completing all data cleaning, standardization, and validation steps, we finalize the feature set for modelling.  \n",
    "Non-predictive, redundant, or inconsistent fields (such as IDs, text metadata, or variables unavailable in the BBE dataset) are dropped to ensure model generalization and consistency across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc079e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_existing_drops = [c for c in drop_gb_cols if c in books_clean.columns]\n",
    "print(f\"Will drop {len(gb_existing_drops)} columns: {gb_existing_drops}\")\n",
    "\n",
    "books_clean = books_clean.drop(columns=drop_gb_cols, errors='ignore')\n",
    "print(f\"Remaining columns: {len(books_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "clean_gb_path = Path(\"data/interim/goodbooks\")\n",
    "clean_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 7\n",
    "\n",
    "clean_gb_path = Path(\"data/interim/goodbooks\")\n",
    "\n",
    "books_clean.to_csv(clean_gb_path / f\"books_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned Goodbooks dataset saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19bd84",
   "metadata": {},
   "source": [
    "## Ratings\n",
    "\n",
    "### Filtering Non-English Ratings\n",
    "\n",
    "In this step, we align the `ratings.csv` dataset with the previously cleaned `books_clean` dataset, which only includes English-language titles.  \n",
    "Since the predictive analysis focuses on the English-speaking subscription market, any ratings linked to non-English books must be excluded to maintain data consistency and model relevance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load dataset\n",
    "ratings_raw = pd.read_csv('data/raw/ratings.csv')\n",
    "\n",
    "# create copy for cleaning\n",
    "ratings_clean = ratings_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f69e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "interim_gb_path = Path(\"data/interim/goodbooks\")\n",
    "interim_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 0\n",
    "\n",
    "ratings_clean.to_csv(interim_gb_path / f\"ratings_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Interim dataset saved successfully in data/interim/ directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97294623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data\n",
    "display(ratings_clean.head(3))\n",
    "\n",
    "# Check shape and missing values\n",
    "for name, df in {'Ratings': ratings_clean,}.items():\n",
    "    print(f\"\\n{name} — Shape: {df.shape}\")\n",
    "    print(df.info())\n",
    "    print(df.isna().sum().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned books dataset\n",
    "books_clean = pd.read_csv(\"data/interim/goodbooks/books_clean_v7.csv\")\n",
    "\n",
    "# merge ratings with book language info\n",
    "ratings_with_lang = ratings_clean.merge(\n",
    "    books_clean[['book_id', 'language_clean']],\n",
    "    on='book_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# identify how many ratings belong to non-English books\n",
    "non_english_ratings = ratings_with_lang[ratings_with_lang['language_clean'] != 'en']\n",
    "english_ratings = ratings_with_lang[ratings_with_lang['language_clean'] == 'en']\n",
    "\n",
    "print(f\"Non-English ratings: {len(non_english_ratings):,}\")\n",
    "print(f\"English ratings: {len(english_ratings):,}\")\n",
    "print(f\"Share of non-English ratings: {len(non_english_ratings) / len(ratings_with_lang):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f50fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_clean = english_ratings.drop(columns=['language_clean'])\n",
    "print(f\"Filtered ratings shape: {ratings_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d40b93",
   "metadata": {},
   "source": [
    "After merging the ratings data with book language information, **397,333 ratings (6.65%)** were identified as belonging to non-English titles. These entries were removed, leaving **5,579,146 valid English-language ratings** for further analysis and modeling.\n",
    "\n",
    "This ensures that subsequent recommendation and engagement models are trained exclusively on data representing the intended audience and market scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040862c",
   "metadata": {},
   "source": [
    "#### **user_id**\n",
    "\n",
    "In the next step, we will ensure IDs are valid, non-null, and that unique counts reasonable. Then we will explore user engagement patterns via a descriptive analysis of user rating activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16dacc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect user_id\n",
    "ratings_clean['user_id'].describe()\n",
    "print(\"Unique users:\", ratings_clean['user_id'].nunique())\n",
    "\n",
    "# Check for missing or invalid values\n",
    "print(\"Missing user_id:\", ratings_clean['user_id'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8717c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many ratings per user?\n",
    "user_activity = ratings_clean['user_id'].value_counts()\n",
    "print(user_activity.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b477b3ac",
   "metadata": {},
   "source": [
    "The dataset includes **53,424 unique users**, each providing between **1 and 195 ratings**, with an **average of 104** ratings per user.  \n",
    "The median and mean values are nearly identical (≈104), indicating a balanced and symmetric distribution of user activity.\n",
    "\n",
    "This level of engagement suggests that the dataset is sufficiently dense for collaborative filtering approaches.  \n",
    "Only a few users provided minimal input (single ratings), while no users exhibited extreme behavior.  \n",
    "As such, all users were retained for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(user_activity, bins=30, edgecolor='black')\n",
    "plt.title('Distribution of Ratings per User')\n",
    "plt.xlabel('Number of Ratings per User')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.axvline(user_activity.mean(), color='red', linestyle='--', label=f'Mean: {user_activity.mean():.1f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa3c56",
   "metadata": {},
   "source": [
    "To validate the numerical summary, we visualize how many books each user rated.  \n",
    "The histogram above confirms that user engagement is evenly distributed, with most users rating between **80 and 120 books**.  \n",
    "There are no extreme outliers, reinforcing the earlier conclusion that the dataset is well-balanced and suitable for collaborative filtering models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189be0f7",
   "metadata": {},
   "source": [
    "#### **book_id**\n",
    "\n",
    "The `book_id` column was cross-checked against the cleaned `books_clean` dataset to confirm relational integrity.  \n",
    "All ratings correspond to valid English-language books, ensuring consistency for downstream joins and recommendation modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreign key consistency\n",
    "valid_books = set(books_clean['book_id'])\n",
    "invalid_books = ratings_clean.loc[~ratings_clean['book_id'].isin(valid_books)]\n",
    "print(f\"Invalid book IDs found: {len(invalid_books)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a483194",
   "metadata": {},
   "source": [
    "#### **rating**\n",
    "\n",
    "The `rating` variable was verified to contain only integer values within the expected 1–5 range.  \n",
    "No invalid entries were detected. The distribution shows a strong positive bias, with ratings of 4 and 5 dominating the dataset, a known pattern in Goodreads data reflecting general positivity bias in user ratings. Also consistent to the trend we seen in the both datasets average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_clean['rating'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bfebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invalid or extreme values\n",
    "invalid_ratings = ratings_clean[~ratings_clean['rating'].between(1, 5)]\n",
    "print(f\"Invalid ratings: {len(invalid_ratings)}\")\n",
    "\n",
    "# plot rating distribution\n",
    "ratings_clean['rating'].value_counts().sort_index().plot(kind='bar', title='Distribution of Ratings (1-5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030af960",
   "metadata": {},
   "source": [
    "#### Exporting Clean Ratings Dataset\n",
    "\n",
    "After completing validation of all three columns (`user_id`, `book_id`, `rating`), the cleaned and language-aligned dataset was exported to the `data/interim/goodbooks/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb01481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "clean_gb_path = Path(\"data/interim/goodbooks\")\n",
    "clean_gb_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 1\n",
    "\n",
    "clean_gb_path = Path(\"data/interim/goodbooks\")\n",
    "\n",
    "ratings_clean.to_csv(clean_gb_path / f\"ratings_clean_v{version}.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned Goodbooks ratings dataset saved successfully in data/interim/ directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
