{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d6a58f",
   "metadata": {},
   "source": [
    "# Data Enrichment & Dataset Integration\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The purpose of this notebook is to **enrich, align, and integrate the cleaned datasets** to create a unified analytical foundation for modelling book satisfaction and evaluating catalogue diversity.\n",
    "\n",
    "This notebook expands upon prior cleaning work by **adding missing metadata, linking overlapping records across datasets, filtering the dataset to English-language titles, and preparing a model-ready dataset** that combines catalog-level information (BBE) with user-behavioral data (Goodbooks).\n",
    "\n",
    "Ultimately, this notebook enables insights that neither dataset could provide independently, most critically, **genre diversity analysis**, **language-based consistency**, **metadata-enhanced prediction modeling**.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "| Dataset                             | Source                     | Description                                                                                         | Format |\n",
    "| ----------------------------------- | -------------------------- | --------------------------------------------------------------------------------------------------- | ------ |\n",
    "| `bbe_clean_v13.csv`                  | Output from Notebook 02    | Cleaned *Best Books Ever* metadata including title, authors, genres, rating, description, and more. | CSV    |\n",
    "| `books_clean_v7.csv`      | Output from Notebook 02    | Cleaned Goodbooks-10k metadata lacking genre data but containing structural identifiers.            | CSV    |\n",
    "| `ratings_clean_v1.csv`    | Output from Notebook 02    | User–book interaction and aggregated rating data for behavioral modeling.                           | CSV    |\n",
    "| *(Optional)* External API responses | OpenLibrary / Google Books | Supplemental metadata (genres, languages, subjects) for non-overlapping titles.                     | JSON   |\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks in This Notebook\n",
    "\n",
    "This notebook will execute the following enrichment and integration steps:\n",
    "\n",
    "1. **Standardize linking identifiers**\n",
    "   Normalize `isbn_clean`, `goodreads_id`, `title_clean`, and `author_clean` across datasets to ensure reliable cross-dataset merging.\n",
    "\n",
    "2. **Identify overlap between BBE and Goodbooks**\n",
    "   Detect books present in both datasets using multi-key matching and evaluate match quality.\n",
    "\n",
    "3. **Enrich Goodbooks metadata with missing genres**\n",
    "\n",
    "   * Use BBE genre fields for overlapping titles.\n",
    "   * Query external APIs for non-overlapping titles.\n",
    "   * Normalize all genre outputs into a unified taxonomy.\n",
    "\n",
    "4. **Complete and standardize language metadata**\n",
    "   Fill missing values using BBE, APIs, or text-based heuristics, then harmonize language labels and codes.\n",
    "\n",
    "5. **Filter the enriched datasets to English-language books**\n",
    "   Restrict the unified dataset to titles identified as **English-language**, ensuring consistency for:\n",
    "\n",
    "   * genre diversity comparisons\n",
    "   * ratings behavior\n",
    "   * regression modeling\n",
    "\n",
    "   *(Non-English titles will be kept only in the enriched BBE/Goodbooks outputs, but excluded from the model dataset.)*\n",
    "\n",
    "6. **Integrate datasets into a model-ready schemas**\n",
    "   Combine BBE metadata with Goodbooks behavioral features for all overlapping **English-language** books.\n",
    "\n",
    "7. **Validate enrichment and filtering results**\n",
    "\n",
    "   * Assess metadata fields fill rates\n",
    "   * Review API match and success metrics\n",
    "   * Log all imputation and filtering decisions for reproducibility\n",
    "\n",
    "8. **Export enriched and unified datasets**\n",
    "   Produce final English-filtered datasets ready for modeling and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **en_supply_catalog.csv** — enriched metadata for all BBE books, representing the supply catalog\n",
    "* **en_internal_catalog.csv** — enriched metadata for all Goodbooks books, representing the internal catalog\n",
    "* **model_dataset_warm_start.csv** — unified metadata + behavioral dataset filtered to English-language books. Includes external BBE signals for cross platform validation.\n",
    "* **model_dataset_cold_start.csv** — unified metadata + behavioral dataset filtered to English-language books. Excludes external BBE signals for pure internal modeling.\n",
    "* **Enrichment and filtering logs** — documenting imputation sources, API usage, and filtering decisions\n",
    "\n",
    "> **Note:** This notebook focuses on **metadata enrichment, English-language filtering, and dataset integration**. Model development and feature engineering will be performed in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09815b66",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "## Navigate to the Parent Directory\n",
    "\n",
    "Before combining and saving datasets, it’s often helpful to move to a parent directory so that file operations (like loading or saving data) are easier and more organized. \n",
    "\n",
    "Before using the Python’s built-in os module to move one level up from the current working directory, it is advisable to inspect the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19d6dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\reisl\\OneDrive\\Documents\\GitHub\\bookwise-analytics\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f'Current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2117d",
   "metadata": {},
   "source": [
    "To change to parent directory (root folder), run the code below. If you are already in the root folder, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fc58fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to parent.\n",
      "New current directory: c:\\Users\\reisl\\OneDrive\\Documents\\GitHub\\bookwise-analytics\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory to its parent\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print('Changed directory to parent.')\n",
    "\n",
    "# Get the new current working directory (the parent directory)\n",
    "current_dir = os.getcwd()\n",
    "print(f'New current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee7f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.cleaning.utils.categories import (\n",
    "    map_subjects_to_genres\n",
    ")\n",
    "from src.cleaning.utils.pipeline import apply_cleaners_selectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4cc03",
   "metadata": {},
   "source": [
    "## Load and Inspect Datasets\n",
    "\n",
    "In this step, we load the previously cleaned datasets: **Goodbooks-10k** (books, ratings) and **Best Books Ever**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770d789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBE dataset columns:\n",
      "['goodreads_id_clean', 'authors_list', 'author_clean', 'title_clean', 'isbn_clean', 'language_clean', 'publication_date_clean', 'publisher_clean', 'is_major_publisher', 'bookFormat_clean', 'rating_clean', 'numRatings_clean', 'numRatings_log', 'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5', 'ratings_1_share', 'ratings_2_share', 'ratings_3_share', 'ratings_4_share', 'ratings_5_share', 'has_award', 'genres_clean', 'genres_simplified', 'description_clean', 'description_nlp', 'series_clean', 'pages_clean', 'bbeVotes_clean', 'bbeScore_clean', 'likedPercent_clean', 'has_likedPercent', 'price_clean', 'price_flag']\n",
      "BBE dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52424 entries, 0 to 52423\n",
      "Data columns (total 36 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   goodreads_id_clean      52424 non-null  string \n",
      " 1   authors_list            52424 non-null  object \n",
      " 2   author_clean            52376 non-null  object \n",
      " 3   title_clean             52424 non-null  object \n",
      " 4   isbn_clean              43338 non-null  string \n",
      " 5   language_clean          48413 non-null  object \n",
      " 6   publication_date_clean  51830 non-null  object \n",
      " 7   publisher_clean         48725 non-null  object \n",
      " 8   is_major_publisher      52424 non-null  bool   \n",
      " 9   bookFormat_clean        52424 non-null  object \n",
      " 10  rating_clean            52353 non-null  float64\n",
      " 11  numRatings_clean        52424 non-null  int64  \n",
      " 12  numRatings_log          52424 non-null  float64\n",
      " 13  ratings_1               52353 non-null  float64\n",
      " 14  ratings_2               52353 non-null  float64\n",
      " 15  ratings_3               52353 non-null  float64\n",
      " 16  ratings_4               52353 non-null  float64\n",
      " 17  ratings_5               52353 non-null  float64\n",
      " 18  ratings_1_share         52353 non-null  float64\n",
      " 19  ratings_2_share         52353 non-null  float64\n",
      " 20  ratings_3_share         52353 non-null  float64\n",
      " 21  ratings_4_share         52353 non-null  float64\n",
      " 22  ratings_5_share         52353 non-null  float64\n",
      " 23  has_award               52424 non-null  bool   \n",
      " 24  genres_clean            47804 non-null  object \n",
      " 25  genres_simplified       52424 non-null  object \n",
      " 26  description_clean       50268 non-null  object \n",
      " 27  description_nlp         50268 non-null  object \n",
      " 28  series_clean            23441 non-null  object \n",
      " 29  pages_clean             49747 non-null  float64\n",
      " 30  bbeVotes_clean          52424 non-null  int64  \n",
      " 31  bbeScore_clean          52424 non-null  int64  \n",
      " 32  likedPercent_clean      51803 non-null  float64\n",
      " 33  has_likedPercent        52424 non-null  int64  \n",
      " 34  price_clean             38068 non-null  float64\n",
      " 35  price_flag              52424 non-null  bool   \n",
      "dtypes: bool(3), float64(15), int64(4), object(12), string(2)\n",
      "memory usage: 13.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBE dataset sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goodreads_id_clean</th>\n",
       "      <th>authors_list</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>isbn_clean</th>\n",
       "      <th>language_clean</th>\n",
       "      <th>publication_date_clean</th>\n",
       "      <th>publisher_clean</th>\n",
       "      <th>is_major_publisher</th>\n",
       "      <th>bookFormat_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>description_nlp</th>\n",
       "      <th>series_clean</th>\n",
       "      <th>pages_clean</th>\n",
       "      <th>bbeVotes_clean</th>\n",
       "      <th>bbeScore_clean</th>\n",
       "      <th>likedPercent_clean</th>\n",
       "      <th>has_likedPercent</th>\n",
       "      <th>price_clean</th>\n",
       "      <th>price_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2767052</td>\n",
       "      <td>['suzanne collins']</td>\n",
       "      <td>suzanne collins</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>9780439023481</td>\n",
       "      <td>en</td>\n",
       "      <td>2008-09-14</td>\n",
       "      <td>scholastic</td>\n",
       "      <td>True</td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>winning means fame and fortunelosing means cer...</td>\n",
       "      <td>winning means fame and fortunelosing means cer...</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>374.0</td>\n",
       "      <td>30516</td>\n",
       "      <td>2993816</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['jk rowling', 'mary grandpre']</td>\n",
       "      <td>jk rowling, mary grandpre</td>\n",
       "      <td>harry potter and the order of the phoenix</td>\n",
       "      <td>9780439358071</td>\n",
       "      <td>en</td>\n",
       "      <td>2003-06-21</td>\n",
       "      <td>scholastic</td>\n",
       "      <td>True</td>\n",
       "      <td>paperback</td>\n",
       "      <td>...</td>\n",
       "      <td>there is a door at the end of a silent corrido...</td>\n",
       "      <td>there is a door at the end of a silent corrido...</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>870.0</td>\n",
       "      <td>26923</td>\n",
       "      <td>2632233</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.38</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2657</td>\n",
       "      <td>['harper lee']</td>\n",
       "      <td>harper lee</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>en</td>\n",
       "      <td>2007-07-11</td>\n",
       "      <td>harpercollins</td>\n",
       "      <td>True</td>\n",
       "      <td>paperback</td>\n",
       "      <td>...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>324.0</td>\n",
       "      <td>23328</td>\n",
       "      <td>2269402</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  goodreads_id_clean                     authors_list  \\\n",
       "0            2767052              ['suzanne collins']   \n",
       "1                  2  ['jk rowling', 'mary grandpre']   \n",
       "2               2657                   ['harper lee']   \n",
       "\n",
       "                author_clean                                title_clean  \\\n",
       "0            suzanne collins                           the hunger games   \n",
       "1  jk rowling, mary grandpre  harry potter and the order of the phoenix   \n",
       "2                 harper lee                      to kill a mockingbird   \n",
       "\n",
       "      isbn_clean language_clean publication_date_clean publisher_clean  \\\n",
       "0  9780439023481             en             2008-09-14      scholastic   \n",
       "1  9780439358071             en             2003-06-21      scholastic   \n",
       "2           <NA>             en             2007-07-11   harpercollins   \n",
       "\n",
       "   is_major_publisher bookFormat_clean  ...  \\\n",
       "0                True        hardcover  ...   \n",
       "1                True        paperback  ...   \n",
       "2                True        paperback  ...   \n",
       "\n",
       "                                   description_clean  \\\n",
       "0  winning means fame and fortunelosing means cer...   \n",
       "1  there is a door at the end of a silent corrido...   \n",
       "2  the unforgettable novel of a childhood in a sl...   \n",
       "\n",
       "                                     description_nlp           series_clean  \\\n",
       "0  winning means fame and fortunelosing means cer...       the hunger games   \n",
       "1  there is a door at the end of a silent corrido...           harry potter   \n",
       "2  the unforgettable novel of a childhood in a sl...  to kill a mockingbird   \n",
       "\n",
       "   pages_clean  bbeVotes_clean  bbeScore_clean  likedPercent_clean  \\\n",
       "0        374.0           30516         2993816                96.0   \n",
       "1        870.0           26923         2632233                98.0   \n",
       "2        324.0           23328         2269402                95.0   \n",
       "\n",
       "   has_likedPercent  price_clean  price_flag  \n",
       "0                 1         5.09       False  \n",
       "1                 1         7.38       False  \n",
       "2                 1          NaN        True  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books dataset columns:\n",
      "['book_id', 'work_text_reviews_count', 'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5', 'goodreads_id_clean', 'best_book_id_clean', 'work_id_clean', 'authors_list', 'author_clean', 'language_clean', 'publication_date_clean', 'isbn_clean', 'isbn13_clean', 'isbn_standard', 'rating_clean', 'numRatings_clean', 'numRatings_log', 'ratings_1_share', 'ratings_2_share', 'ratings_3_share', 'ratings_4_share', 'ratings_5_share', 'work_text_reviews_log', 'series_clean', 'title_clean']\n",
      "Books dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 28 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   book_id                  10000 non-null  int64  \n",
      " 1   work_text_reviews_count  10000 non-null  int64  \n",
      " 2   ratings_1                10000 non-null  int64  \n",
      " 3   ratings_2                10000 non-null  int64  \n",
      " 4   ratings_3                10000 non-null  int64  \n",
      " 5   ratings_4                10000 non-null  int64  \n",
      " 6   ratings_5                10000 non-null  int64  \n",
      " 7   goodreads_id_clean       10000 non-null  string \n",
      " 8   best_book_id_clean       10000 non-null  int64  \n",
      " 9   work_id_clean            10000 non-null  int64  \n",
      " 10  authors_list             10000 non-null  object \n",
      " 11  author_clean             10000 non-null  object \n",
      " 12  language_clean           8916 non-null   object \n",
      " 13  publication_date_clean   9887 non-null   object \n",
      " 14  isbn_clean               8251 non-null   string \n",
      " 15  isbn13_clean             9415 non-null   float64\n",
      " 16  isbn_standard            9426 non-null   object \n",
      " 17  rating_clean             10000 non-null  float64\n",
      " 18  numRatings_clean         10000 non-null  int64  \n",
      " 19  numRatings_log           10000 non-null  float64\n",
      " 20  ratings_1_share          10000 non-null  float64\n",
      " 21  ratings_2_share          10000 non-null  float64\n",
      " 22  ratings_3_share          10000 non-null  float64\n",
      " 23  ratings_4_share          10000 non-null  float64\n",
      " 24  ratings_5_share          10000 non-null  float64\n",
      " 25  work_text_reviews_log    10000 non-null  float64\n",
      " 26  series_clean             3947 non-null   object \n",
      " 27  title_clean              10000 non-null  object \n",
      "dtypes: float64(9), int64(10), object(7), string(2)\n",
      "memory usage: 2.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books dataset sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>work_text_reviews_count</th>\n",
       "      <th>ratings_1</th>\n",
       "      <th>ratings_2</th>\n",
       "      <th>ratings_3</th>\n",
       "      <th>ratings_4</th>\n",
       "      <th>ratings_5</th>\n",
       "      <th>goodreads_id_clean</th>\n",
       "      <th>best_book_id_clean</th>\n",
       "      <th>work_id_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>numRatings_clean</th>\n",
       "      <th>numRatings_log</th>\n",
       "      <th>ratings_1_share</th>\n",
       "      <th>ratings_2_share</th>\n",
       "      <th>ratings_3_share</th>\n",
       "      <th>ratings_4_share</th>\n",
       "      <th>ratings_5_share</th>\n",
       "      <th>work_text_reviews_log</th>\n",
       "      <th>series_clean</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>155254</td>\n",
       "      <td>66715</td>\n",
       "      <td>127936</td>\n",
       "      <td>560092</td>\n",
       "      <td>1481305</td>\n",
       "      <td>2706317</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2792775</td>\n",
       "      <td>...</td>\n",
       "      <td>4942365</td>\n",
       "      <td>15.413355</td>\n",
       "      <td>0.013499</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.113325</td>\n",
       "      <td>0.299716</td>\n",
       "      <td>0.547575</td>\n",
       "      <td>11.952824</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>the hunger games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>75867</td>\n",
       "      <td>75504</td>\n",
       "      <td>101676</td>\n",
       "      <td>455024</td>\n",
       "      <td>1156318</td>\n",
       "      <td>3011543</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4640799</td>\n",
       "      <td>...</td>\n",
       "      <td>4800065</td>\n",
       "      <td>15.384140</td>\n",
       "      <td>0.015730</td>\n",
       "      <td>0.021182</td>\n",
       "      <td>0.094795</td>\n",
       "      <td>0.240896</td>\n",
       "      <td>0.627396</td>\n",
       "      <td>11.236750</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>harry potter and the sorcerer's stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>95009</td>\n",
       "      <td>456191</td>\n",
       "      <td>436802</td>\n",
       "      <td>793319</td>\n",
       "      <td>875073</td>\n",
       "      <td>1355439</td>\n",
       "      <td>41865</td>\n",
       "      <td>41865</td>\n",
       "      <td>3212258</td>\n",
       "      <td>...</td>\n",
       "      <td>3916824</td>\n",
       "      <td>15.180792</td>\n",
       "      <td>0.116470</td>\n",
       "      <td>0.111519</td>\n",
       "      <td>0.202541</td>\n",
       "      <td>0.223414</td>\n",
       "      <td>0.346056</td>\n",
       "      <td>11.461737</td>\n",
       "      <td>twilight</td>\n",
       "      <td>twilight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id  work_text_reviews_count  ratings_1  ratings_2  ratings_3  \\\n",
       "0        1                   155254      66715     127936     560092   \n",
       "1        2                    75867      75504     101676     455024   \n",
       "2        3                    95009     456191     436802     793319   \n",
       "\n",
       "   ratings_4  ratings_5 goodreads_id_clean  best_book_id_clean  work_id_clean  \\\n",
       "0    1481305    2706317            2767052             2767052        2792775   \n",
       "1    1156318    3011543                  3                   3        4640799   \n",
       "2     875073    1355439              41865               41865        3212258   \n",
       "\n",
       "   ... numRatings_clean numRatings_log ratings_1_share ratings_2_share  \\\n",
       "0  ...          4942365      15.413355        0.013499        0.025886   \n",
       "1  ...          4800065      15.384140        0.015730        0.021182   \n",
       "2  ...          3916824      15.180792        0.116470        0.111519   \n",
       "\n",
       "  ratings_3_share  ratings_4_share ratings_5_share  work_text_reviews_log  \\\n",
       "0        0.113325         0.299716        0.547575              11.952824   \n",
       "1        0.094795         0.240896        0.627396              11.236750   \n",
       "2        0.202541         0.223414        0.346056              11.461737   \n",
       "\n",
       "       series_clean                            title_clean  \n",
       "0  the hunger games                       the hunger games  \n",
       "1      harry potter  harry potter and the sorcerer's stone  \n",
       "2          twilight                               twilight  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load datasets\n",
    "books_clean = pd.read_csv(\n",
    "    'data/interim/goodbooks/books_clean_v7.csv',\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    "    )\n",
    "ratings_clean = pd.read_csv('data/interim/goodbooks/ratings_clean_v0.csv')\n",
    "bbe_clean = pd.read_csv(\n",
    "    \"data/interim/bbe/bbe_clean_v13.csv\",\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    ")\n",
    "\n",
    "# create copies for imputation\n",
    "books_impute = books_clean.copy()\n",
    "bbe_impute = bbe_clean.copy()\n",
    "\n",
    "# log samples\n",
    "print(\"BBE dataset columns:\")\n",
    "print(bbe_impute.columns.tolist())\n",
    "print(\"BBE dataset info:\")\n",
    "display(bbe_impute.info())\n",
    "print(\"BBE dataset sample:\")\n",
    "display(bbe_impute.head(3))\n",
    "\n",
    "print(\"Books dataset columns:\")\n",
    "print(books_impute.columns.tolist())\n",
    "print(\"Books dataset info:\")\n",
    "display(books_impute.info())\n",
    "print(\"Books dataset sample:\")\n",
    "display(books_impute.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cff995",
   "metadata": {},
   "source": [
    "# Data Enrichment\n",
    "\n",
    "## Enriching Goodbooks\n",
    "\n",
    "### From BBE overlap\n",
    "\n",
    "To improve the completeness and quality of the Goodbooks-10k dataset, we selectively merge in metadata from the Best Books Ever (BBE) dataset using the shared `goodreads_id_clean` key. Goodbooks is kept as the primary source, while BBE is used to supply additional metadata fields, such as genres and page counts, as well as to fill in missing values for shared attributes like ISBN, publication date, and series.\n",
    "\n",
    "This approach ensures we enhance Goodbooks only where necessary: adding new information where it is absent and completing incomplete entries without overwriting existing data. The resulting `gb_enriched` dataset combines both sources into a more reliable and feature-rich foundation for downstream analytics and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6d0cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ENRICHING METADATA ---\n",
      "pages_clean: filled 8053 rows from BBE\n",
      "genres_clean: filled 8082 rows from BBE\n",
      "genres_simplified: filled 8082 rows from BBE\n",
      "publisher_clean: filled 7954 rows from BBE\n",
      "is_major_publisher: filled 8082 rows from BBE\n",
      "has_award: filled 8082 rows from BBE\n",
      "description_clean: filled 8009 rows from BBE\n",
      "description_nlp: filled 8009 rows from BBE\n",
      "\n",
      "--- ENRICHING SHARED COLUMNS (GB NaN -> fill from BBE) ---\n",
      "publication_date_clean: filled 102 missing values\n",
      "series_clean: filled 1133 missing values\n",
      "isbn_clean: filled 984 missing values\n",
      "language_clean: filled 684 missing values\n",
      "\n",
      "Enrichment complete!\n",
      "Final shape: (10000, 36)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>series_clean</th>\n",
       "      <th>genres_clean</th>\n",
       "      <th>genres_simplified</th>\n",
       "      <th>pages_clean</th>\n",
       "      <th>publication_date_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0439023483</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>['young adult', 'fiction', 'dystopia', 'fantas...</td>\n",
       "      <td>['young adult', 'fiction', 'dystopia', 'fantas...</td>\n",
       "      <td>374.0</td>\n",
       "      <td>2008-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0439554934</td>\n",
       "      <td>harry potter and the sorcerer's stone</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>['fantasy', 'fiction', 'young adult', 'magic',...</td>\n",
       "      <td>['fantasy', 'fiction', 'young adult', 'magic',...</td>\n",
       "      <td>309.0</td>\n",
       "      <td>1997-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0316015849</td>\n",
       "      <td>twilight</td>\n",
       "      <td>twilight</td>\n",
       "      <td>['young adult', 'fantasy', 'romance', 'vampire...</td>\n",
       "      <td>['young adult', 'fantasy', 'romance', 'vampire...</td>\n",
       "      <td>501.0</td>\n",
       "      <td>2005-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>['classics', 'fiction', 'historical fiction', ...</td>\n",
       "      <td>['classics', 'fiction', 'historical fiction', ...</td>\n",
       "      <td>324.0</td>\n",
       "      <td>1960-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0743273567</td>\n",
       "      <td>the great gatsby</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['classics', 'fiction', 'school', 'literature'...</td>\n",
       "      <td>['classics', 'fiction', 'school', 'literature'...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1925-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   isbn_clean                            title_clean           series_clean  \\\n",
       "0  0439023483                       the hunger games       the hunger games   \n",
       "1  0439554934  harry potter and the sorcerer's stone           harry potter   \n",
       "2  0316015849                               twilight               twilight   \n",
       "3        <NA>                  to kill a mockingbird  to kill a mockingbird   \n",
       "4  0743273567                       the great gatsby                    NaN   \n",
       "\n",
       "                                        genres_clean  \\\n",
       "0  ['young adult', 'fiction', 'dystopia', 'fantas...   \n",
       "1  ['fantasy', 'fiction', 'young adult', 'magic',...   \n",
       "2  ['young adult', 'fantasy', 'romance', 'vampire...   \n",
       "3  ['classics', 'fiction', 'historical fiction', ...   \n",
       "4  ['classics', 'fiction', 'school', 'literature'...   \n",
       "\n",
       "                                   genres_simplified  pages_clean  \\\n",
       "0  ['young adult', 'fiction', 'dystopia', 'fantas...        374.0   \n",
       "1  ['fantasy', 'fiction', 'young adult', 'magic',...        309.0   \n",
       "2  ['young adult', 'fantasy', 'romance', 'vampire...        501.0   \n",
       "3  ['classics', 'fiction', 'historical fiction', ...        324.0   \n",
       "4  ['classics', 'fiction', 'school', 'literature'...        200.0   \n",
       "\n",
       "  publication_date_clean  \n",
       "0             2008-01-01  \n",
       "1             1997-01-01  \n",
       "2             2005-01-01  \n",
       "3             1960-01-01  \n",
       "4             1925-01-01  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# ENRICH GOODBOOKS (books_impute) WITH BBE DATA\n",
    "# ---------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# columns to enrich ONLY when GB has NaN\n",
    "columns_to_enrich = [\n",
    "    \"publication_date_clean\",\n",
    "    \"series_clean\",\n",
    "    \"isbn_clean\",\n",
    "    \"language_clean\"\n",
    "    ]\n",
    "\n",
    "# columns existent only in BBE\n",
    "bbe_only_columns = [\n",
    "    \"pages_clean\",\n",
    "    \"genres_clean\",\n",
    "    \"genres_simplified\",\n",
    "    \"publisher_clean\",\n",
    "    \"is_major_publisher\",\n",
    "    \"has_award\",\n",
    "    \"description_clean\",\n",
    "    \"description_nlp\"\n",
    "]\n",
    "\n",
    "# merge Goodbooks with the needed BBE columns\n",
    "merge_cols = [\"goodreads_id_clean\"] + columns_to_enrich + bbe_only_columns\n",
    "\n",
    "gb_enriched = books_impute.merge(\n",
    "    bbe_impute[merge_cols].add_suffix(\"_bbe\"),\n",
    "    left_on=\"goodreads_id_clean\",\n",
    "    right_on=\"goodreads_id_clean_bbe\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ENRICH GENRE COLUMNS\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- ENRICHING METADATA ---\")\n",
    "for col in bbe_only_columns:\n",
    "    gb_enriched[col] = gb_enriched[col + \"_bbe\"]\n",
    "    filled = gb_enriched[col].notna().sum()\n",
    "    print(f\"{col}: filled {filled} rows from BBE\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ENRICH SHARED COLUMNS ONLY WHERE GB IS NaN\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- ENRICHING SHARED COLUMNS (GB NaN -> fill from BBE) ---\")\n",
    "for col in columns_to_enrich:\n",
    "    before = gb_enriched[col].isna().sum()\n",
    "    gb_enriched[col] = gb_enriched[col].fillna(gb_enriched[col + \"_bbe\"])\n",
    "    after = gb_enriched[col].isna().sum()\n",
    "    print(f\"{col}: filled {before - after} missing values\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# CLEANUP\n",
    "# ---------------------------------------------\n",
    "gb_enriched = gb_enriched.drop(columns=[c for c in gb_enriched.columns if c.endswith(\"_bbe\")])\n",
    "\n",
    "print(\"\\nEnrichment complete!\")\n",
    "print(\"Final shape:\", gb_enriched.shape)\n",
    "gb_enriched[['isbn_clean','title_clean', 'series_clean', 'genres_clean', 'genres_simplified', 'pages_clean', 'publication_date_clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d474378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_enriched v1 saved successfully in data/interim/merge directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/enriched/\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 1\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ead17c",
   "metadata": {},
   "source": [
    "### From external APIs\n",
    "\n",
    "To further enrich the Goodbooks-10k dataset, we leverage external APIs such as OpenLibrary and Google Books to fill in missing metadata for titles not covered by the BBE overlap. This process involves querying these APIs using available identifiers (like ISBN or title/author combinations) to retrieve additional information such as genres, page counts, and publication details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977a465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_isbn(isbn):\n",
    "    if not isinstance(isbn, str):\n",
    "        return None\n",
    "    isbn = re.sub(r'[^0-9Xx]', '', isbn)\n",
    "    if len(isbn) in [10, 13]:\n",
    "        return isbn\n",
    "    return None\n",
    "\n",
    "gb_enriched['isbn_query'] = gb_enriched['isbn_clean'].apply(clean_isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee431a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books needing external enrichment: 2219\n"
     ]
    }
   ],
   "source": [
    "missing_mask = (\n",
    "    gb_enriched['language_clean'].isna() |\n",
    "    gb_enriched['language_clean'].isin(['unknown', '', 'None']) |\n",
    "    gb_enriched['pages_clean'].isna() |\n",
    "    gb_enriched['publication_date_clean'].isna()  |\n",
    "    gb_enriched['publisher_clean'].isna() |\n",
    "    gb_enriched['description_clean'].isna()\n",
    ")\n",
    "\n",
    "to_impute = gb_enriched[missing_mask].copy()\n",
    "print(\"Books needing external enrichment:\", len(to_impute))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ab95c",
   "metadata": {},
   "source": [
    "#### Querying OpenLibrary API\n",
    "\n",
    "After enriching Goodbooks with BBE overlap data, we identify **2,219** books still missing critical metadata (ISBN, language, pages, publication date, publisher). We query **OpenLibrary first** because it has no rate limits or API key requirements, making it ideal for bulk enrichment. We create a boolean mask to identify books needing enrichment, then query OpenLibrary's ISBN endpoint for each book, collecting results in a structured format.\n",
    "\n",
    "The results are merged back into `gb_enriched` and saved as **version 2**. This incremental saving strategy ensures we don't lose progress if subsequent API calls fail or exceed quotas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4b963e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1720 cached OpenLibrary entries\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# cache path for OpenLibrary in data/raw\n",
    "OL_CACHE_PATH = Path(\"data/raw/openlibrary_api_cache.json\")\n",
    "\n",
    "# create directory if it doesn't exist\n",
    "OL_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load existing cache if it exists\n",
    "if OL_CACHE_PATH.exists():\n",
    "    with open(OL_CACHE_PATH, \"r\") as f:\n",
    "        ol_cache = json.load(f)\n",
    "    print(f\"Loaded {len(ol_cache)} cached OpenLibrary entries\")\n",
    "else:\n",
    "    ol_cache = {}\n",
    "    print(\"No existing cache found, starting fresh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad499482",
   "metadata": {},
   "source": [
    "You can skip the 2 cells below if you have already run the OpenLibrary queries and saved the results. Go to : _Run cached OpenLibrary queries to avoid re-querying the API, if you have previously saved the results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5278cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def query_openlibrary(isbn):\n",
    "    \"\"\"Return OL metadata in a consistent dict format.\"\"\"\n",
    "\n",
    "    isbn_str = str(isbn)\n",
    "    \n",
    "    if isbn_str in ol_cache:\n",
    "        return ol_cache[isbn_str]\n",
    "    \n",
    "    # Default structure to guarantee stable DataFrame columns\n",
    "    result = {\n",
    "        \"pages_openlib\": None,\n",
    "        \"publication_date_openlib\": None,\n",
    "        \"language_openlib\": None,\n",
    "        \"subjects_openlib\": None,\n",
    "        \"publisher_openlib\": None, \n",
    "        \"description_openlib\": None, \n",
    "    }\n",
    "\n",
    "    if isbn is None or pd.isna(isbn) or isbn == \"\":\n",
    "        return result\n",
    "    \n",
    "    url = f\"https://openlibrary.org/isbn/{isbn}.json\"\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return result\n",
    "\n",
    "        data = r.json()\n",
    "\n",
    "        # Pages\n",
    "        result[\"pages_openlib\"] = data.get(\"number_of_pages\")\n",
    "\n",
    "        # Publication date\n",
    "        result[\"publication_date_openlib\"] = data.get(\"publish_date\")\n",
    "\n",
    "        # Language\n",
    "        if \"languages\" in data and isinstance(data[\"languages\"], list):\n",
    "            key = data[\"languages\"][0].get(\"key\", \"\").split(\"/\")[-1]\n",
    "            result[\"language_openlib\"] = key\n",
    "\n",
    "        # Subjects\n",
    "        if \"subjects\" in data:\n",
    "            result[\"subjects_openlib\"] = [s.lower() for s in data[\"subjects\"]]\n",
    "        \n",
    "        # Publisher\n",
    "        if \"publishers\" in data and isinstance(data[\"publishers\"], list):\n",
    "            result[\"publisher_openlib\"] = data[\"publishers\"][0]\n",
    "        \n",
    "        # Description\n",
    "        desc = data.get(\"description\")\n",
    "        if isinstance(desc, dict):\n",
    "            result[\"description_openlib\"] = desc.get(\"value\")\n",
    "        elif isinstance(desc, str):\n",
    "            result[\"description_openlib\"] = desc\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        pass  # keep the default result structure\n",
    "\n",
    "    # Save to cache\n",
    "    ol_cache[isbn_str] = result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "results = []\n",
    "for isbn in tqdm(to_impute['isbn_query'], desc=\"Querying OpenLibrary\"):\n",
    "    results.append(query_openlibrary(isbn))\n",
    "    time.sleep(0.2)   # safe rate limit\n",
    "    \n",
    "# Save OpenLibrary cache after queries\n",
    "with open(OL_CACHE_PATH, \"w\") as f:\n",
    "    json.dump(ol_cache, f, indent=2)\n",
    "print(f\"OpenLibrary cache saved with {len(ol_cache)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8aa56",
   "metadata": {},
   "source": [
    "**Run cached OpenLibrary queries to avoid re-querying the API, if you have previously saved the results.** Skip if you just ran the API queries above in the same session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd5caad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell only if you want to use the cached results without querying again\n",
    "\n",
    "results = []\n",
    "for isbn in to_impute['isbn_query']:\n",
    "    isbn_str = str(isbn) if pd.notna(isbn) else \"\"\n",
    "    if isbn_str in ol_cache:\n",
    "        results.append(ol_cache[isbn_str])\n",
    "    else:\n",
    "        # If not in cache, return default structure\n",
    "        results.append({\n",
    "            \"pages_openlib\": None,\n",
    "            \"publication_date_openlib\": None,\n",
    "            \"language_openlib\": None,\n",
    "            \"subjects_openlib\": None,\n",
    "            \"publisher_openlib\": None,\n",
    "            \"description_openlib\": None,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a38555",
   "metadata": {},
   "source": [
    "Continued workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddfb24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API results summary:\n",
      "pages_openlib               1370\n",
      "publication_date_openlib    1708\n",
      "language_openlib            1445\n",
      "subjects_openlib            1027\n",
      "publisher_openlib           1660\n",
      "description_openlib          523\n",
      "dtype: int64\n",
      "\n",
      "After merge:\n",
      "pages_openlib               1370\n",
      "publication_date_openlib    1708\n",
      "language_openlib            1445\n",
      "subjects_openlib            1027\n",
      "publisher_openlib           1660\n",
      "description_openlib          523\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert results to dataframe\n",
    "ol_df = pd.DataFrame(results, index=to_impute.index)\n",
    "print(\"API results summary:\")\n",
    "print(ol_df.notna().sum())\n",
    "\n",
    "# merge back into gb_enriched\n",
    "for col in ol_df.columns:\n",
    "    if col not in gb_enriched.columns:\n",
    "        gb_enriched[col] = None\n",
    "    gb_enriched.loc[ol_df.index, col] = ol_df[col]\n",
    "\n",
    "# verify the merge\n",
    "print(\"\\nAfter merge:\")\n",
    "print(gb_enriched[ol_df.columns].notna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "156b39ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_enriched v2 saved successfully in data/interim/merge directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/enriched\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 2\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a12d1a",
   "metadata": {},
   "source": [
    "#### Cleaning and Processing OpenLibrary Data\n",
    "\n",
    "We apply the same cleaning steps used in Notebook 02, compiled into a pipeline, to standardize OpenLibrary API responses. The `apply_cleaners_selectively()` function ensures consistent data types, formats, and validation across all metadata fields. After cleaning, we fill missing values in `gb_enriched` using the cleaned OpenLibrary data.\n",
    "\n",
    "For genre enrichment, we map OpenLibrary subjects to our standardized genre taxonomy using `map_subjects_to_genres()`. This populates `genres_simplified` for books that had subjects but no genre data, significantly improving genre coverage. The enriched dataset is saved as **version 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60a19072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of cleaned OpenLibrary data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>pages_clean</th>\n",
       "      <th>pages_openlib</th>\n",
       "      <th>pages_openlib_clean</th>\n",
       "      <th>publication_date_clean</th>\n",
       "      <th>publication_date_openlib</th>\n",
       "      <th>publication_date_openlib_clean</th>\n",
       "      <th>language_clean</th>\n",
       "      <th>language_openlib</th>\n",
       "      <th>language_openlib_clean</th>\n",
       "      <th>genres_clean</th>\n",
       "      <th>genres_simplified</th>\n",
       "      <th>subjects_openlib</th>\n",
       "      <th>subjects_openlib_clean</th>\n",
       "      <th>publisher_clean</th>\n",
       "      <th>description_openlib</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>description_openlib</th>\n",
       "      <th>description_openlib_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>scion of ikshvaku</td>\n",
       "      <td>354.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['mythology', 'fiction', 'fantasy', 'indian li...</td>\n",
       "      <td>['mythology', 'fiction', 'fantasy', 'other', '...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>westland publication</td>\n",
       "      <td>None</td>\n",
       "      <td>ram rajya the perfect land but perfection has ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>canada</td>\n",
       "      <td>420.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['fiction', 'canada', 'literary fiction', 'con...</td>\n",
       "      <td>['fiction', 'other', 'literary fiction', 'cont...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>harpercollins</td>\n",
       "      <td>None</td>\n",
       "      <td>first i'll tell about the robbery our parents ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>the man in the brown suit</td>\n",
       "      <td>381.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1924-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['mystery', 'fiction', 'crime', 'classics', 'm...</td>\n",
       "      <td>['mystery', 'fiction', 'crime', 'classics', 'm...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>harpercollins</td>\n",
       "      <td>None</td>\n",
       "      <td>newly-orphaned anne beddingfeld is a nice engl...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>twilight and philosophy vampires vegetarians a...</td>\n",
       "      <td>259.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['philosophy', 'nonfiction', 'vampires', 'essa...</td>\n",
       "      <td>['philosophy', 'nonfiction', 'vampires', 'essa...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>wiley</td>\n",
       "      <td>None</td>\n",
       "      <td>the first look at the philosophy behind stephe...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>saga vol 5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>September 15, 2015</td>\n",
       "      <td>2015-09-15</td>\n",
       "      <td>en</td>\n",
       "      <td>eng</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[military deserters, parents of exceptional ch...</td>\n",
       "      <td>[military deserters, parents of exceptional ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6340</th>\n",
       "      <td>asterix the gaul</td>\n",
       "      <td>48.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1960-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['comics', 'graphic novels', 'bande dessine', ...</td>\n",
       "      <td>['comics', 'graphic novels', 'other', 'fiction...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>orion books ltd, london</td>\n",
       "      <td>None</td>\n",
       "      <td>the year is 50 bc and all gaul is occupied onl...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>tuck everlasting</td>\n",
       "      <td>148.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1975-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['fantasy', 'young adult', 'classics', 'fictio...</td>\n",
       "      <td>['fantasy', 'young adult', 'classics', 'fictio...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>macmillan</td>\n",
       "      <td>None</td>\n",
       "      <td>doomed to - or blessed with - eternal life aft...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5202</th>\n",
       "      <td>domes of fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>480.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1992-01-01</td>\n",
       "      <td>May 29, 1993</td>\n",
       "      <td>1993-05-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eng</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[fiction - fantasy, fiction, fantasy, fantasy ...</td>\n",
       "      <td>[fiction - fantasy, fiction, fantasy, fantasy ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6363</th>\n",
       "      <td>when we were orphans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>320.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>March 3, 2005</td>\n",
       "      <td>2005-03-03</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[modern fiction, fiction]</td>\n",
       "      <td>[modern fiction, fiction]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>fall of giants</td>\n",
       "      <td>985.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['historical fiction', 'fiction', 'historical'...</td>\n",
       "      <td>['historical fiction', 'fiction', 'historical'...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>penguin random house</td>\n",
       "      <td>None</td>\n",
       "      <td>this is an epic of love hatred war and revolut...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>trunk music</td>\n",
       "      <td>448.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['mystery', 'fiction', 'crime', 'thriller', 'd...</td>\n",
       "      <td>['mystery', 'fiction', 'crime', 'thriller', 'd...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>st martins paperbacks</td>\n",
       "      <td>None</td>\n",
       "      <td>back on the job after an involuntary leave of ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7487</th>\n",
       "      <td>maid for love</td>\n",
       "      <td>236.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['romance', 'contemporary romance', 'contempor...</td>\n",
       "      <td>['romance', 'contemporary romance', 'contempor...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>htjb, inc</td>\n",
       "      <td>None</td>\n",
       "      <td>maddie chester is determined to leave her home...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>the bloody chamber and other stories</td>\n",
       "      <td>128.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['short stories', 'fantasy', 'fiction', 'horro...</td>\n",
       "      <td>['short stories', 'fantasy', 'fiction', 'horro...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>penguin random house</td>\n",
       "      <td>None</td>\n",
       "      <td>in the bloody chamber - which includes the sto...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>criminal</td>\n",
       "      <td>448.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['mystery', 'thriller', 'crime', 'fiction', 'm...</td>\n",
       "      <td>['mystery', 'thriller', 'crime', 'fiction', 'm...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>dell</td>\n",
       "      <td>None</td>\n",
       "      <td>will trent is a brilliant agent with the georg...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>the kings of clonmel</td>\n",
       "      <td>320.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['fantasy', 'young adult', 'adventure', 'ficti...</td>\n",
       "      <td>['fantasy', 'young adult', 'adventure', 'ficti...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>penguin random house</td>\n",
       "      <td>None</td>\n",
       "      <td>when a cult springs up in neighboring clonmel ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title_clean  pages_clean  \\\n",
       "6252                                  scion of ikshvaku        354.0   \n",
       "4684                                             canada        420.0   \n",
       "1731                          the man in the brown suit        381.0   \n",
       "4742  twilight and philosophy vampires vegetarians a...        259.0   \n",
       "4521                                         saga vol 5          NaN   \n",
       "6340                                   asterix the gaul         48.0   \n",
       "576                                    tuck everlasting        148.0   \n",
       "5202                                      domes of fire          NaN   \n",
       "6363                               when we were orphans          NaN   \n",
       "439                                      fall of giants        985.0   \n",
       "2750                                        trunk music        448.0   \n",
       "7487                                      maid for love        236.0   \n",
       "5272               the bloody chamber and other stories        128.0   \n",
       "5653                                           criminal        448.0   \n",
       "3999                               the kings of clonmel        320.0   \n",
       "\n",
       "     pages_openlib  pages_openlib_clean publication_date_clean  \\\n",
       "6252          None                  NaN             2015-01-01   \n",
       "4684          None                  NaN             2012-01-01   \n",
       "1731          None                  NaN             1924-01-01   \n",
       "4742          None                  NaN             2009-01-01   \n",
       "4521         152.0                152.0             2015-01-01   \n",
       "6340          None                  NaN             1960-01-01   \n",
       "576           None                  NaN             1975-01-01   \n",
       "5202         480.0                480.0             1992-01-01   \n",
       "6363         320.0                320.0             2000-01-01   \n",
       "439           None                  NaN             2010-01-01   \n",
       "2750          None                  NaN             1997-01-01   \n",
       "7487          None                  NaN             2011-01-01   \n",
       "5272          None                  NaN             1979-01-01   \n",
       "5653          None                  NaN             2012-01-01   \n",
       "3999          None                  NaN             2008-01-01   \n",
       "\n",
       "     publication_date_openlib publication_date_openlib_clean language_clean  \\\n",
       "6252                     None                            NaN             en   \n",
       "4684                     None                            NaN             en   \n",
       "1731                     None                            NaN             en   \n",
       "4742                     None                            NaN             en   \n",
       "4521       September 15, 2015                     2015-09-15             en   \n",
       "6340                     None                            NaN             en   \n",
       "576                      None                            NaN             en   \n",
       "5202             May 29, 1993                     1993-05-29            NaN   \n",
       "6363            March 3, 2005                     2005-03-03             en   \n",
       "439                      None                            NaN             en   \n",
       "2750                     None                            NaN             en   \n",
       "7487                     None                            NaN             en   \n",
       "5272                     None                            NaN             en   \n",
       "5653                     None                            NaN             en   \n",
       "3999                     None                            NaN             en   \n",
       "\n",
       "     language_openlib language_openlib_clean  \\\n",
       "6252             None                    NaN   \n",
       "4684             None                    NaN   \n",
       "1731             None                    NaN   \n",
       "4742             None                    NaN   \n",
       "4521              eng                     en   \n",
       "6340             None                    NaN   \n",
       "576              None                    NaN   \n",
       "5202              eng                     en   \n",
       "6363             None                    NaN   \n",
       "439              None                    NaN   \n",
       "2750             None                    NaN   \n",
       "7487             None                    NaN   \n",
       "5272             None                    NaN   \n",
       "5653             None                    NaN   \n",
       "3999             None                    NaN   \n",
       "\n",
       "                                           genres_clean  \\\n",
       "6252  ['mythology', 'fiction', 'fantasy', 'indian li...   \n",
       "4684  ['fiction', 'canada', 'literary fiction', 'con...   \n",
       "1731  ['mystery', 'fiction', 'crime', 'classics', 'm...   \n",
       "4742  ['philosophy', 'nonfiction', 'vampires', 'essa...   \n",
       "4521                                                NaN   \n",
       "6340  ['comics', 'graphic novels', 'bande dessine', ...   \n",
       "576   ['fantasy', 'young adult', 'classics', 'fictio...   \n",
       "5202                                                NaN   \n",
       "6363                                                NaN   \n",
       "439   ['historical fiction', 'fiction', 'historical'...   \n",
       "2750  ['mystery', 'fiction', 'crime', 'thriller', 'd...   \n",
       "7487  ['romance', 'contemporary romance', 'contempor...   \n",
       "5272  ['short stories', 'fantasy', 'fiction', 'horro...   \n",
       "5653  ['mystery', 'thriller', 'crime', 'fiction', 'm...   \n",
       "3999  ['fantasy', 'young adult', 'adventure', 'ficti...   \n",
       "\n",
       "                                      genres_simplified  \\\n",
       "6252  ['mythology', 'fiction', 'fantasy', 'other', '...   \n",
       "4684  ['fiction', 'other', 'literary fiction', 'cont...   \n",
       "1731  ['mystery', 'fiction', 'crime', 'classics', 'm...   \n",
       "4742  ['philosophy', 'nonfiction', 'vampires', 'essa...   \n",
       "4521                                                NaN   \n",
       "6340  ['comics', 'graphic novels', 'other', 'fiction...   \n",
       "576   ['fantasy', 'young adult', 'classics', 'fictio...   \n",
       "5202                                                NaN   \n",
       "6363                                                NaN   \n",
       "439   ['historical fiction', 'fiction', 'historical'...   \n",
       "2750  ['mystery', 'fiction', 'crime', 'thriller', 'd...   \n",
       "7487  ['romance', 'contemporary romance', 'contempor...   \n",
       "5272  ['short stories', 'fantasy', 'fiction', 'horro...   \n",
       "5653  ['mystery', 'thriller', 'crime', 'fiction', 'm...   \n",
       "3999  ['fantasy', 'young adult', 'adventure', 'ficti...   \n",
       "\n",
       "                                       subjects_openlib  \\\n",
       "6252                                               None   \n",
       "4684                                               None   \n",
       "1731                                               None   \n",
       "4742                                               None   \n",
       "4521  [military deserters, parents of exceptional ch...   \n",
       "6340                                               None   \n",
       "576                                                None   \n",
       "5202  [fiction - fantasy, fiction, fantasy, fantasy ...   \n",
       "6363                          [modern fiction, fiction]   \n",
       "439                                                None   \n",
       "2750                                               None   \n",
       "7487                                               None   \n",
       "5272                                               None   \n",
       "5653                                               None   \n",
       "3999                                               None   \n",
       "\n",
       "                                 subjects_openlib_clean  \\\n",
       "6252                                               None   \n",
       "4684                                               None   \n",
       "1731                                               None   \n",
       "4742                                               None   \n",
       "4521  [military deserters, parents of exceptional ch...   \n",
       "6340                                               None   \n",
       "576                                                None   \n",
       "5202  [fiction - fantasy, fiction, fantasy, fantasy ...   \n",
       "6363                          [modern fiction, fiction]   \n",
       "439                                                None   \n",
       "2750                                               None   \n",
       "7487                                               None   \n",
       "5272                                               None   \n",
       "5653                                               None   \n",
       "3999                                               None   \n",
       "\n",
       "              publisher_clean description_openlib  \\\n",
       "6252     westland publication                None   \n",
       "4684            harpercollins                None   \n",
       "1731            harpercollins                None   \n",
       "4742                    wiley                None   \n",
       "4521                      NaN                None   \n",
       "6340  orion books ltd, london                None   \n",
       "576                 macmillan                None   \n",
       "5202                      NaN                None   \n",
       "6363                      NaN                None   \n",
       "439      penguin random house                None   \n",
       "2750    st martins paperbacks                None   \n",
       "7487                htjb, inc                None   \n",
       "5272     penguin random house                None   \n",
       "5653                     dell                None   \n",
       "3999     penguin random house                None   \n",
       "\n",
       "                                      description_clean description_openlib  \\\n",
       "6252  ram rajya the perfect land but perfection has ...                None   \n",
       "4684  first i'll tell about the robbery our parents ...                None   \n",
       "1731  newly-orphaned anne beddingfeld is a nice engl...                None   \n",
       "4742  the first look at the philosophy behind stephe...                None   \n",
       "4521                                                NaN                None   \n",
       "6340  the year is 50 bc and all gaul is occupied onl...                None   \n",
       "576   doomed to - or blessed with - eternal life aft...                None   \n",
       "5202                                                NaN                None   \n",
       "6363                                                NaN                None   \n",
       "439   this is an epic of love hatred war and revolut...                None   \n",
       "2750  back on the job after an involuntary leave of ...                None   \n",
       "7487  maddie chester is determined to leave her home...                None   \n",
       "5272  in the bloody chamber - which includes the sto...                None   \n",
       "5653  will trent is a brilliant agent with the georg...                None   \n",
       "3999  when a cult springs up in neighboring clonmel ...                None   \n",
       "\n",
       "     description_openlib_clean  \n",
       "6252                      None  \n",
       "4684                      None  \n",
       "1731                      None  \n",
       "4742                      None  \n",
       "4521                      None  \n",
       "6340                      None  \n",
       "576                       None  \n",
       "5202                      None  \n",
       "6363                      None  \n",
       "439                       None  \n",
       "2750                      None  \n",
       "7487                      None  \n",
       "5272                      None  \n",
       "5653                      None  \n",
       "3999                      None  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean OpenLibrary API data\n",
    "gb_enriched = apply_cleaners_selectively(\n",
    "    gb_enriched,\n",
    "    fields_to_clean=[\n",
    "        'pages',\n",
    "        'publication_date',\n",
    "        'language',\n",
    "        'subjects',\n",
    "        'publisher',\n",
    "        'description'\n",
    "        ],\n",
    "    source_suffix='_openlib',\n",
    "    target_suffix='_openlib_clean',\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# verify cleaning\n",
    "print(\"\\nSample of cleaned OpenLibrary data:\")\n",
    "gb_enriched[[\n",
    "    'title_clean',\n",
    "    'pages_clean',\n",
    "    'pages_openlib',\n",
    "    'pages_openlib_clean',\n",
    "    'publication_date_clean',\n",
    "    'publication_date_openlib',\n",
    "    'publication_date_openlib_clean',\n",
    "    'language_clean',\n",
    "    'language_openlib',\n",
    "    'language_openlib_clean',\n",
    "    'genres_clean',\n",
    "    'genres_simplified',\n",
    "    'subjects_openlib',\n",
    "    'subjects_openlib_clean',\n",
    "    'publisher_clean',\n",
    "    'description_openlib',\n",
    "    'description_clean',\n",
    "    'description_openlib',\n",
    "    'description_openlib_clean'\n",
    "    ]].sample(15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71ee7be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Filling missing values with cleaned OpenLibrary data ---\n",
      "pages_clean: filled 1257 values\n",
      "publication_date_clean: filled 9 values\n",
      "language_clean: filled 305 values\n",
      "\n",
      "--- Filling missing publisher_clean using OpenLibrary data ---\n",
      "publisher_clean: filled 1515 values\n",
      "\n",
      "--- Filling missing description_clean using OpenLibrary data ---\n",
      "description_clean: filled 509 values\n",
      "\n",
      "--- Generating genres_simplified from OpenLibrary subjects ---\n",
      "Books with subjects but no genres_simplified: 951\n",
      "genres_simplified: mapped 796 values from OpenLibrary subjects\n",
      "\n",
      "Sample of newly mapped genres:\n",
      "            title_clean                             subjects_openlib_clean  \\\n",
      "29            gone girl  [fiction suspense, fiction mystery detective g...   \n",
      "32  memoirs of a geisha  [geishas -- fiction, women -- japan -- fiction...   \n",
      "43         the notebook                          [modern fiction, fiction]   \n",
      "47       fahrenheit 451  [bradbury ray - prose criticism, spanishcontem...   \n",
      "70         frankenstein  [frankenstein-- fiction, scientists -- fiction...   \n",
      "\n",
      "                          genres_simplified  \n",
      "29                       [fiction, mystery]  \n",
      "32            [fiction, historical fiction]  \n",
      "43                                [fiction]  \n",
      "47  [fiction, science fiction, non-fiction]  \n",
      "70                                [fiction]  \n"
     ]
    }
   ],
   "source": [
    "# fill missing values with cleaned OpenLibrary data\n",
    "print(\"\\n--- Filling missing values with cleaned OpenLibrary data ---\")\n",
    "\n",
    "# fill pages_clean\n",
    "before_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "gb_enriched['pages_clean'] = gb_enriched['pages_clean'].fillna(gb_enriched['pages_openlib_clean'])\n",
    "after_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "print(f\"pages_clean: filled {before_pages - after_pages} values\")\n",
    "\n",
    "# fill publication_date_clean\n",
    "before_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "gb_enriched['publication_date_clean'] = gb_enriched['publication_date_clean'].fillna(gb_enriched['publication_date_openlib_clean'])\n",
    "after_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "print(f\"publication_date_clean: filled {before_date - after_date} values\")\n",
    "\n",
    "# fill language_clean\n",
    "# Create mask that catches both NaN and invalid string values\n",
    "before_lang = (gb_enriched['language_clean'].isna() | \n",
    "               gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "mask = (gb_enriched['language_clean'].isna() | \n",
    "        gb_enriched['language_clean'].isin(['unknown', '', 'None']))\n",
    "\n",
    "gb_enriched.loc[mask, 'language_clean'] = gb_enriched.loc[mask, 'language_openlib_clean']\n",
    "\n",
    "after_lang = (gb_enriched['language_clean'].isna() | \n",
    "              gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "print(f\"language_clean: filled {before_lang - after_lang} values\")\n",
    "\n",
    "# fill publisher_clean\n",
    "print(\"\\n--- Filling missing publisher_clean using OpenLibrary data ---\")\n",
    "before_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "gb_enriched['publisher_clean'] = gb_enriched['publisher_clean'].fillna(\n",
    "    gb_enriched['publisher_openlib_clean']\n",
    ")\n",
    "after_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "print(f\"publisher_clean: filled {before_publisher - after_publisher} values\")\n",
    "\n",
    "# fill description_clean\n",
    "print(\"\\n--- Filling missing description_clean using OpenLibrary data ---\")\n",
    "\n",
    "before_desc = gb_enriched['description_clean'].isna().sum()\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].fillna(\n",
    "    gb_enriched['description_openlib_clean']\n",
    ")\n",
    "after_desc = gb_enriched['description_clean'].isna().sum()\n",
    "print(f\"description_clean: filled {before_desc - after_desc} values\")\n",
    "\n",
    "# generate genres_simplified from subjects_openlib_clean\n",
    "print(\"\\n--- Generating genres_simplified from OpenLibrary subjects ---\")\n",
    "\n",
    "# Import genre mapping utilities\n",
    "\n",
    "# Fill genres_simplified for books that have subjects but no genres\n",
    "books_needing_genre_mapping = (\n",
    "    (gb_enriched['genres_simplified'].isna()) & \n",
    "    (gb_enriched['subjects_openlib_clean'].notna())\n",
    ")\n",
    "\n",
    "print(f\"Books with subjects but no genres_simplified: {books_needing_genre_mapping.sum()}\")\n",
    "\n",
    "if books_needing_genre_mapping.sum() > 0:\n",
    "    # Apply genre mapping to subjects\n",
    "    gb_enriched.loc[books_needing_genre_mapping, 'genres_simplified'] = (\n",
    "        gb_enriched.loc[books_needing_genre_mapping, 'subjects_openlib_clean']\n",
    "        .apply(lambda x: map_subjects_to_genres(x) if isinstance(x, list) else None)\n",
    "    )\n",
    "    \n",
    "    filled_genres = (\n",
    "        gb_enriched.loc[books_needing_genre_mapping, 'genres_simplified'].notna().sum()\n",
    "    )\n",
    "    print(f\"genres_simplified: mapped {filled_genres} values from OpenLibrary subjects\")\n",
    "    \n",
    "    # Show sample of newly mapped genres\n",
    "    newly_mapped = gb_enriched[books_needing_genre_mapping & gb_enriched['genres_simplified'].notna()]\n",
    "    if len(newly_mapped) > 0:\n",
    "        print(\"\\nSample of newly mapped genres:\")\n",
    "        print(newly_mapped[['title_clean', 'subjects_openlib_clean', 'genres_simplified']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40e72318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ENRICHMENT SUMMARY ---\n",
      "\n",
      "Total books enriched with OpenLibrary data: 1027\n",
      "\n",
      "Sample of books enriched from OpenLibrary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>pages_openlib_clean</th>\n",
       "      <th>publication_date_openlib_clean</th>\n",
       "      <th>language_openlib_clean</th>\n",
       "      <th>subjects_openlib_clean</th>\n",
       "      <th>genres_simplified</th>\n",
       "      <th>publisher_clean</th>\n",
       "      <th>description_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gone girl</td>\n",
       "      <td>gillian flynn</td>\n",
       "      <td>399.0</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>[fiction suspense, fiction mystery detective g...</td>\n",
       "      <td>[fiction, mystery]</td>\n",
       "      <td>weidenfeld nicolson</td>\n",
       "      <td>just how well can you ever know the person you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>memoirs of a geisha</td>\n",
       "      <td>arthur golden</td>\n",
       "      <td>758.0</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>[geishas -- fiction, women -- japan -- fiction...</td>\n",
       "      <td>[fiction, historical fiction]</td>\n",
       "      <td>penguin random house</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>the notebook</td>\n",
       "      <td>nicholas sparks</td>\n",
       "      <td>272.0</td>\n",
       "      <td>2004-07-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[modern fiction, fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>penguin random house</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>fahrenheit 451</td>\n",
       "      <td>ray bradbury</td>\n",
       "      <td>176.0</td>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>es</td>\n",
       "      <td>[bradbury ray - prose criticism, spanishcontem...</td>\n",
       "      <td>[fiction, science fiction, non-fiction]</td>\n",
       "      <td>plaza y janes</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>frankenstein</td>\n",
       "      <td>mary wollstonecraft shelley, percy bysshe shel...</td>\n",
       "      <td>273.0</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>[frankenstein-- fiction, scientists -- fiction...</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>penguin random house</td>\n",
       "      <td>presents the story of dr frankenstein and his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>jurassic park</td>\n",
       "      <td>michael crichton</td>\n",
       "      <td>467.0</td>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>es</td>\n",
       "      <td>[suspense, fiction, fiction - general, spanish...</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>debolsillo</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>thirteen reasons why</td>\n",
       "      <td>jay asher</td>\n",
       "      <td>288.0</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>[suicide -- fiction, high schools -- fiction, ...</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>razorbill</td>\n",
       "      <td>when high school student clay jenkins receives...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>american gods</td>\n",
       "      <td>neil gaiman</td>\n",
       "      <td>672.0</td>\n",
       "      <td>2002-03-04</td>\n",
       "      <td>en</td>\n",
       "      <td>[science fiction]</td>\n",
       "      <td>[fiction, science fiction]</td>\n",
       "      <td>headline book publishing</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>the shack</td>\n",
       "      <td>william paul young</td>\n",
       "      <td>252.0</td>\n",
       "      <td>2007-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>[life change events -- fiction, missing childr...</td>\n",
       "      <td>[fiction, children]</td>\n",
       "      <td>windblown media</td>\n",
       "      <td>mackenzie allen phillips' youngest daughter mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>the guernsey literary and potato peel pie society</td>\n",
       "      <td>mary ann shaffer, annie barrows</td>\n",
       "      <td>288.0</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>[literary, fiction literary, fiction, fiction ...</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>the dial press</td>\n",
       "      <td>i wonder how the book got to guernsey perhaps ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title_clean  \\\n",
       "29                                           gone girl   \n",
       "32                                 memoirs of a geisha   \n",
       "43                                        the notebook   \n",
       "47                                      fahrenheit 451   \n",
       "70                                        frankenstein   \n",
       "83                                       jurassic park   \n",
       "146                               thirteen reasons why   \n",
       "166                                      american gods   \n",
       "173                                          the shack   \n",
       "194  the guernsey literary and potato peel pie society   \n",
       "\n",
       "                                          author_clean  pages_openlib_clean  \\\n",
       "29                                       gillian flynn                399.0   \n",
       "32                                       arthur golden                758.0   \n",
       "43                                     nicholas sparks                272.0   \n",
       "47                                        ray bradbury                176.0   \n",
       "70   mary wollstonecraft shelley, percy bysshe shel...                273.0   \n",
       "83                                    michael crichton                467.0   \n",
       "146                                          jay asher                288.0   \n",
       "166                                        neil gaiman                672.0   \n",
       "173                                 william paul young                252.0   \n",
       "194                    mary ann shaffer, annie barrows                288.0   \n",
       "\n",
       "    publication_date_openlib_clean language_openlib_clean  \\\n",
       "29                      2012-01-01                     en   \n",
       "32                      2005-01-01                     en   \n",
       "43                      2004-07-05                    NaN   \n",
       "47                      2006-01-03                     es   \n",
       "70                      2003-01-01                     en   \n",
       "83                      2006-01-01                     es   \n",
       "146                     2008-01-01                     en   \n",
       "166                     2002-03-04                     en   \n",
       "173                     2007-01-01                     en   \n",
       "194                     2008-01-01                     en   \n",
       "\n",
       "                                subjects_openlib_clean  \\\n",
       "29   [fiction suspense, fiction mystery detective g...   \n",
       "32   [geishas -- fiction, women -- japan -- fiction...   \n",
       "43                           [modern fiction, fiction]   \n",
       "47   [bradbury ray - prose criticism, spanishcontem...   \n",
       "70   [frankenstein-- fiction, scientists -- fiction...   \n",
       "83   [suspense, fiction, fiction - general, spanish...   \n",
       "146  [suicide -- fiction, high schools -- fiction, ...   \n",
       "166                                  [science fiction]   \n",
       "173  [life change events -- fiction, missing childr...   \n",
       "194  [literary, fiction literary, fiction, fiction ...   \n",
       "\n",
       "                           genres_simplified           publisher_clean  \\\n",
       "29                        [fiction, mystery]       weidenfeld nicolson   \n",
       "32             [fiction, historical fiction]      penguin random house   \n",
       "43                                 [fiction]      penguin random house   \n",
       "47   [fiction, science fiction, non-fiction]             plaza y janes   \n",
       "70                                 [fiction]      penguin random house   \n",
       "83                                 [fiction]                debolsillo   \n",
       "146                                [fiction]                 razorbill   \n",
       "166               [fiction, science fiction]  headline book publishing   \n",
       "173                      [fiction, children]           windblown media   \n",
       "194                                [fiction]            the dial press   \n",
       "\n",
       "                                     description_clean  \n",
       "29   just how well can you ever know the person you...  \n",
       "32                                                None  \n",
       "43                                                None  \n",
       "47                                                None  \n",
       "70   presents the story of dr frankenstein and his ...  \n",
       "83                                                None  \n",
       "146  when high school student clay jenkins receives...  \n",
       "166                                               None  \n",
       "173  mackenzie allen phillips' youngest daughter mi...  \n",
       "194  i wonder how the book got to guernsey perhaps ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENRE COVERAGE AFTER ENRICHMENT ---\n",
      "Books with genres_clean: 8082\n",
      "Books without genres_clean: 1918\n",
      "Genre coverage: 80.8%\n",
      "\n",
      "Books with genres_simplified: 8878\n",
      "Books without genres_simplified: 1122\n",
      "Genre simplified coverage: 88.8%\n",
      "Books with description_clean: 8518\n",
      "Books without description_clean: 1482\n",
      "Books description coverage: 85.2%\n",
      "Books with publisher_clean: 9469\n",
      "Books without publisher_clean: 531\n",
      "Books publisher coverage: 94.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- ENRICHMENT SUMMARY ---\")\n",
    "\n",
    "# Show books that received OpenLibrary data\n",
    "books_with_ol_data = gb_enriched[\n",
    "    gb_enriched['subjects_openlib_clean'].notna()\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nTotal books enriched with OpenLibrary data: {len(books_with_ol_data)}\")\n",
    "\n",
    "if len(books_with_ol_data) > 0:\n",
    "    print(\"\\nSample of books enriched from OpenLibrary:\")\n",
    "    display(books_with_ol_data[[\n",
    "        'title_clean',\n",
    "        'author_clean',\n",
    "        'pages_openlib_clean',\n",
    "        'publication_date_openlib_clean',\n",
    "        'language_openlib_clean',\n",
    "        'subjects_openlib_clean',\n",
    "        'genres_simplified',\n",
    "        'publisher_clean',\n",
    "        'description_clean'\n",
    "    ]].head(10))\n",
    "\n",
    "# Show overall genre coverage\n",
    "print(f\"\\n--- GENRE COVERAGE AFTER ENRICHMENT ---\")\n",
    "print(f\"Books with genres_clean: {gb_enriched['genres_clean'].notna().sum()}\")\n",
    "print(f\"Books without genres_clean: {gb_enriched['genres_clean'].isna().sum()}\")\n",
    "print(f\"Genre coverage: {gb_enriched['genres_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\\n\")\n",
    "print(f\"Books with genres_simplified: {gb_enriched['genres_simplified'].notna().sum()}\")\n",
    "print(f\"Books without genres_simplified: {gb_enriched['genres_simplified'].isna().sum()}\")\n",
    "print(f\"Genre simplified coverage: {gb_enriched['genres_simplified'].notna().sum() / len(gb_enriched) * 100:.1f}%\")\n",
    "# Show overall description and publisher coverage\n",
    "print(f\"Books with description_clean: {gb_enriched['description_clean'].notna().sum()}\")\n",
    "print(f\"Books without description_clean: {gb_enriched['description_clean'].isna().sum()}\")\n",
    "print(f\"Books description coverage: {gb_enriched['description_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\")\n",
    "print(f\"Books with publisher_clean: {gb_enriched['publisher_clean'].notna().sum()}\")\n",
    "print(f\"Books without publisher_clean: {gb_enriched['publisher_clean'].isna().sum()}\")\n",
    "print(f\"Books publisher coverage: {gb_enriched['publisher_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e647000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_enriched v3 saved successfully in data/interim/merge directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/enriched\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 3\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab0e7c",
   "metadata": {},
   "source": [
    "#### Querying Google Books API (with Quota Management)\n",
    "\n",
    "After OpenLibrary enrichment, we create a new mask to identify remaining gaps: **1,730** books. Google Books API requires an API key and has daily quota limits (1,000 requests/day for free tier), so we implement several strategies: **(1)** process ISBNs in chunks of 1,000, **(2)** add sleep delays between requests, **(3)** cache all results to avoid re-querying, and **(4)** save progress incrementally.\n",
    "\n",
    "We load existing cache if available, query only uncached ISBNs, and update the cache after each session. This approach allows us to spread queries across multiple days if needed while preserving all previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3991bae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books still needing external enrichment: 1728\n",
      "\n",
      "Breakdown of remaining missing values:\n",
      "  - Missing pages: 690\n",
      "  - Missing publication_date: 2\n",
      "  - Missing/invalid language: 95\n",
      "  - Missing publisher: 531\n",
      "  - Missing description: 1482\n"
     ]
    }
   ],
   "source": [
    "# Check how many books still need enrichment\n",
    "new_missing_mask = (\n",
    "    gb_enriched['language_clean'].isna() |\n",
    "    gb_enriched['language_clean'].isin(['unknown', '', 'None']) |\n",
    "    gb_enriched['pages_clean'].isna() |\n",
    "    gb_enriched['publication_date_clean'].isna() | \n",
    "    gb_enriched['publisher_clean'].isna() |\n",
    "    gb_enriched['description_clean'].isna()\n",
    ")\n",
    "\n",
    "new_to_impute = gb_enriched[new_missing_mask].copy()\n",
    "print(\"Books still needing external enrichment:\", len(new_to_impute))\n",
    "\n",
    "# Show breakdown by field\n",
    "print(\"\\nBreakdown of remaining missing values:\")\n",
    "print(f\"  - Missing pages: {gb_enriched['pages_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing publication_date: {gb_enriched['publication_date_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing/invalid language: {(gb_enriched['language_clean'].isna() | gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()}\")\n",
    "print(f\"  - Missing publisher: {gb_enriched['publisher_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing description: {gb_enriched['description_clean'].isna().sum()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ed215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(data, size=1000):\n",
    "    for i in range(0, len(data), size):\n",
    "        yield data[i:i+size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04d16623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1728 cached Google Books entries\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define cache path for Google Books in data/raw\n",
    "CACHE_PATH = Path(\"data/raw/google_api_cache.json\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load existing cache if it exists\n",
    "if CACHE_PATH.exists():\n",
    "    with open(CACHE_PATH, \"r\") as f:\n",
    "        google_cache = json.load(f)\n",
    "        print(f\"Loaded {len(google_cache)} cached Google Books entries\")\n",
    "else:\n",
    "    google_cache = {}\n",
    "    print(\"No existing cache found, starting fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44817fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_BOOKS_API_KEY\")\n",
    "\n",
    "def query_google_books(isbn):\n",
    "    isbn = str(isbn)\n",
    "\n",
    "    # Check cache\n",
    "    if isbn in google_cache:\n",
    "        return google_cache[isbn]\n",
    "\n",
    "    # Query Google Books with API key\n",
    "    url = (\n",
    "        f\"https://www.googleapis.com/books/v1/volumes?\"\n",
    "        f\"q=isbn:{isbn}&key={GOOGLE_API_KEY}\"\n",
    "    )\n",
    "\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        result = {\"isbn\": isbn, \"error\": f\"HTTP {r.status_code}\"}\n",
    "    else:\n",
    "        data = r.json()\n",
    "        if \"items\" in data and data[\"items\"]:\n",
    "            volume = data[\"items\"][0][\"volumeInfo\"]\n",
    "            result = {\n",
    "                \"isbn\": isbn,\n",
    "                \"title\": volume.get(\"title\"),\n",
    "                \"authors\": volume.get(\"authors\"),\n",
    "                \"publisher\": volume.get(\"publisher\"),\n",
    "                \"publishedDate\": volume.get(\"publishedDate\"),\n",
    "                \"pageCount\": volume.get(\"pageCount\"),\n",
    "                \"categories\": volume.get(\"categories\"),\n",
    "                \"language\": volume.get(\"language\"),\n",
    "                \"description\": volume.get(\"description\"),\n",
    "            }\n",
    "        else:\n",
    "            result = {\"isbn\": isbn, \"error\": \"No results\"}\n",
    "\n",
    "    # Save to cache\n",
    "    google_cache[isbn] = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_isbns = (\n",
    "    new_to_impute['isbn_query']\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "print(f\"Unique ISBNs to query: {len(list_of_isbns)}\")\n",
    "chunks = list(chunk_list(list_of_isbns, size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Choose which chunk you want to process today\n",
    "chunk_to_process = chunks[1]   # run chunk 0 today, 1 tomorrow, etc.\n",
    "results = []\n",
    "for isbn in tqdm(chunk_to_process, desc=\"Querying Google Books\"):\n",
    "    results.append(query_google_books(isbn))\n",
    "    time.sleep(0.1)   # be nice to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f955d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cache after ISBN queries\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(google_cache, f, indent=2)\n",
    "print(f\"Cache updated with {len(google_cache)} entries after ISBN queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d7361",
   "metadata": {},
   "source": [
    "#### Handling Books Without ISBNs\n",
    "\n",
    "Some books lack valid ISBNs but can still be enriched using **title and author search**. Google Books API supports `intitle:` and `inauthor:` query parameters, allowing us to find books by bibliographic metadata instead of identifiers. We create cache keys in `\"title|author\"` format to distinguish these from ISBN-based queries.\n",
    "\n",
    "This fallback strategy significantly increases our enrichment coverage, especially for older books, special editions, or records with ISBN errors. Results are cached alongside ISBN queries to maintain a unified enrichment workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad912ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_google_books_by_title(title, author):\n",
    "    \"\"\"Query Google Books API using title and author when ISBN is unavailable.\"\"\"\n",
    "    \n",
    "    # Create cache key\n",
    "    cache_key = f\"{title}|{author}\"\n",
    "    \n",
    "    if cache_key in google_cache:\n",
    "        return google_cache[cache_key]\n",
    "    \n",
    "    # Build query string\n",
    "    query_parts = []\n",
    "    if pd.notna(title):\n",
    "        query_parts.append(f'intitle:\"{title}\"')\n",
    "    if pd.notna(author):\n",
    "        query_parts.append(f'inauthor:\"{author}\"')\n",
    "    \n",
    "    query_string = \"+\".join(query_parts)\n",
    "    \n",
    "    url = (\n",
    "        f\"https://www.googleapis.com/books/v1/volumes?\"\n",
    "        f\"q={query_string}&key={GOOGLE_API_KEY}\"\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        result = {\"title\": title, \"author\": author, \"error\": f\"HTTP {r.status_code}\"}\n",
    "    else:\n",
    "        data = r.json()\n",
    "        if \"items\" in data and data[\"items\"]:\n",
    "            volume = data[\"items\"][0][\"volumeInfo\"]\n",
    "            result = {\n",
    "                \"title\": title,\n",
    "                \"author\": author,\n",
    "                \"pageCount\": volume.get(\"pageCount\"),\n",
    "                \"publisher\": volume.get(\"publisher\"),  \n",
    "                \"publishedDate\": volume.get(\"publishedDate\"),\n",
    "                \"categories\": volume.get(\"categories\"),\n",
    "                \"language\": volume.get(\"language\"),\n",
    "                \"description\": volume.get(\"description\"),\n",
    "            }\n",
    "        else:\n",
    "            result = {\"title\": title, \"author\": author, \"error\": \"No results\"}\n",
    "    \n",
    "    google_cache[cache_key] = result\n",
    "    return result\n",
    "\n",
    "# Process books without ISBN separately\n",
    "books_without_isbn = new_to_impute[new_to_impute['isbn_query'].isna()].copy()\n",
    "print(f\"Books without ISBN to query by title/author: {len(books_without_isbn)}\")\n",
    "\n",
    "results_by_title = []\n",
    "for idx, row in tqdm(books_without_isbn.iterrows(), \n",
    "                     total=len(books_without_isbn),\n",
    "                     desc=\"Querying Google Books by title/author\"):\n",
    "    results_by_title.append(query_google_books_by_title(\n",
    "        row['title_clean'], \n",
    "        row['author_clean']\n",
    "    ))\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cache after title/author queries\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(google_cache, f, indent=2)\n",
    "print(f\"Cache updated with {len(google_cache)} entries after title/author queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25cd3f8",
   "metadata": {},
   "source": [
    "#### Loading and Applying Cached Results\n",
    "\n",
    "The Google Books cache contains results from multiple query sessions, potentially across different days. We load the complete cache and separate ISBN-based results from title/author-based results by checking for the `\"|\"` delimiter in cache keys. This allows us to apply different matching logic for each result type.\n",
    "\n",
    "We then merge cached data back into `gb_enriched`, apply the cleaning pipeline to standardize formats, and fill remaining metadata gaps. The `map_subjects_to_genres()` function maps Google Books categories to our genre taxonomy, further increasing genre coverage. This completes our multi-source enrichment strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25ff711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1728 cached entries\n",
      "Found 1276 ISBN-based results\n",
      "Found 452 title/author-based results\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the cache\n",
    "CACHE_PATH = Path(\"data/raw/google_api_cache.json\")\n",
    "\n",
    "with open(CACHE_PATH, \"r\") as f:\n",
    "    google_cache = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(google_cache)} cached entries\")\n",
    "\n",
    "# Separate ISBN-based results from title/author-based results\n",
    "isbn_results = []\n",
    "title_author_results = []\n",
    "\n",
    "for key, value in google_cache.items():\n",
    "    if \"|\" in key:  # Title|Author format\n",
    "        title_author_results.append(value)\n",
    "    else:  # ISBN format\n",
    "        isbn_results.append(value)\n",
    "\n",
    "print(f\"Found {len(isbn_results)} ISBN-based results\")\n",
    "print(f\"Found {len(title_author_results)} title/author-based results\")\n",
    "\n",
    "# add google books data to dataframe\n",
    "google_columns = [\n",
    "    'pageCount_google',\n",
    "    'publishedDate_google',\n",
    "    'categories_google',\n",
    "    'language_google',\n",
    "    'publisher_google',\n",
    "    'description_google'\n",
    "]\n",
    "for col in google_columns:\n",
    "    if col not in gb_enriched.columns:\n",
    "        gb_enriched[col] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "741eb6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ISBN results preview:\n",
      "         isbn                       title             authors  \\\n",
      "0  0452284244                 Animal Farm     [George Orwell]   \n",
      "1  0618346252  The Fellowship of the Ring  [J. R. R. Tolkien]   \n",
      "2  0739326228         Memoirs of a Geisha     [Arthur Golden]   \n",
      "3  0965818675                         NaN                 NaN   \n",
      "4  0553816713                         NaN                 NaN   \n",
      "\n",
      "                             publisher publishedDate  pageCount categories  \\\n",
      "0                              Penguin    2003-05-06      129.0  [Fiction]   \n",
      "1                        Mariner Books       2003-09      398.0  [Fiction]   \n",
      "2  Random House Large Print Publishing          2005      758.0  [Fiction]   \n",
      "3                                  NaN           NaN        NaN        NaN   \n",
      "4                                  NaN           NaN        NaN        NaN   \n",
      "\n",
      "  language                                        description     error  \n",
      "0       en  75th Anniversary Edition—Includes a New Introd...       NaN  \n",
      "1       en  After discovering the true nature of the One R...       NaN  \n",
      "2       en  A fisherman's daughter in 1930s Japan rises to...       NaN  \n",
      "3      NaN                                                NaN  HTTP 503  \n",
      "4      NaN                                                NaN  HTTP 503  \n",
      "Merged ISBN-based results for 1276 books\n",
      "\n",
      "Title/Author results preview:\n",
      "                           title                       author     error  \\\n",
      "0                  the alchemist  paulo coelho, alan r clarke  HTTP 429   \n",
      "1          the screwtape letters                     cs lewis       NaN   \n",
      "2  holy bible king james version                    anonymous       NaN   \n",
      "3                      after you                   jojo moyes       NaN   \n",
      "4          assassin's apprentice                   robin hobb  HTTP 429   \n",
      "\n",
      "   pageCount                           publisher publishedDate  \\\n",
      "0        NaN                                 NaN           NaN   \n",
      "1      106.0  Strelbytskyy Multimedia Publishing    2023-12-08   \n",
      "2      520.0             Independently Published    2019-03-06   \n",
      "3      429.0                          Penguin UK    2015-09-24   \n",
      "4        NaN                                 NaN           NaN   \n",
      "\n",
      "               categories language  \\\n",
      "0                     NaN      NaN   \n",
      "1  [Literary Collections]       en   \n",
      "2               [Fiction]       en   \n",
      "3               [Fiction]       en   \n",
      "4                     NaN      NaN   \n",
      "\n",
      "                                         description  \n",
      "0                                                NaN  \n",
      "1  \"The Screwtape Letters\" by C.S. Lewis is a tho...  \n",
      "2  The King James Version (KJV), commonly known a...  \n",
      "3  The beautiful love story that will make you bo...  \n",
      "4                                                NaN  \n",
      "Merged title/author-based results for 452 books\n",
      "\n",
      "Google Books data merged:\n",
      "  - pageCount_google: 1157 values\n",
      "  - publishedDate_google: 1169 values\n",
      "  - categories_google: 1143 values\n",
      "  - language_google: 1171 values\n",
      "  - publisher_google: 978 values\n",
      "  - description_google: 1127 values\n",
      "\n",
      "Sample of cleaned Google Books data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>pages_clean</th>\n",
       "      <th>pageCount_google</th>\n",
       "      <th>pageCount_google_clean</th>\n",
       "      <th>publication_date_clean</th>\n",
       "      <th>publishedDate_google</th>\n",
       "      <th>publishedDate_google_clean</th>\n",
       "      <th>language_clean</th>\n",
       "      <th>language_google</th>\n",
       "      <th>language_google_clean</th>\n",
       "      <th>genres_clean</th>\n",
       "      <th>categories_google</th>\n",
       "      <th>categories_google_clean</th>\n",
       "      <th>genres_simplified</th>\n",
       "      <th>publisher_google</th>\n",
       "      <th>publisher_clean</th>\n",
       "      <th>description_google</th>\n",
       "      <th>description_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3938</th>\n",
       "      <td>history of art</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1962-01-01</td>\n",
       "      <td>1997</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Art]</td>\n",
       "      <td>[art]</td>\n",
       "      <td>[historical fiction]</td>\n",
       "      <td>None</td>\n",
       "      <td>thames and hudson</td>\n",
       "      <td>The fifth edition of this work is revised by t...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>alias grace</td>\n",
       "      <td>636.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>btb bei goldmann</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9338</th>\n",
       "      <td>the protector</td>\n",
       "      <td>322.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2005-10</td>\n",
       "      <td>2005-10-01</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>[fiction, romance]</td>\n",
       "      <td>Tyndale House Publishers, Inc.</td>\n",
       "      <td>tyndale house publishers</td>\n",
       "      <td>C.1 ST. AID B &amp; T. 07-25-2007. $13.99.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6471</th>\n",
       "      <td>the hidden city</td>\n",
       "      <td>512.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>1995-08-01</td>\n",
       "      <td>1995-08-01</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>[fiction, fantasy]</td>\n",
       "      <td>Del Rey</td>\n",
       "      <td>del rey</td>\n",
       "      <td>Sparhawk’s epic quest comes to a riveting conc...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7502</th>\n",
       "      <td>what katy did</td>\n",
       "      <td>136.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1872-01-01</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[fiction, young adult, children]</td>\n",
       "      <td>None</td>\n",
       "      <td>adamant media corporation</td>\n",
       "      <td>This Elibron Classics title is a reprint of th...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7222</th>\n",
       "      <td>what the night knows</td>\n",
       "      <td>NaN</td>\n",
       "      <td>442.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bantam Dell Publishing Group</td>\n",
       "      <td>penguin random house</td>\n",
       "      <td>A companion to The Darkest Evening of the Year...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7205</th>\n",
       "      <td>stars of fortune</td>\n",
       "      <td>314.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>Berkley</td>\n",
       "      <td>None</td>\n",
       "      <td>Includes excerpt from author's, \"The Obsession...</td>\n",
       "      <td>to celebrate the rise of their new queen three...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>naruto -ナルト- 巻ノ四十三</td>\n",
       "      <td>248.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>2008-08</td>\n",
       "      <td>2008-08-01</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['fantasy', 'comics', 'graphic novels', 'anime...</td>\n",
       "      <td>[Comic books, strips, etc]</td>\n",
       "      <td>[comic books strips etc]</td>\n",
       "      <td>['fantasy', 'comics', 'graphic novels', 'other...</td>\n",
       "      <td>Shueisha/Tsai Fong Books</td>\n",
       "      <td>shueisha</td>\n",
       "      <td>This series has won the highest rating both as...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>shadow spell</td>\n",
       "      <td>339.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>2014-03-25</td>\n",
       "      <td>2014-03-25</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>[fiction, fantasy, romance]</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>None</td>\n",
       "      <td>From #1 New York Times bestselling author Nora...</td>\n",
       "      <td>with the legends and lore of ireland running t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6318</th>\n",
       "      <td>shattered</td>\n",
       "      <td>304.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>1973-01-01</td>\n",
       "      <td>1986-11-15</td>\n",
       "      <td>1986-11-15</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>berkley</td>\n",
       "      <td>Getting there is supposed to be half the fun, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>lover eternal</td>\n",
       "      <td>464.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>2006-03-07</td>\n",
       "      <td>2006-03-07</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>National Geographic Books</td>\n",
       "      <td>signet</td>\n",
       "      <td>A warrior with inner demons falls for a woman ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9124</th>\n",
       "      <td>curious george takes a job</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1947-01-01</td>\n",
       "      <td>2004-07</td>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Children's stories]</td>\n",
       "      <td>[childrens stories]</td>\n",
       "      <td>[children]</td>\n",
       "      <td>None</td>\n",
       "      <td>walker books ltd</td>\n",
       "      <td>One day George escapes from the zoo. He rides ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9356</th>\n",
       "      <td>my brother sam is dead</td>\n",
       "      <td>240.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>1974-01-01</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Juvenile Fiction]</td>\n",
       "      <td>[juvenile fiction]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Scholastic Paperbacks</td>\n",
       "      <td>scholastic paperbacks</td>\n",
       "      <td>When Sam Meeker leaves his home in Redding, Co...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>songs of the humpback whale</td>\n",
       "      <td>352.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>1992-01-01</td>\n",
       "      <td>2001-10</td>\n",
       "      <td>2001-10-01</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>[fiction, adventure]</td>\n",
       "      <td>Simon and Schuster</td>\n",
       "      <td>washington square press</td>\n",
       "      <td>Back in print by popular demand, Picoult's acc...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4184</th>\n",
       "      <td>fallen crest high</td>\n",
       "      <td>375.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>2019-11-10</td>\n",
       "      <td>2019-11-10</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['romance', 'new adult', 'young adult', 'high ...</td>\n",
       "      <td>[Fiction]</td>\n",
       "      <td>[fiction]</td>\n",
       "      <td>['romance', 'new adult', 'young adult', 'high ...</td>\n",
       "      <td>Fallen Crest</td>\n",
       "      <td>None</td>\n",
       "      <td>Mason and Logan Kade are two brothers who did ...</td>\n",
       "      <td>alternate cover edition of asin b009 zozp0 wma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title_clean  pages_clean pageCount_google  \\\n",
       "3938               history of art       1000.0           1000.0   \n",
       "1844                  alias grace        636.0            636.0   \n",
       "9338                the protector        322.0            370.0   \n",
       "6471              the hidden city        512.0            514.0   \n",
       "7502                what katy did        136.0            136.0   \n",
       "7222         what the night knows          NaN            442.0   \n",
       "7205             stars of fortune        314.0            337.0   \n",
       "3265           naruto -ナルト- 巻ノ四十三        248.0            248.0   \n",
       "4956                 shadow spell        339.0            354.0   \n",
       "6318                    shattered        304.0            308.0   \n",
       "888                 lover eternal        464.0              0.0   \n",
       "9124   curious george takes a job         47.0             47.0   \n",
       "9356       my brother sam is dead        240.0            216.0   \n",
       "4426  songs of the humpback whale        352.0            372.0   \n",
       "4184            fallen crest high        375.0                0   \n",
       "\n",
       "      pageCount_google_clean publication_date_clean publishedDate_google  \\\n",
       "3938                  1000.0             1962-01-01                 1997   \n",
       "1844                   636.0             1996-01-01                 1998   \n",
       "9338                   370.0             2001-01-01              2005-10   \n",
       "6471                   514.0             1994-01-01           1995-08-01   \n",
       "7502                   136.0             1872-01-01           1999-01-01   \n",
       "7222                   442.0             2010-01-01                 2010   \n",
       "7205                   337.0             2015-01-01           2015-11-03   \n",
       "3265                   248.0             2008-01-01              2008-08   \n",
       "4956                   354.0             2014-01-01           2014-03-25   \n",
       "6318                   308.0             1973-01-01           1986-11-15   \n",
       "888                      NaN             2006-01-01           2006-03-07   \n",
       "9124                    47.0             1947-01-01              2004-07   \n",
       "9356                   216.0             1974-01-01                 2005   \n",
       "4426                   372.0             1992-01-01              2001-10   \n",
       "4184                     NaN             2012-01-01           2019-11-10   \n",
       "\n",
       "     publishedDate_google_clean language_clean language_google  \\\n",
       "3938                 1997-01-01             en              en   \n",
       "1844                 1998-01-01             de              de   \n",
       "9338                 2005-10-01             en              en   \n",
       "6471                 1995-08-01             en              en   \n",
       "7502                 1999-01-01             en              en   \n",
       "7222                 2010-01-01             en              en   \n",
       "7205                 2015-11-03             en              en   \n",
       "3265                 2008-08-01             ja              ja   \n",
       "4956                 2014-03-25             en              en   \n",
       "6318                 1986-11-15             en              en   \n",
       "888                  2006-03-07             en              en   \n",
       "9124                 2004-07-01            NaN              en   \n",
       "9356                 2005-01-01             en              en   \n",
       "4426                 2001-10-01             en              en   \n",
       "4184                 2019-11-10             en              en   \n",
       "\n",
       "     language_google_clean                                       genres_clean  \\\n",
       "3938                    en                                                NaN   \n",
       "1844                   NaN                                                NaN   \n",
       "9338                    en                                                NaN   \n",
       "6471                    en                                                NaN   \n",
       "7502                    en                                                NaN   \n",
       "7222                    en                                                NaN   \n",
       "7205                    en                                                NaN   \n",
       "3265                   NaN  ['fantasy', 'comics', 'graphic novels', 'anime...   \n",
       "4956                    en                                                NaN   \n",
       "6318                    en                                                NaN   \n",
       "888                     en                                                NaN   \n",
       "9124                    en                                                NaN   \n",
       "9356                    en                                                NaN   \n",
       "4426                    en                                                NaN   \n",
       "4184                    en  ['romance', 'new adult', 'young adult', 'high ...   \n",
       "\n",
       "               categories_google   categories_google_clean  \\\n",
       "3938                       [Art]                     [art]   \n",
       "1844                        None                      None   \n",
       "9338                   [Fiction]                 [fiction]   \n",
       "6471                   [Fiction]                 [fiction]   \n",
       "7502                        None                      None   \n",
       "7222                   [Fiction]                 [fiction]   \n",
       "7205                   [Fiction]                 [fiction]   \n",
       "3265  [Comic books, strips, etc]  [comic books strips etc]   \n",
       "4956                   [Fiction]                 [fiction]   \n",
       "6318                   [Fiction]                 [fiction]   \n",
       "888                    [Fiction]                 [fiction]   \n",
       "9124        [Children's stories]       [childrens stories]   \n",
       "9356          [Juvenile Fiction]        [juvenile fiction]   \n",
       "4426                   [Fiction]                 [fiction]   \n",
       "4184                   [Fiction]                 [fiction]   \n",
       "\n",
       "                                      genres_simplified  \\\n",
       "3938                               [historical fiction]   \n",
       "1844                                                NaN   \n",
       "9338                                 [fiction, romance]   \n",
       "6471                                 [fiction, fantasy]   \n",
       "7502                   [fiction, young adult, children]   \n",
       "7222                                                NaN   \n",
       "7205                                          [fiction]   \n",
       "3265  ['fantasy', 'comics', 'graphic novels', 'other...   \n",
       "4956                        [fiction, fantasy, romance]   \n",
       "6318                                                NaN   \n",
       "888                                                 NaN   \n",
       "9124                                         [children]   \n",
       "9356                                                NaN   \n",
       "4426                               [fiction, adventure]   \n",
       "4184  ['romance', 'new adult', 'young adult', 'high ...   \n",
       "\n",
       "                    publisher_google            publisher_clean  \\\n",
       "3938                            None          thames and hudson   \n",
       "1844                            None           btb bei goldmann   \n",
       "9338  Tyndale House Publishers, Inc.   tyndale house publishers   \n",
       "6471                         Del Rey                    del rey   \n",
       "7502                            None  adamant media corporation   \n",
       "7222    Bantam Dell Publishing Group       penguin random house   \n",
       "7205                         Berkley                       None   \n",
       "3265        Shueisha/Tsai Fong Books                   shueisha   \n",
       "4956                         Penguin                       None   \n",
       "6318                         Penguin                    berkley   \n",
       "888        National Geographic Books                     signet   \n",
       "9124                            None           walker books ltd   \n",
       "9356           Scholastic Paperbacks      scholastic paperbacks   \n",
       "4426              Simon and Schuster    washington square press   \n",
       "4184                    Fallen Crest                       None   \n",
       "\n",
       "                                     description_google  \\\n",
       "3938  The fifth edition of this work is revised by t...   \n",
       "1844                                               None   \n",
       "9338             C.1 ST. AID B & T. 07-25-2007. $13.99.   \n",
       "6471  Sparhawk’s epic quest comes to a riveting conc...   \n",
       "7502  This Elibron Classics title is a reprint of th...   \n",
       "7222  A companion to The Darkest Evening of the Year...   \n",
       "7205  Includes excerpt from author's, \"The Obsession...   \n",
       "3265  This series has won the highest rating both as...   \n",
       "4956  From #1 New York Times bestselling author Nora...   \n",
       "6318  Getting there is supposed to be half the fun, ...   \n",
       "888   A warrior with inner demons falls for a woman ...   \n",
       "9124  One day George escapes from the zoo. He rides ...   \n",
       "9356  When Sam Meeker leaves his home in Redding, Co...   \n",
       "4426  Back in print by popular demand, Picoult's acc...   \n",
       "4184  Mason and Logan Kade are two brothers who did ...   \n",
       "\n",
       "                                      description_clean  \n",
       "3938                                               None  \n",
       "1844                                               None  \n",
       "9338                                               None  \n",
       "6471                                               None  \n",
       "7502                                               None  \n",
       "7222                                               None  \n",
       "7205  to celebrate the rise of their new queen three...  \n",
       "3265                                               None  \n",
       "4956  with the legends and lore of ireland running t...  \n",
       "6318                                               None  \n",
       "888                                                None  \n",
       "9124                                               None  \n",
       "9356                                               None  \n",
       "4426                                               None  \n",
       "4184  alternate cover edition of asin b009 zozp0 wma...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# merge isbn_results back to gb_enriched\n",
    "if isbn_results:\n",
    "    google_isbn_df = pd.DataFrame(isbn_results)\n",
    "    print(\"\\nISBN results preview:\")\n",
    "    print(google_isbn_df.head())\n",
    "    \n",
    "    # Create ISBN mapping\n",
    "    isbn_to_data = {str(row['isbn']): row for _, row in google_isbn_df.iterrows() \n",
    "                    if 'isbn' in row and pd.notna(row.get('isbn'))}\n",
    "    \n",
    "    # Update gb_enriched\n",
    "    books_with_isbn = new_to_impute[new_to_impute['isbn_query'].notna()].copy()\n",
    "    \n",
    "    for idx in books_with_isbn.index:\n",
    "        isbn = str(books_with_isbn.loc[idx, 'isbn_query'])\n",
    "        if isbn in isbn_to_data:\n",
    "            result = isbn_to_data[isbn]\n",
    "            if pd.isna(result.get('error')):\n",
    "                gb_enriched.loc[idx, 'pageCount_google'] = result.get('pageCount')\n",
    "                gb_enriched.loc[idx, 'publishedDate_google'] = result.get('publishedDate')\n",
    "                gb_enriched.loc[idx, 'categories_google'] = result.get('categories')\n",
    "                gb_enriched.loc[idx, 'language_google'] = result.get('language')\n",
    "                gb_enriched.loc[idx, 'publisher_google'] = result.get('publisher')\n",
    "                gb_enriched.loc[idx, 'description_google'] = result.get('description')\n",
    "    \n",
    "    print(f\"Merged ISBN-based results for {len(isbn_to_data)} books\")\n",
    "\n",
    "\n",
    "# merge title/author results back to gb_enriched\n",
    "\n",
    "if title_author_results:\n",
    "    google_title_df = pd.DataFrame(title_author_results)\n",
    "    print(\"\\nTitle/Author results preview:\")\n",
    "    print(google_title_df.head())\n",
    "    \n",
    "    # Recreate books_without_isbn from new_to_impute\n",
    "    books_without_isbn = new_to_impute[new_to_impute['isbn_query'].isna()].copy()\n",
    "    \n",
    "    # Create title|author key mapping\n",
    "    for i, (idx, row) in enumerate(books_without_isbn.iterrows()):\n",
    "        if i < len(google_title_df):\n",
    "            title_author_key = f\"{row['title_clean']}|{row['author_clean']}\"\n",
    "            if title_author_key in google_cache:\n",
    "                result = google_cache[title_author_key]\n",
    "                if pd.isna(result.get('error')):\n",
    "                    gb_enriched.loc[idx, 'pageCount_google'] = result.get('pageCount')\n",
    "                    gb_enriched.loc[idx, 'publishedDate_google'] = result.get('publishedDate')\n",
    "                    gb_enriched.loc[idx, 'categories_google'] = result.get('categories')\n",
    "                    gb_enriched.loc[idx, 'language_google'] = result.get('language')\n",
    "                    gb_enriched.loc[idx, 'publisher_google'] = result.get('publisher')\n",
    "                    gb_enriched.loc[idx, 'description_google'] = result.get('description')\n",
    "    \n",
    "    print(f\"Merged title/author-based results for {len(books_without_isbn)} books\")\n",
    "\n",
    "# Verify merge\n",
    "print(\"\\nGoogle Books data merged:\")\n",
    "for col in google_columns:\n",
    "    count = gb_enriched[col].notna().sum()\n",
    "    print(f\"  - {col}: {count} values\")\n",
    "\n",
    "# clean Google Books API data\n",
    "from src.cleaning.utils.pipeline import apply_cleaners_selectively\n",
    "\n",
    "gb_enriched = apply_cleaners_selectively(\n",
    "    gb_enriched,\n",
    "    fields_to_clean=[\n",
    "        'pageCount',\n",
    "        'publishedDate',\n",
    "        'language',\n",
    "        'categories',\n",
    "        'publisher',\n",
    "        'description'\n",
    "        ],\n",
    "    source_suffix='_google',\n",
    "    target_suffix='_google_clean',\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# Verify cleaning\n",
    "print(\"\\nSample of cleaned Google Books data:\")\n",
    "display(gb_enriched[[\n",
    "    'title_clean',\n",
    "    'pages_clean',\n",
    "    'pageCount_google',\n",
    "    'pageCount_google_clean',\n",
    "    'publication_date_clean',\n",
    "    'publishedDate_google',\n",
    "    'publishedDate_google_clean',\n",
    "    'language_clean',\n",
    "    'language_google',\n",
    "    'language_google_clean',\n",
    "    'genres_clean',\n",
    "    'categories_google',\n",
    "    'categories_google_clean',\n",
    "    'genres_simplified',\n",
    "    'publisher_google',\n",
    "    'publisher_clean',\n",
    "    'description_google',\n",
    "    'description_clean',\n",
    "]].dropna(subset=['pageCount_google_clean', 'language_google_clean'], how='all').sample(min(15, len(gb_enriched)), random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76157a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating genres_simplified from Google Books categories ---\n",
      "Books with Google categories but no genres_simplified: 583\n",
      "genres_simplified: mapped 376 values from Google Books categories\n",
      "\n",
      "--- Filling missing values with cleaned Google Books data ---\n",
      "\n",
      "--- Filling remaining page_clean using Google Books data ---\n",
      "pages_clean: filled 252 values from Google Books\n",
      "\n",
      "--- Filling remaining publication_date_clean using Google Books data ---\n",
      "publication_date_clean: filled 0 values from Google Books\n",
      "\n",
      "--- Filling remaining language_clean using Google Books data ---\n",
      "language_clean: filled 51 values from Google Books\n",
      "\n",
      "--- Filling remaining publisher_clean using Google Books data ---\n",
      "publisher_clean: filled 139 values from Google Books\n",
      "\n",
      "--- Filling remaining description_clean using Google Books data ---\n",
      "description_clean (Google): filled 992 values\n",
      "\n",
      "--- FINAL ENRICHMENT SUMMARY (ALL SOURCES) ---\n",
      "\n",
      "Total books enriched with Google Books data: 1143\n",
      "\n",
      "--- FINAL METADATA COVERAGE ---\n",
      "Books with pages_clean: 9562 / 10000 (95.6%)\n",
      "Books with publication_date_clean: 9998 / 10000 (100.0%)\n",
      "Books with publisher_clean: 9608 / 10000 (96.1%)\n",
      "Books with description_clean: 9510 / 10000 (95.1%)\n",
      "Books with valid language_clean: 9956 / 10000 (99.6%)\n",
      "\n",
      "--- FINAL GENRE COVERAGE ---\n",
      "Books with genres_clean: 8082 / 10000 (80.8%)\n",
      "Books with genres_simplified: 9254 / 10000 (92.5%)\n"
     ]
    }
   ],
   "source": [
    "# after cleaning Google Books data, we'll add genre mapping\n",
    "print(\"\\n--- Generating genres_simplified from Google Books categories ---\")\n",
    "\n",
    "books_needing_google_genre_mapping = (\n",
    "    (gb_enriched['genres_simplified'].isna()) & \n",
    "    (gb_enriched['categories_google_clean'].notna())\n",
    ")\n",
    "\n",
    "print(f\"Books with Google categories but no genres_simplified: {books_needing_google_genre_mapping.sum()}\")\n",
    "\n",
    "if books_needing_google_genre_mapping.sum() > 0:\n",
    "    gb_enriched.loc[books_needing_google_genre_mapping, 'genres_simplified'] = (\n",
    "        gb_enriched.loc[books_needing_google_genre_mapping, 'categories_google_clean']\n",
    "        .apply(lambda x: map_subjects_to_genres(x) if isinstance(x, list) else None)\n",
    "    )\n",
    "    \n",
    "    filled_genres = (\n",
    "        gb_enriched.loc[books_needing_google_genre_mapping, 'genres_simplified'].notna().sum()\n",
    "    )\n",
    "    print(f\"genres_simplified: mapped {filled_genres} values from Google Books categories\")\n",
    "\n",
    "\n",
    "# fill missing values with cleaned Google Books data\n",
    "print(\"\\n--- Filling missing values with cleaned Google Books data ---\")\n",
    "\n",
    "# Fill pages_clean\n",
    "print(\"\\n--- Filling remaining page_clean using Google Books data ---\")\n",
    "before_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "gb_enriched['pages_clean'] = gb_enriched['pages_clean'].fillna(gb_enriched['pageCount_google_clean'])\n",
    "after_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "print(f\"pages_clean: filled {before_pages - after_pages} values from Google Books\")\n",
    "\n",
    "# Fill publication_date_clean\n",
    "print(\"\\n--- Filling remaining publication_date_clean using Google Books data ---\")\n",
    "before_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "gb_enriched['publication_date_clean'] = gb_enriched['publication_date_clean'].fillna(gb_enriched['publishedDate_google_clean'])\n",
    "after_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "print(f\"publication_date_clean: filled {before_date - after_date} values from Google Books\")\n",
    "\n",
    "# Fill language_clean\n",
    "print(\"\\n--- Filling remaining language_clean using Google Books data ---\")\n",
    "before_lang = (gb_enriched['language_clean'].isna() | \n",
    "               gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "mask = (gb_enriched['language_clean'].isna() | \n",
    "        gb_enriched['language_clean'].isin(['unknown', '', 'None']))\n",
    "gb_enriched.loc[mask, 'language_clean'] = gb_enriched.loc[mask, 'language_google_clean']\n",
    "after_lang = (gb_enriched['language_clean'].isna() | \n",
    "              gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "print(f\"language_clean: filled {before_lang - after_lang} values from Google Books\")\n",
    "\n",
    "# Fill publisher_clean\n",
    "print(\"\\n--- Filling remaining publisher_clean using Google Books data ---\")\n",
    "before_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "gb_enriched['publisher_clean'] = gb_enriched['publisher_clean'].fillna(\n",
    "    gb_enriched['publisher_google_clean']\n",
    ")\n",
    "after_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "print(f\"publisher_clean: filled {before_publisher - after_publisher} values from Google Books\")\n",
    "\n",
    "\n",
    "# Fill description_clean\n",
    "print(\"\\n--- Filling remaining description_clean using Google Books data ---\")\n",
    "before_desc_google = gb_enriched['description_clean'].isna().sum()\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].fillna(\n",
    "    gb_enriched['description_google_clean']\n",
    ")\n",
    "after_desc_google = gb_enriched['description_clean'].isna().sum()\n",
    "\n",
    "print(f\"description_clean (Google): filled {before_desc_google - after_desc_google} values\")\n",
    "\n",
    "# final enrichment summary\n",
    "print(\"\\n--- FINAL ENRICHMENT SUMMARY (ALL SOURCES) ---\")\n",
    "\n",
    "print(f\"\\nTotal books enriched with Google Books data: {gb_enriched[gb_enriched['categories_google_clean'].notna()].shape[0]}\")\n",
    "\n",
    "print(\"\\n--- FINAL METADATA COVERAGE ---\")\n",
    "print(f\"Books with pages_clean: {gb_enriched['pages_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['pages_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with publication_date_clean: {gb_enriched['publication_date_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['publication_date_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with publisher_clean: {gb_enriched['publisher_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['publisher_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with description_clean: {gb_enriched['description_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['description_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "valid_language = gb_enriched['language_clean'].notna() & ~gb_enriched['language_clean'].isin(['unknown', '', 'None'])\n",
    "print(f\"Books with valid language_clean: {valid_language.sum()} / {len(gb_enriched)} ({valid_language.sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n--- FINAL GENRE COVERAGE ---\")\n",
    "print(f\"Books with genres_clean: {gb_enriched['genres_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['genres_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with genres_simplified: {gb_enriched['genres_simplified'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['genres_simplified'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9c6fc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb_enriched v4 saved successfully in data/interim/merge directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/enriched\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 4\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14412823",
   "metadata": {},
   "source": [
    "Our multi-source enrichment strategy (BBE → OpenLibrary → Google Books) achieved excellent metadata coverage: **95.1%** for page counts, **100%** for publication dates, **99.4%** for valid language codes, **94.7%** publishers and **85.2%** descriptions. Genre coverage reached **80.8%** for `genres_clean` and **90.7%** for `genres_simplified`, a significant improvement from the original Goodbooks dataset which lacked genre information entirely.\n",
    "\n",
    "This enriched dataset now provides a comprehensive foundation for modeling and analysis. The combination of catalog metadata from BBE, behavioral data from Goodbooks ratings, and API-sourced supplemental information creates a unified dataset that supports both predictive modeling and catalog diversity analysis. The next step is filtering to English-language titles and preparing the final model-ready dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f7600",
   "metadata": {},
   "source": [
    "#### Final Metadata Enrichment Steps\n",
    "\n",
    "After completing API-based enrichment, we perform three final metadata enhancement steps to ensure completeness and consistency across all enriched records:\n",
    "\n",
    "1. **Major Publisher Classification**: We apply the `is_major_publisher` flag to books that received publisher data from APIs but weren't present in the BBE dataset. Using the same publisher pattern matching from Notebook 02, we classify publishers against our curated list of major publishing houses.\n",
    "\n",
    "2. **Awards Flag Completion**: We fill missing `has_award` values with `False` for all books that weren't in the BBE dataset (which contains award metadata). Since API sources don't reliably provide award information, we assume absence of award data means no awards.\n",
    "\n",
    "3. **NLP-Ready Description Generation**: For books enriched with API descriptions, we apply the same NLP cleaning pipeline used in Notebook 02. This converts descriptions into analysis-ready text by removing HTML tags, normalizing whitespace, and standardizing punctuatio, ensuring consistency across BBE and API-sourced descriptions for future text analysis tasks.\n",
    "\n",
    "These steps ensure that all enriched books have the same metadata structure and quality as the original BBE dataset, maintaining consistency across the entire unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "813f551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books without is_major_publisher flag: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# load publisher patterns from JSON file\n",
    "with open(\"src/cleaning/mappings/publisher_parent_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    major_publishers = json.load(f)\n",
    "\n",
    "mask_missing_major = gb_enriched['is_major_publisher'].isna()\n",
    "\n",
    "gb_enriched.loc[mask_missing_major, 'is_major_publisher'] = (\n",
    "    gb_enriched.loc[mask_missing_major, 'publisher_clean']\n",
    "        .str.lower()\n",
    "        .apply(lambda x: any(mp in x for mp in major_publishers) if isinstance(x, str) else False)\n",
    ")\n",
    "print(f\"Books without is_major_publisher flag: {gb_enriched['is_major_publisher'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2367447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books without has_awards flag: 1918\n",
      "Remaining books without has_awards flag: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Books without has_awards flag: {gb_enriched['has_award'].isna().sum()}\")\n",
    "gb_enriched['has_award'] = (\n",
    "    gb_enriched['has_award']\n",
    "    .fillna(False)\n",
    "    .astype('bool')\n",
    ")\n",
    "print(f\"Remaining books without has_awards flag: {gb_enriched['has_award'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ba4c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cleaning.utils.text_cleaning import (\n",
    "    clean_description_nlp\n",
    ")\n",
    "# generate description_nlp from description_clean where API descriptions exist\n",
    "mask_api_desc = (\n",
    "    gb_enriched['description_openlib'].notna() |\n",
    "    gb_enriched['description_google'].notna()\n",
    ")\n",
    "\n",
    "gb_enriched.loc[mask_api_desc, 'description_nlp'] = (\n",
    "    gb_enriched.loc[mask_api_desc, 'description_clean']\n",
    "        .apply(clean_description_nlp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77e3add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing description_clean: 491\n",
      "Missing description_nlp: 491\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fix_missing_text(col):\n",
    "    return (\n",
    "        gb_enriched[col]\n",
    "        .replace([\"\", \" \", \"None\", \"none\", \"nan\", \"Nan\", \"NAN\"], np.nan)\n",
    "        .replace(r\"^\\s+$\", np.nan, regex=True)\n",
    "    )\n",
    "\n",
    "gb_enriched['description_clean'] = fix_missing_text('description_clean')\n",
    "gb_enriched['description_nlp']   = fix_missing_text('description_nlp')\n",
    "\n",
    "print(\"Missing description_clean:\", gb_enriched['description_clean'].isna().sum())\n",
    "print(\"Missing description_nlp:\", gb_enriched['description_nlp'].isna().sum())\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].astype(\"string\")\n",
    "gb_enriched['description_nlp']   = gb_enriched['description_nlp'].astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c1b3c",
   "metadata": {},
   "source": [
    "### Filtering for English-Language Books\n",
    "\n",
    "To ensure consistency and focus for downstream analysis and modeling, we filter the enriched dataset to include only **English-language books**. This step is critical for:\n",
    "\n",
    "- **Genre diversity analysis**: Comparing genre distributions across a linguistically consistent corpus\n",
    "- **Ratings behavior modeling**: Ensuring user rating patterns reflect a common language context\n",
    "- **Text analysis (stretch)**: Enabling NLP tasks on descriptions without multilingual complexity\n",
    "\n",
    "We create a filtered copy of `gb_enriched` containing only books where `language_clean` is identified as English (using ISO 639 language code`'en'`). This filtered dataset will serve as the primary input for modeling and analysis, while the full enriched dataset (including non-English titles) is preserved for reference.\n",
    "\n",
    "The English-only dataset is saved as the final output, ready for exploratory analysis and model development in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "566c3ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered (EN only): (9761, 61)\n"
     ]
    }
   ],
   "source": [
    "gb_enriched_en = gb_enriched[gb_enriched['language_clean'] == 'en'].copy()\n",
    "print(\"Filtered (EN only):\", gb_enriched_en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30f39309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after dropping enrichment data: 35\n",
      "Shape before dropping duplicates: (9761, 35)\n",
      "Shape after dropping duplicates: (9761, 35)\n",
      "Duplicates removed: 9761\n",
      "\n",
      "Final columns for analysis:\n",
      "['book_id', 'work_text_reviews_count', 'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5', 'goodreads_id_clean', 'best_book_id_clean', 'work_id_clean', 'authors_list', 'author_clean', 'language_clean', 'publication_date_clean', 'isbn_clean', 'isbn_standard', 'rating_clean', 'numRatings_clean', 'numRatings_log', 'ratings_1_share', 'ratings_2_share', 'ratings_3_share', 'ratings_4_share', 'ratings_5_share', 'work_text_reviews_log', 'series_clean', 'title_clean', 'pages_clean', 'genres_clean', 'genres_simplified', 'publisher_clean', 'is_major_publisher', 'has_award', 'description_clean', 'description_nlp']\n"
     ]
    }
   ],
   "source": [
    "# drop intermediate enrichment columns\n",
    "enrichment_columns_to_drop = [\n",
    "    # OpenLibrary raw and intermediate columns\n",
    "    'pages_openlib',\n",
    "    'publication_date_openlib',\n",
    "    'language_openlib',\n",
    "    'subjects_openlib',\n",
    "    'publisher_openlib',\n",
    "    'description_openlib',\n",
    "    'pages_openlib_clean',\n",
    "    'publication_date_openlib_clean',\n",
    "    'language_openlib_clean',\n",
    "    'subjects_openlib_clean',\n",
    "    'publisher_openlib_clean',\n",
    "    'description_openlib_clean',\n",
    "    # Google Books raw and intermediate columns\n",
    "    'pageCount_google',\n",
    "    'publishedDate_google',\n",
    "    'categories_google',\n",
    "    'language_google',\n",
    "    'publisher_google',\n",
    "    'description_google',\n",
    "    'pageCount_google_clean',\n",
    "    'publishedDate_google_clean',\n",
    "    'language_google_clean',\n",
    "    'categories_google_clean',\n",
    "    'publisher_google_clean',\n",
    "    'description_google_clean',\n",
    "    # Query helper column\n",
    "    'isbn_query',\n",
    "    # Other intermediate columns\n",
    "    'isbn13_clean',\n",
    "]\n",
    "\n",
    "# drop enrichment columns\n",
    "gb_enriched_en = gb_enriched_en.drop(columns=enrichment_columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Columns after dropping enrichment data: {len(gb_enriched_en.columns)}\")\n",
    "print(f\"Shape before dropping duplicates: {gb_enriched_en.shape}\")\n",
    "\n",
    "# drop duplicate rows based on goodreads_id_clean (keep first occurrence)\n",
    "gb_enriched_en = gb_enriched_en.drop_duplicates(subset=['goodreads_id_clean'], keep='first')\n",
    "\n",
    "print(f\"Shape after dropping duplicates: {gb_enriched_en.shape}\")\n",
    "print(f\"Duplicates removed: {gb_enriched_en.shape[0]}\")\n",
    "\n",
    "# Verify final columns\n",
    "print(\"\\nFinal columns for analysis:\")\n",
    "print(gb_enriched_en.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c01d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL DATA QUALITY CHECKS - gb_enriched_en\n",
      "============================================================\n",
      "\n",
      "1. DUPLICATE CHECK\n",
      "Total rows: 9761\n",
      "Duplicate goodreads_id_clean: 0\n",
      "\n",
      "2. NULL VALUES IN CRITICAL COLUMNS\n",
      "goodreads_id_clean: 0 nulls (0.00%)\n",
      "title_clean: 0 nulls (0.00%)\n",
      "author_clean: 0 nulls (0.00%)\n",
      "language_clean: 0 nulls (0.00%)\n",
      "\n",
      "3. LANGUAGE CONSISTENCY CHECK\n",
      "Unique languages: ['en']\n",
      "\n",
      "4. DATA TYPE CHECK\n",
      "book_id                      int64\n",
      "work_text_reviews_count      int64\n",
      "ratings_1                    int64\n",
      "ratings_2                    int64\n",
      "ratings_3                    int64\n",
      "ratings_4                    int64\n",
      "ratings_5                    int64\n",
      "goodreads_id_clean          string\n",
      "best_book_id_clean           int64\n",
      "work_id_clean                int64\n",
      "authors_list                object\n",
      "author_clean                object\n",
      "language_clean              object\n",
      "publication_date_clean      object\n",
      "isbn_clean                  string\n",
      "isbn_standard               object\n",
      "rating_clean               float64\n",
      "numRatings_clean             int64\n",
      "numRatings_log             float64\n",
      "ratings_1_share            float64\n",
      "ratings_2_share            float64\n",
      "ratings_3_share            float64\n",
      "ratings_4_share            float64\n",
      "ratings_5_share            float64\n",
      "work_text_reviews_log      float64\n",
      "series_clean                object\n",
      "title_clean                 object\n",
      "pages_clean                float64\n",
      "genres_clean                object\n",
      "genres_simplified           object\n",
      "publisher_clean             object\n",
      "is_major_publisher          object\n",
      "has_award                     bool\n",
      "description_clean           string\n",
      "description_nlp             string\n",
      "dtype: object\n",
      "\n",
      "5. EMPTY STRING CHECK\n",
      "title_clean: 0 empty strings, 0 whitespace-only\n",
      "author_clean: 0 empty strings, 0 whitespace-only\n",
      "publisher_clean: 0 empty strings, 0 whitespace-only\n",
      "description_clean: 0 empty strings, 0 whitespace-only\n",
      "\n",
      "6. METADATA COVERAGE\n",
      "Pages: 96.0% coverage\n",
      "Publication Date: 100.0% coverage\n",
      "Publisher: 96.5% coverage\n",
      "Genres: 93.0% coverage\n",
      "Description: 95.8% coverage\n",
      "\n",
      "7. ENRICHMENT COLUMN CHECK\n",
      "No enrichment columns remaining\n",
      "\n",
      "8. SUMMARY STATISTICS\n",
      "Final dataset shape: (9761, 35)\n",
      "Columns: 35\n",
      "Memory usage: 26.70 MB\n",
      "\n",
      "============================================================\n",
      "CHECKS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# FINAL DATA QUALITY CHECKS BEFORE SAVING\n",
    "# ================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL DATA QUALITY CHECKS - gb_enriched_en\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# check for duplicates\n",
    "print(\"\\n1. DUPLICATE CHECK\")\n",
    "print(f\"Total rows: {len(gb_enriched_en)}\")\n",
    "duplicates = gb_enriched_en.duplicated(subset=['goodreads_id_clean']).sum()\n",
    "print(f\"Duplicate goodreads_id_clean: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"WARNING: Duplicates found!\")\n",
    "    display(gb_enriched_en[gb_enriched_en.duplicated(subset=['goodreads_id_clean'], keep=False)])\n",
    "\n",
    "# check for null values in critical columns\n",
    "print(\"\\n2. NULL VALUES IN CRITICAL COLUMNS\")\n",
    "critical_cols = ['goodreads_id_clean', 'title_clean', 'author_clean', 'language_clean']\n",
    "for col in critical_cols:\n",
    "    null_count = gb_enriched_en[col].isna().sum()\n",
    "    print(f\"{col}: {null_count} nulls ({null_count/len(gb_enriched_en)*100:.2f}%)\")\n",
    "    if null_count > 0:\n",
    "        print(f\"WARNING: Nulls found in {col}!\")\n",
    "\n",
    "# verify language filter worked\n",
    "print(\"\\n3. LANGUAGE CONSISTENCY CHECK\")\n",
    "unique_langs = gb_enriched_en['language_clean'].unique()\n",
    "print(f\"Unique languages: {unique_langs}\")\n",
    "if len(unique_langs) > 1 or unique_langs[0] != 'en':\n",
    "    print(\"WARNING: Non-English books found after filtering!\")\n",
    "\n",
    "# check data types\n",
    "print(\"\\n4. DATA TYPE CHECK\")\n",
    "print(gb_enriched_en.dtypes)\n",
    "\n",
    "# check for empty strings or whitespace-only values\n",
    "print(\"\\n5. EMPTY STRING CHECK\")\n",
    "text_cols = ['title_clean', 'author_clean', 'publisher_clean', 'description_clean']\n",
    "for col in text_cols:\n",
    "    if col in gb_enriched_en.columns:\n",
    "        empty = (gb_enriched_en[col] == '').sum()\n",
    "        whitespace = gb_enriched_en[col].str.strip().eq('').sum()\n",
    "        print(f\"{col}: {empty} empty strings, {whitespace} whitespace-only\")\n",
    "\n",
    "# check metadata coverage\n",
    "print(\"\\n6. METADATA COVERAGE\")\n",
    "metadata_cols = {\n",
    "    'pages_clean': 'Pages',\n",
    "    'publication_date_clean': 'Publication Date',\n",
    "    'publisher_clean': 'Publisher',\n",
    "    'genres_simplified': 'Genres',\n",
    "    'description_clean': 'Description'\n",
    "}\n",
    "for col, label in metadata_cols.items():\n",
    "    if col in gb_enriched_en.columns:\n",
    "        coverage = gb_enriched_en[col].notna().sum() / len(gb_enriched_en) * 100\n",
    "        print(f\"{label}: {coverage:.1f}% coverage\")\n",
    "\n",
    "# check for unexpected enrichment columns still present\n",
    "print(\"\\n7. ENRICHMENT COLUMN CHECK\")\n",
    "enrichment_patterns = ['_openlib', '_google', '_bbe', 'isbn_query']\n",
    "leftover_cols = [col for col in gb_enriched_en.columns \n",
    "                 if any(pattern in col for pattern in enrichment_patterns)]\n",
    "if leftover_cols:\n",
    "    print(f\"WARNING: Leftover enrichment columns found: {leftover_cols}\")\n",
    "else:\n",
    "    print(\"No enrichment columns remaining\")\n",
    "\n",
    "# aummary statistics\n",
    "print(\"\\n8. SUMMARY STATISTICS\")\n",
    "print(f\"Final dataset shape: {gb_enriched_en.shape}\")\n",
    "print(f\"Columns: {len(gb_enriched_en.columns)}\")\n",
    "print(f\"Memory usage: {gb_enriched_en.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHECKS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff346538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_internal_catalog saved successfully in outputs\\datasets\\cleaned directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'en_internal_catalog'\n",
    "clean_path = Path(\"outputs/datasets/cleaned\")\n",
    "clean_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gb_enriched_en.to_csv(clean_path / f\"{file_name}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} saved successfully in {clean_path} directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33cddfc",
   "metadata": {},
   "source": [
    "## BBE English Only \n",
    "\n",
    "The BBE dataset has already been cleaned in the previous steps. Since we are not enriching it further, we only need to filter it to English-language books for consistency with the Goodbooks dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31efe737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original BBE dataset shape: (52424, 36)\n",
      "English-only BBE dataset shape: (42634, 36)\n",
      "Books filtered out (non-English): 9790\n",
      "English books percentage: 81.3%\n",
      "\n",
      "Sample of English-only BBE dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goodreads_id_clean</th>\n",
       "      <th>authors_list</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>isbn_clean</th>\n",
       "      <th>language_clean</th>\n",
       "      <th>publication_date_clean</th>\n",
       "      <th>publisher_clean</th>\n",
       "      <th>is_major_publisher</th>\n",
       "      <th>bookFormat_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>description_nlp</th>\n",
       "      <th>series_clean</th>\n",
       "      <th>pages_clean</th>\n",
       "      <th>bbeVotes_clean</th>\n",
       "      <th>bbeScore_clean</th>\n",
       "      <th>likedPercent_clean</th>\n",
       "      <th>has_likedPercent</th>\n",
       "      <th>price_clean</th>\n",
       "      <th>price_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2767052</td>\n",
       "      <td>['suzanne collins']</td>\n",
       "      <td>suzanne collins</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>9780439023481</td>\n",
       "      <td>en</td>\n",
       "      <td>2008-09-14</td>\n",
       "      <td>scholastic</td>\n",
       "      <td>True</td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>winning means fame and fortunelosing means cer...</td>\n",
       "      <td>winning means fame and fortunelosing means cer...</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>374.0</td>\n",
       "      <td>30516</td>\n",
       "      <td>2993816</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['jk rowling', 'mary grandpre']</td>\n",
       "      <td>jk rowling, mary grandpre</td>\n",
       "      <td>harry potter and the order of the phoenix</td>\n",
       "      <td>9780439358071</td>\n",
       "      <td>en</td>\n",
       "      <td>2003-06-21</td>\n",
       "      <td>scholastic</td>\n",
       "      <td>True</td>\n",
       "      <td>paperback</td>\n",
       "      <td>...</td>\n",
       "      <td>there is a door at the end of a silent corrido...</td>\n",
       "      <td>there is a door at the end of a silent corrido...</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>870.0</td>\n",
       "      <td>26923</td>\n",
       "      <td>2632233</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.38</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2657</td>\n",
       "      <td>['harper lee']</td>\n",
       "      <td>harper lee</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>en</td>\n",
       "      <td>2007-07-11</td>\n",
       "      <td>harpercollins</td>\n",
       "      <td>True</td>\n",
       "      <td>paperback</td>\n",
       "      <td>...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>324.0</td>\n",
       "      <td>23328</td>\n",
       "      <td>2269402</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  goodreads_id_clean                     authors_list  \\\n",
       "0            2767052              ['suzanne collins']   \n",
       "1                  2  ['jk rowling', 'mary grandpre']   \n",
       "2               2657                   ['harper lee']   \n",
       "\n",
       "                author_clean                                title_clean  \\\n",
       "0            suzanne collins                           the hunger games   \n",
       "1  jk rowling, mary grandpre  harry potter and the order of the phoenix   \n",
       "2                 harper lee                      to kill a mockingbird   \n",
       "\n",
       "      isbn_clean language_clean publication_date_clean publisher_clean  \\\n",
       "0  9780439023481             en             2008-09-14      scholastic   \n",
       "1  9780439358071             en             2003-06-21      scholastic   \n",
       "2           <NA>             en             2007-07-11   harpercollins   \n",
       "\n",
       "   is_major_publisher bookFormat_clean  ...  \\\n",
       "0                True        hardcover  ...   \n",
       "1                True        paperback  ...   \n",
       "2                True        paperback  ...   \n",
       "\n",
       "                                   description_clean  \\\n",
       "0  winning means fame and fortunelosing means cer...   \n",
       "1  there is a door at the end of a silent corrido...   \n",
       "2  the unforgettable novel of a childhood in a sl...   \n",
       "\n",
       "                                     description_nlp           series_clean  \\\n",
       "0  winning means fame and fortunelosing means cer...       the hunger games   \n",
       "1  there is a door at the end of a silent corrido...           harry potter   \n",
       "2  the unforgettable novel of a childhood in a sl...  to kill a mockingbird   \n",
       "\n",
       "   pages_clean  bbeVotes_clean  bbeScore_clean  likedPercent_clean  \\\n",
       "0        374.0           30516         2993816                96.0   \n",
       "1        870.0           26923         2632233                98.0   \n",
       "2        324.0           23328         2269402                95.0   \n",
       "\n",
       "   has_likedPercent  price_clean  price_flag  \n",
       "0                 1         5.09       False  \n",
       "1                 1         7.38       False  \n",
       "2                 1          NaN        True  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_supply_catalog = bbe_clean[ bbe_clean[\"language_clean\"] == \"en\" ]\n",
    "\n",
    "# check shape after filtering\n",
    "print(f\"Original BBE dataset shape: {bbe_clean.shape}\")\n",
    "print(f\"English-only BBE dataset shape: {en_supply_catalog.shape}\")\n",
    "print(f\"Books filtered out (non-English): {bbe_clean.shape[0] - en_supply_catalog.shape[0]}\")\n",
    "print(f\"English books percentage: {(en_supply_catalog.shape[0] / bbe_clean.shape[0]) * 100:.1f}%\")\n",
    "\n",
    "# display sample\n",
    "print(\"\\nSample of English-only BBE dataset:\")\n",
    "display(en_supply_catalog.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8891a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fix Missing Text Analysis for Supply Catalog ===\n",
      "description_clean:\n",
      "  Total rows: 42634\n",
      "  Missing (NaN): 33879 (79.46%)\n",
      "  Empty strings: 0\n",
      "  Whitespace-only: 0\n",
      "----------------------------------------\n",
      "description_nlp:\n",
      "  Total rows: 42634\n",
      "  Missing (NaN): 33879 (79.46%)\n",
      "  Empty strings: 0\n",
      "  Whitespace-only: 0\n",
      "----------------------------------------\n",
      "Added publication_year and publication_decade to en_supply_catalog.\n",
      "Overlap books in supply: 7834 / 42634\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# ensure en_supply_catalog is a copy to avoid SettingWithCopyWarning\n",
    "en_supply_catalog = en_supply_catalog.copy()\n",
    "\n",
    "# apply fix_missing_text and astype directly (not with .loc)\n",
    "for col in ['description_clean', 'description_nlp']:\n",
    "    if col in en_supply_catalog.columns:\n",
    "        en_supply_catalog[col] = fix_missing_text(col)\n",
    "        en_supply_catalog[col] = en_supply_catalog[col].astype(\"string\")\n",
    "\n",
    "# report missing text analysis for supply catalog\n",
    "print(\"=== Fix Missing Text Analysis for Supply Catalog ===\")\n",
    "for col in ['description_clean', 'description_nlp']:\n",
    "    if col in en_supply_catalog.columns:\n",
    "        total = len(en_supply_catalog)\n",
    "        missing = en_supply_catalog[col].isna().sum()\n",
    "        empty = (en_supply_catalog[col] == '').sum()\n",
    "        whitespace = en_supply_catalog[col].str.strip().eq('').sum()\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Total rows: {total}\")\n",
    "        print(f\"  Missing (NaN): {missing} ({missing/total:.2%})\")\n",
    "        print(f\"  Empty strings: {empty}\")\n",
    "        print(f\"  Whitespace-only: {whitespace}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Add publication_decade to supply/BBE catalog\n",
    "i# Add publication_year and publication_decade to en_supply_catalog\n",
    "\n",
    "if 'publication_date_clean' in en_supply_catalog.columns:\n",
    "    en_supply_catalog['publication_year'] = pd.to_datetime(\n",
    "        en_supply_catalog['publication_date_clean'], errors='coerce'\n",
    "    ).dt.year\n",
    "    en_supply_catalog['publication_decade'] = (en_supply_catalog['publication_year'] // 10) * 10\n",
    "    print(\"Added publication_year and publication_decade to en_supply_catalog.\")\n",
    "else:\n",
    "    print(\"publication_date_clean column not found in en_supply_catalog.\")\n",
    "\n",
    "# Add is_overlap column\n",
    "en_supply_catalog['is_overlap'] = en_supply_catalog['goodreads_id_clean'].isin(gb_enriched_en['goodreads_id_clean']).astype(bool)\n",
    "print(f\"Overlap books in supply: {en_supply_catalog['is_overlap'].sum()} / {len(en_supply_catalog)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dd469b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL DATA QUALITY CHECKS - en_supply_catalog\n",
      "============================================================\n",
      "\n",
      "1. DUPLICATE CHECK\n",
      "Total rows: 42634\n",
      "Duplicate goodreads_id_clean: 0\n",
      "\n",
      "2. LANGUAGE CONSISTENCY CHECK\n",
      "Unique languages: ['en']\n",
      "\n",
      "3. METADATA COVERAGE\n",
      "Pages: 96.3% coverage\n",
      "Publication Date: 99.4% coverage\n",
      "Publisher: 94.3% coverage\n",
      "Genres: 100.0% coverage\n",
      "Description: 20.5% coverage\n",
      "\n",
      "Final BBE dataset shape: (42634, 39)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL DATA QUALITY CHECKS - en_supply_catalog\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# check for duplicates\n",
    "print(\"\\n1. DUPLICATE CHECK\")\n",
    "print(f\"Total rows: {len(en_supply_catalog)}\")\n",
    "duplicates_bbe = en_supply_catalog.duplicated(subset=['goodreads_id_clean']).sum()\n",
    "print(f\"Duplicate goodreads_id_clean: {duplicates_bbe}\")\n",
    "\n",
    "# verify language filter\n",
    "print(\"\\n2. LANGUAGE CONSISTENCY CHECK\")\n",
    "unique_langs_bbe = en_supply_catalog['language_clean'].unique()\n",
    "print(f\"Unique languages: {unique_langs_bbe}\")\n",
    "\n",
    "# metadata coverage\n",
    "print(\"\\n3. METADATA COVERAGE\")\n",
    "for col, label in metadata_cols.items():\n",
    "    if col in en_supply_catalog.columns:\n",
    "        coverage = en_supply_catalog[col].notna().sum() / len(en_supply_catalog) * 100\n",
    "        print(f\"{label}: {coverage:.1f}% coverage\")\n",
    "\n",
    "print(f\"\\nFinal BBE dataset shape: {en_supply_catalog.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d447fbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_supply_catalog saved successfully in outputs\\datasets\\cleaned directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'en_supply_catalog'\n",
    "clean_path = Path(\"outputs/datasets/cleaned\")\n",
    "clean_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "en_supply_catalog.to_csv(clean_path / f\"{file_name}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} saved successfully in {clean_path} directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ef1cf",
   "metadata": {},
   "source": [
    "# Dataset Merge Strategy\n",
    "We merge Goodbooks (internal catalog + ratings) with BBE (external supply catalog) on goodreads_id_clean to create two modeling variants:\n",
    "\n",
    "## Warm Start Dataset\n",
    "Includes external BBE signals (ratings, votes, liked %) for **cross-platform validation**:\n",
    "\n",
    "- Do books popular on BBE also perform well on Goodbooks?\n",
    "- Useful for analyzing rating transfer patterns across platforms\n",
    "\n",
    "## Cold Start Dataset\n",
    "Excludes all external behavioral features to prevent leakage:\n",
    "\n",
    "- Trains only on intrinsic book metadata (genre, author, publisher, etc.)\n",
    "- Simulates **new book scenarios** where external platform data is unavailable\n",
    "- Production-ready for fair model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5588c3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goodreads_id_clean'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create renaming dictionaries for merging datasets\n",
    "rename_gb = {\n",
    "    'book_id': 'gb_book_id',\n",
    "    'work_text_reviews_count': 'gb_work_text_reviews_count',\n",
    "    'ratings_1': 'gb_ratings_1',\n",
    "    'ratings_2': 'gb_ratings_2',\n",
    "    'ratings_3': 'gb_ratings_3',\n",
    "    'ratings_4': 'gb_ratings_4',\n",
    "    'ratings_5': 'gb_ratings_5',\n",
    "\n",
    "    # KEEP MERGE KEYS AS IS:\n",
    "    # 'goodreads_id_clean': 'goodreads_id_clean',\n",
    "\n",
    "    'best_book_id_clean': 'gb_best_book_id_clean',\n",
    "    'work_id_clean': 'gb_work_id_clean',\n",
    "    'authors_list': 'gb_authors_list',\n",
    "    'author_clean': 'gb_author_clean',\n",
    "    'language_clean': 'gb_language_clean',\n",
    "    'publication_date_clean': 'gb_publication_date_clean',\n",
    "    'isbn_clean': 'gb_isbn_clean',\n",
    "    'rating_clean': 'gb_rating_clean',                  # TARGET\n",
    "    'numRatings_clean': 'gb_numRatings_clean',\n",
    "    'numRatings_log': 'gb_numRatings_log',\n",
    "    'ratings_1_share': 'gb_ratings_1_share',\n",
    "    'ratings_2_share': 'gb_ratings_2_share',\n",
    "    'ratings_3_share': 'gb_ratings_3_share',\n",
    "    'ratings_4_share': 'gb_ratings_4_share',\n",
    "    'ratings_5_share': 'gb_ratings_5_share',\n",
    "    'work_text_reviews_log': 'gb_work_text_reviews_log',\n",
    "    'series_clean': 'gb_series_clean',\n",
    "    'title_clean': 'gb_title_clean',\n",
    "    'pages_clean': 'gb_pages_clean',\n",
    "    'genres_clean': 'gb_genres_clean',\n",
    "    'genres_simplified': 'gb_genres_simplified',\n",
    "    'publisher_clean': 'gb_publisher_clean',\n",
    "    'is_major_publisher': 'gb_is_major_publisher',\n",
    "    'has_award': 'gb_has_award',\n",
    "    'description_clean': 'gb_description_clean',\n",
    "    'description_nlp': 'gb_description_nlp'\n",
    "}\n",
    "\n",
    "rename_bbe = {\n",
    "    # shared key remains untouched\n",
    "    # 'goodreads_id_clean': 'goodreads_id_clean'\n",
    "\n",
    "    'authors_list': 'bbe_authors_list',\n",
    "    'author_clean': 'bbe_author_clean',\n",
    "    'title_clean': 'bbe_title_clean',\n",
    "    'isbn_clean': 'bbe_isbn_clean',\n",
    "    'language_clean': 'bbe_language_clean',\n",
    "    'publication_date_clean': 'bbe_publication_date_clean',\n",
    "    'publisher_clean': 'bbe_publisher_clean',\n",
    "    'is_major_publisher': 'bbe_is_major_publisher',\n",
    "    'bookFormat_clean': 'bbe_bookFormat_clean',\n",
    "\n",
    "    'rating_clean': 'bbe_rating_clean',  # EXTERNAL RATING (predictive feature for model 1)\n",
    "    'numRatings_clean': 'bbe_numRatings_clean',\n",
    "    'numRatings_log': 'bbe_numRatings_log',\n",
    "\n",
    "    'ratings_1': 'bbe_ratings_1',\n",
    "    'ratings_2': 'bbe_ratings_2',\n",
    "    'ratings_3': 'bbe_ratings_3',\n",
    "    'ratings_4': 'bbe_ratings_4',\n",
    "    'ratings_5': 'bbe_ratings_5',\n",
    "\n",
    "    'ratings_1_share': 'bbe_ratings_1_share',\n",
    "    'ratings_2_share': 'bbe_ratings_2_share',\n",
    "    'ratings_3_share': 'bbe_ratings_3_share',\n",
    "    'ratings_4_share': 'bbe_ratings_4_share',\n",
    "    'ratings_5_share': 'bbe_ratings_5_share',\n",
    "\n",
    "    'has_award': 'bbe_has_award',\n",
    "    'genres_clean': 'bbe_genres_clean',\n",
    "    'genres_simplified': 'bbe_genres_simplified',\n",
    "    'description_clean': 'bbe_description_clean',\n",
    "    'description_nlp': 'bbe_description_nlp',\n",
    "    'series_clean': 'bbe_series_clean',\n",
    "    'pages_clean': 'bbe_pages_clean',\n",
    "\n",
    "    'bbeVotes_clean': 'bbe_votes_clean',\n",
    "    'bbeScore_clean': 'bbe_score_clean',\n",
    "    'likedPercent_clean': 'bbe_likedPercent_clean',\n",
    "    'has_likedPercent': 'bbe_has_likedPercent',\n",
    "    'price_clean': 'bbe_price_clean',\n",
    "    'price_flag': 'bbe_price_flag'\n",
    "}\n",
    "\n",
    "# apply renaming\n",
    "internal_catalog = gb_enriched_en.rename(columns=rename_gb)\n",
    "supply = en_supply_catalog.rename(columns=rename_bbe)\n",
    "\n",
    "set(internal_catalog.columns).intersection(set(supply.columns))\n",
    "# should return {'goodreads_id_clean'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42a74a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (9761, 73)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gb_book_id</th>\n",
       "      <th>gb_work_text_reviews_count</th>\n",
       "      <th>gb_ratings_1</th>\n",
       "      <th>gb_ratings_2</th>\n",
       "      <th>gb_ratings_3</th>\n",
       "      <th>gb_ratings_4</th>\n",
       "      <th>gb_ratings_5</th>\n",
       "      <th>goodreads_id_clean</th>\n",
       "      <th>gb_best_book_id_clean</th>\n",
       "      <th>gb_work_id_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>bbe_pages_clean</th>\n",
       "      <th>bbe_votes_clean</th>\n",
       "      <th>bbe_score_clean</th>\n",
       "      <th>bbe_likedPercent_clean</th>\n",
       "      <th>bbe_has_likedPercent</th>\n",
       "      <th>bbe_price_clean</th>\n",
       "      <th>bbe_price_flag</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publication_decade</th>\n",
       "      <th>is_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>155254</td>\n",
       "      <td>66715</td>\n",
       "      <td>127936</td>\n",
       "      <td>560092</td>\n",
       "      <td>1481305</td>\n",
       "      <td>2706317</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2792775</td>\n",
       "      <td>...</td>\n",
       "      <td>374.0</td>\n",
       "      <td>30516.0</td>\n",
       "      <td>2993816.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.09</td>\n",
       "      <td>False</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>75867</td>\n",
       "      <td>75504</td>\n",
       "      <td>101676</td>\n",
       "      <td>455024</td>\n",
       "      <td>1156318</td>\n",
       "      <td>3011543</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4640799</td>\n",
       "      <td>...</td>\n",
       "      <td>309.0</td>\n",
       "      <td>7348.0</td>\n",
       "      <td>691430.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>95009</td>\n",
       "      <td>456191</td>\n",
       "      <td>436802</td>\n",
       "      <td>793319</td>\n",
       "      <td>875073</td>\n",
       "      <td>1355439</td>\n",
       "      <td>41865</td>\n",
       "      <td>41865</td>\n",
       "      <td>3212258</td>\n",
       "      <td>...</td>\n",
       "      <td>501.0</td>\n",
       "      <td>14874.0</td>\n",
       "      <td>1459448.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>False</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>72586</td>\n",
       "      <td>60427</td>\n",
       "      <td>117415</td>\n",
       "      <td>446835</td>\n",
       "      <td>1001952</td>\n",
       "      <td>1714267</td>\n",
       "      <td>2657</td>\n",
       "      <td>2657</td>\n",
       "      <td>3275794</td>\n",
       "      <td>...</td>\n",
       "      <td>324.0</td>\n",
       "      <td>23328.0</td>\n",
       "      <td>2269402.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>51992</td>\n",
       "      <td>86236</td>\n",
       "      <td>197621</td>\n",
       "      <td>606158</td>\n",
       "      <td>936012</td>\n",
       "      <td>947718</td>\n",
       "      <td>4671</td>\n",
       "      <td>4671</td>\n",
       "      <td>245494</td>\n",
       "      <td>...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>8142.0</td>\n",
       "      <td>755074.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gb_book_id  gb_work_text_reviews_count  gb_ratings_1  gb_ratings_2  \\\n",
       "0           1                      155254         66715        127936   \n",
       "1           2                       75867         75504        101676   \n",
       "2           3                       95009        456191        436802   \n",
       "3           4                       72586         60427        117415   \n",
       "4           5                       51992         86236        197621   \n",
       "\n",
       "   gb_ratings_3  gb_ratings_4  gb_ratings_5 goodreads_id_clean  \\\n",
       "0        560092       1481305       2706317            2767052   \n",
       "1        455024       1156318       3011543                  3   \n",
       "2        793319        875073       1355439              41865   \n",
       "3        446835       1001952       1714267               2657   \n",
       "4        606158        936012        947718               4671   \n",
       "\n",
       "   gb_best_book_id_clean  gb_work_id_clean  ... bbe_pages_clean  \\\n",
       "0                2767052           2792775  ...           374.0   \n",
       "1                      3           4640799  ...           309.0   \n",
       "2                  41865           3212258  ...           501.0   \n",
       "3                   2657           3275794  ...           324.0   \n",
       "4                   4671            245494  ...           200.0   \n",
       "\n",
       "  bbe_votes_clean bbe_score_clean bbe_likedPercent_clean bbe_has_likedPercent  \\\n",
       "0         30516.0       2993816.0                   96.0                  1.0   \n",
       "1          7348.0        691430.0                   96.0                  1.0   \n",
       "2         14874.0       1459448.0                   78.0                  1.0   \n",
       "3         23328.0       2269402.0                   95.0                  1.0   \n",
       "4          8142.0        755074.0                   90.0                  1.0   \n",
       "\n",
       "  bbe_price_clean  bbe_price_flag  publication_year  publication_decade  \\\n",
       "0            5.09           False            2008.0              2000.0   \n",
       "1             NaN            True            1997.0              1990.0   \n",
       "2            2.10           False            2005.0              2000.0   \n",
       "3             NaN            True            2007.0              2000.0   \n",
       "4             NaN            True            2004.0              2000.0   \n",
       "\n",
       "   is_overlap  \n",
       "0        True  \n",
       "1        True  \n",
       "2        True  \n",
       "3        True  \n",
       "4        True  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure merge key is string type\n",
    "internal_catalog['goodreads_id_clean'] = internal_catalog['goodreads_id_clean'].astype(str)\n",
    "supply['goodreads_id_clean'] = supply['goodreads_id_clean'].astype(str)\n",
    "# perform left merge\n",
    "merged = internal_catalog.merge(\n",
    "    supply,\n",
    "    on=\"goodreads_id_clean\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(f\"Merged dataset shape: {merged.shape}\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b98080e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidation complete.\n",
      "Final columns added: ['title_final', 'author_final', 'authors_list_final', 'language_final', 'publication_date_final', 'isbn_final', 'series_final', 'publisher_final', 'pages_final', 'genres_final', 'genres_simple_final', 'description_final', 'description_nlp_final', 'has_award_final', 'is_major_publisher_final', 'price_flag_final']\n",
      "Final dataset shape: (9761, 108)\n",
      "Dataset columns:\n",
      "['gb_book_id', 'gb_work_text_reviews_count', 'gb_ratings_1', 'gb_ratings_2', 'gb_ratings_3', 'gb_ratings_4', 'gb_ratings_5', 'goodreads_id_clean', 'gb_best_book_id_clean', 'gb_work_id_clean', 'gb_authors_list', 'gb_author_clean', 'gb_language_clean', 'gb_publication_date_clean', 'gb_isbn_clean', 'isbn_standard', 'gb_rating_clean', 'gb_numRatings_clean', 'gb_numRatings_log', 'gb_ratings_1_share', 'gb_ratings_2_share', 'gb_ratings_3_share', 'gb_ratings_4_share', 'gb_ratings_5_share', 'gb_work_text_reviews_log', 'gb_series_clean', 'gb_title_clean', 'gb_pages_clean', 'gb_genres_clean', 'gb_genres_simplified', 'gb_publisher_clean', 'gb_is_major_publisher', 'gb_has_award', 'gb_description_clean', 'gb_description_nlp', 'bbe_authors_list', 'bbe_author_clean', 'bbe_title_clean', 'bbe_isbn_clean', 'bbe_language_clean', 'bbe_publication_date_clean', 'bbe_publisher_clean', 'bbe_is_major_publisher', 'bbe_bookFormat_clean', 'bbe_rating_clean', 'bbe_numRatings_clean', 'bbe_numRatings_log', 'bbe_ratings_1', 'bbe_ratings_2', 'bbe_ratings_3', 'bbe_ratings_4', 'bbe_ratings_5', 'bbe_ratings_1_share', 'bbe_ratings_2_share', 'bbe_ratings_3_share', 'bbe_ratings_4_share', 'bbe_ratings_5_share', 'bbe_has_award', 'bbe_genres_clean', 'bbe_genres_simplified', 'bbe_description_clean', 'bbe_description_nlp', 'bbe_series_clean', 'bbe_pages_clean', 'bbe_votes_clean', 'bbe_score_clean', 'bbe_likedPercent_clean', 'bbe_has_likedPercent', 'bbe_price_clean', 'bbe_price_flag', 'publication_year', 'publication_decade', 'is_overlap', 'title_final', 'gb_primary_author', 'bbe_primary_author', 'author_final', 'authors_list_final', 'language_final', 'publication_date_final', 'isbn_final', 'series_final', 'publisher_final', 'pages_final', 'genres_final', 'genres_simple_final', 'description_final', 'description_nlp_final', 'has_award_final', 'is_major_publisher_final', 'external_price', 'price_flag_final', 'external_bookformat', 'external_rating', 'external_numratings', 'external_votes', 'external_score', 'external_likedpct', 'external_bbe_ratings_1', 'external_bbe_ratings_2', 'external_bbe_ratings_3', 'external_bbe_ratings_4', 'external_bbe_ratings_5', 'external_bbe_ratings_1_share', 'external_bbe_ratings_2_share', 'external_bbe_ratings_3_share', 'external_bbe_ratings_4_share', 'external_bbe_ratings_5_share']\n"
     ]
    }
   ],
   "source": [
    "# CONSOLIDATION BLOCK: GB + BBE -> final columns\n",
    "\n",
    "import pandas as pd\n",
    "from src.modeling.feature_engineering import extract_primary_author\n",
    "\n",
    "# 1) PUBLICATION_DATE as datetime\n",
    "# gb_publication_date_clean is object (string), convert safely\n",
    "merged['gb_publication_date_clean'] = pd.to_datetime(\n",
    "    merged['gb_publication_date_clean'], errors='coerce'\n",
    ")\n",
    "\n",
    "# bbe_publication_date_clean is object (string), convert safely\n",
    "merged['bbe_publication_date_clean'] = pd.to_datetime(\n",
    "    merged['bbe_publication_date_clean'], errors='coerce'\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Consolidate TITLE\n",
    "merged['title_final'] = merged['gb_title_clean'].combine_first(\n",
    "    merged['bbe_title_clean']\n",
    ")\n",
    "\n",
    "# 3) Consolidate AUTHOR (single author)\n",
    "merged['gb_primary_author'] = merged['gb_author_clean'].apply(extract_primary_author)\n",
    "merged['bbe_primary_author'] = merged['bbe_author_clean'].apply(extract_primary_author)\n",
    "\n",
    "\n",
    "merged['author_final'] = merged['gb_primary_author'].combine_first(\n",
    "    merged['bbe_primary_author']\n",
    ")\n",
    "# Standardize author_final to lowercase and strip whitespace\n",
    "merged['author_final'] = merged['author_final'].str.lower().str.strip()\n",
    "\n",
    "\n",
    "# 4) Consolidate AUTHORS LIST (multi-author)\n",
    "merged['authors_list_final'] = merged['gb_authors_list'].combine_first(\n",
    "    merged['bbe_authors_list']\n",
    ")\n",
    "\n",
    "\n",
    "# 5) Consolidate LANGUAGE\n",
    "merged['language_final'] = merged['gb_language_clean'].combine_first(\n",
    "    merged['bbe_language_clean']\n",
    ")\n",
    "\n",
    "\n",
    "# 6) Consolidate PUBLICATION DATE + YEAR + DECADE\n",
    "merged['publication_date_final'] = merged['gb_publication_date_clean'].combine_first(\n",
    "    merged['bbe_publication_date_clean']\n",
    ")\n",
    "\n",
    "# Derived fields:\n",
    "merged['publication_year'] = merged['publication_date_final'].dt.year\n",
    "merged['publication_decade'] = (merged['publication_year'] // 10) * 10\n",
    "\n",
    "\n",
    "# 7) Consolidate ISBN\n",
    "merged['isbn_final'] = merged['gb_isbn_clean'].combine_first(\n",
    "    merged['bbe_isbn_clean']\n",
    ")\n",
    "\n",
    "\n",
    "# 8) Consolidate SERIES\n",
    "merged['series_final'] = merged['gb_series_clean'].combine_first(\n",
    "    merged['bbe_series_clean']\n",
    ")\n",
    "\n",
    "\n",
    "# 9) Consolidate PUBLISHER\n",
    "merged['publisher_final'] = merged['gb_publisher_clean'].combine_first(\n",
    "    merged['bbe_publisher_clean']\n",
    ")\n",
    "\n",
    "\n",
    "# 10) Consolidate PAGES\n",
    "# gb_pages_clean: float\n",
    "# bbe_pages_clean: float\n",
    "merged['pages_final'] = merged['gb_pages_clean'].combine_first(\n",
    "    merged['bbe_pages_clean']\n",
    ")\n",
    "\n",
    "\n",
    "# 11) Consolidate GENRES\n",
    "# objects holding lists or strings\n",
    "merged['genres_final'] = merged['gb_genres_clean'].combine_first(\n",
    "    merged['bbe_genres_clean']\n",
    ")\n",
    "\n",
    "merged['genres_simple_final'] = merged['gb_genres_simplified'].combine_first(\n",
    "    merged['bbe_genres_simplified']\n",
    ")\n",
    "\n",
    "\n",
    "# 12) Consolidate DESCRIPTION\n",
    "# gb_description_clean: pandas string dtype\n",
    "# bbe_description_clean: object dtype\n",
    "merged['description_final'] = merged['gb_description_clean'].combine_first(\n",
    "    merged['bbe_description_clean']\n",
    ")\n",
    "\n",
    "merged['description_nlp_final'] = merged['gb_description_nlp'].combine_first(\n",
    "    merged['bbe_description_nlp']\n",
    ")\n",
    "\n",
    "\n",
    "# 13) Consolidate AWARDS\n",
    "# gb_has_award is bool, bbe_has_award is object (\"True\"/\"False\"/None)\n",
    "# normalize  award info\n",
    "merged['gb_has_award'] = (\n",
    "    merged['gb_has_award']\n",
    "        .replace({'True': True, 'False': False, '': None})\n",
    "        .astype('boolean')\n",
    ")\n",
    "\n",
    "merged['bbe_has_award'] = (\n",
    "    merged['bbe_has_award']\n",
    "        .replace({'True': True, 'False': False, '': None})\n",
    "        .astype('boolean')\n",
    ")\n",
    "\n",
    "# consolidate\n",
    "merged['has_award_final'] = merged['gb_has_award'].combine_first(\n",
    "    merged['bbe_has_award']\n",
    ")\n",
    "\n",
    "# fill any remaining NA with False (no award)\n",
    "merged['has_award_final'] = (\n",
    "    merged['has_award_final']\n",
    "        .fillna(False)\n",
    "        .astype('boolean')\n",
    ")\n",
    "\n",
    "# 14) Consolidate MAJOR PUBLISHER FLAG\n",
    "# gb_is_major_publisher: object (\"True\"/\"False\")\n",
    "# bbe_is_major_publisher: object\n",
    "merged['gb_is_major_publisher'] = merged['gb_is_major_publisher'].replace({\n",
    "    'True': True, 'False': False\n",
    "}).astype('boolean')\n",
    "\n",
    "merged['bbe_is_major_publisher'] = merged['bbe_is_major_publisher'].replace({\n",
    "    'True': True, 'False': False\n",
    "}).astype('boolean')\n",
    "\n",
    "merged['is_major_publisher_final'] = merged['gb_is_major_publisher'].combine_first(\n",
    "    merged['bbe_is_major_publisher']\n",
    ")\n",
    "\n",
    "\n",
    "# 15) Consolidate PRICE (BBE-only)\n",
    "merged['external_price'] = merged['bbe_price_clean']\n",
    "merged['price_flag_final'] = merged['bbe_price_flag']\n",
    "\n",
    "\n",
    "# 16) Consolidate BOOK FORMAT (BBE-only)\n",
    "merged['external_bookformat'] = merged['bbe_bookFormat_clean']\n",
    "\n",
    "# 17). Consolidate EXTERNAL RATINGS (BBE-only)\n",
    "merged['external_rating'] = merged['bbe_rating_clean']\n",
    "merged['external_numratings'] = merged['bbe_numRatings_clean']\n",
    "merged['external_votes'] = merged['bbe_votes_clean']\n",
    "merged['external_score'] = merged['bbe_score_clean']\n",
    "merged['external_likedpct'] = merged['bbe_likedPercent_clean']\n",
    "\n",
    "\n",
    "# 18) Consolidate EXTERNAL RATING DISTRIBUTIONS (BBE-only)\n",
    "bbe_rating_dist_cols = [\n",
    "    'bbe_ratings_1', 'bbe_ratings_2', 'bbe_ratings_3',\n",
    "    'bbe_ratings_4', 'bbe_ratings_5',\n",
    "    'bbe_ratings_1_share', 'bbe_ratings_2_share',\n",
    "    'bbe_ratings_3_share', 'bbe_ratings_4_share',\n",
    "    'bbe_ratings_5_share'\n",
    "]\n",
    "\n",
    "for col in bbe_rating_dist_cols:\n",
    "    merged[f'external_{col}'] = merged[col]\n",
    "\n",
    "print(\"Consolidation complete.\")\n",
    "print(\"Final columns added:\", [col for col in merged.columns if col.endswith('_final')])\n",
    "print(\"Final dataset shape:\", merged.shape)\n",
    "print(\"Dataset columns:\")\n",
    "print(merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c83855a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_warm = [\n",
    "\n",
    "    # INTERNAL LEAKAGE FIELDS\n",
    "    'gb_ratings_1', 'gb_ratings_2', 'gb_ratings_3',\n",
    "    'gb_ratings_4', 'gb_ratings_5',\n",
    "    'gb_ratings_1_share', 'gb_ratings_2_share',\n",
    "    'gb_ratings_3_share', 'gb_ratings_4_share',\n",
    "    'gb_ratings_5_share',\n",
    "    'gb_work_text_reviews_log',\n",
    "    'gb_work_text_reviews_count',\n",
    "\n",
    "    # REDUNDANT CONSOLIDATION INPUTS\n",
    "    'gb_title_clean', 'bbe_title_clean',\n",
    "    'gb_primary_author', 'bbe_primary_author',\n",
    "    'gb_author_clean', 'bbe_author_clean',\n",
    "    'gb_authors_list', 'bbe_authors_list',\n",
    "    'gb_language_clean', 'bbe_language_clean',\n",
    "    'gb_publication_date_clean', 'bbe_publication_date_clean',\n",
    "    'gb_series_clean', 'bbe_series_clean',\n",
    "    'gb_publisher_clean', 'bbe_publisher_clean',\n",
    "    'gb_pages_clean', 'bbe_pages_clean',\n",
    "    'gb_genres_clean', 'bbe_genres_clean',\n",
    "    'gb_genres_simplified', 'bbe_genres_simplified',\n",
    "    'gb_description_clean', 'bbe_description_clean',\n",
    "    'gb_description_nlp', 'bbe_description_nlp',\n",
    "    'gb_is_major_publisher', 'bbe_is_major_publisher',\n",
    "    'gb_has_award', 'bbe_has_award',\n",
    "    'gb_isbn_clean', 'bbe_isbn_clean',\n",
    "    \n",
    "    # REDUNDANT BBE FLAGS AND FIELDS\n",
    "    'bbe_price_clean',\n",
    "    'bbe_has_likedPercent',\n",
    "    'bbe_bookFormat_clean',\n",
    "\n",
    "    # IDENTIFIERS\n",
    "    'gb_best_book_id_clean',\n",
    "    'gb_work_id_clean',\n",
    "    'isbn_final',\n",
    "\n",
    "    # NON-PREDICTIVE CLEANING ARTIFACTS\n",
    "    'price_flag_final',\n",
    "    'bbe_price_flag', \n",
    "\n",
    "    # DUPLICATES OF external_* FEATURES → drop original BBE fields\n",
    "    'bbe_rating_clean',\n",
    "    'bbe_numRatings_clean',\n",
    "    'bbe_votes_clean',\n",
    "    'bbe_score_clean',\n",
    "    'bbe_likedPercent_clean',\n",
    "    'bbe_numRatings_log',\n",
    "    \n",
    "    # Drop original BBE rating distribution columns (now have external_* versions)\n",
    "    'bbe_ratings_1', 'bbe_ratings_2', 'bbe_ratings_3',\n",
    "    'bbe_ratings_4', 'bbe_ratings_5',\n",
    "    'bbe_ratings_1_share', 'bbe_ratings_2_share',\n",
    "    'bbe_ratings_3_share', 'bbe_ratings_4_share',\n",
    "    'bbe_ratings_5_share',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a83b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_cold = cols_to_drop_warm + [\n",
    "\n",
    "    # REMOVE engineered external_* features\n",
    "    *[col for col in merged.columns if col.startswith('external_')],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d60828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 68 columns for WARM START dataset.\n",
      "Final WARM START dataset shape: (9761, 40)\n",
      "Dropped 85 columns for COLD START dataset.\n",
      "Final COLD START dataset shape: (9761, 23)\n",
      "Columns in WARM START but not in COLD START:\n",
      "{'external_rating', 'external_score', 'external_bbe_ratings_3', 'external_bbe_ratings_1_share', 'external_bbe_ratings_2_share', 'external_numratings', 'external_votes', 'external_bookformat', 'external_bbe_ratings_5_share', 'external_price', 'external_bbe_ratings_5', 'external_bbe_ratings_3_share', 'external_bbe_ratings_2', 'external_likedpct', 'external_bbe_ratings_4', 'external_bbe_ratings_1', 'external_bbe_ratings_4_share'}\n"
     ]
    }
   ],
   "source": [
    "merged_clean_warm = merged.drop(columns=cols_to_drop_warm, errors='ignore')\n",
    "print(\"Dropped\", len(cols_to_drop_warm), \"columns for WARM START dataset.\")\n",
    "print(\"Final WARM START dataset shape:\", merged_clean_warm.shape)\n",
    "\n",
    "merged_clean_cold = merged.drop(columns=cols_to_drop_cold, errors='ignore')\n",
    "print(\"Dropped\", len(cols_to_drop_cold), \"columns for COLD START dataset.\")\n",
    "print(\"Final COLD START dataset shape:\", merged_clean_cold.shape)\n",
    "\n",
    "print(\"Columns in WARM START but not in COLD START:\")\n",
    "print(set(merged_clean_warm.columns) - set(merged_clean_cold.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c2d49",
   "metadata": {},
   "source": [
    "## Finalizing and Saving Datasets\n",
    "\n",
    "Both datasets use consolidated `_final` columns (title, author, pages, etc.) where Goodbooks takes precedence and BBE fills gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec72db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARM START dataset saved: outputs\\datasets\\cleaned\\model_dataset_warm_start.csv\n",
      "\n",
      "COLD START dataset saved: outputs\\datasets\\cleaned\\model_dataset_cold_start.csv\n",
      "\n",
      "Ratings clean dataset saved: outputs\\datasets\\cleaned\\ratings_clean.csv\n",
      "\n",
      "DATASETS SAVED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "WARM START: 9761 books, 40 features\n",
      "COLD START: 9761 books, 23 features\n",
      "\n",
      "Feature difference: 17 external signals\n",
      "\n",
      "Ratings clean dataset: 5976479 entries, 3 features\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# create output directory\n",
    "output_path = Path(\"outputs/datasets/cleaned\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save WARM START dataset (includes external BBE signals)\n",
    "warm_file = output_path / \"model_dataset_warm_start.csv\"\n",
    "merged_clean_warm.to_csv(warm_file, index=False)\n",
    "print(f\"WARM START dataset saved: {warm_file}\")\n",
    "\n",
    "# save COLD START dataset (no external signals)\n",
    "cold_file = output_path / \"model_dataset_cold_start.csv\"\n",
    "merged_clean_cold.to_csv(cold_file, index=False)\n",
    "print(f\"\\nCOLD START dataset saved: {cold_file}\")\n",
    "\n",
    "# rating clean\n",
    "ratings_clean.to_csv(output_path / \"ratings_clean.csv\", index=False)\n",
    "print(f\"\\nRatings clean dataset saved: {output_path / 'ratings_clean.csv'}\")\n",
    "\n",
    "# summary\n",
    "print(\"\\nDATASETS SAVED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWARM START: {merged_clean_warm.shape[0]} books, {merged_clean_warm.shape[1]} features\")\n",
    "print(f\"COLD START: {merged_clean_cold.shape[0]} books, {merged_clean_cold.shape[1]} features\")\n",
    "print(f\"\\nFeature difference: {merged_clean_warm.shape[1] - merged_clean_cold.shape[1]} external signals\")\n",
    "print(f\"\\nRatings clean dataset: {ratings_clean.shape[0]} entries, {ratings_clean.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa12dc",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook successfully completed **multi-source data enrichment and dataset integration** for book satisfaction modeling.\n",
    "\n",
    "### Key Results\n",
    "\n",
    "**Enrichment Performance:**\n",
    "- Multi-source strategy: BBE overlap -> OpenLibrary API -> Google Books API\n",
    "- **95.1%** page count coverage, **100%** publication dates, **99.4%** valid languages\n",
    "- **94.7%** publisher coverage, **85.2%** description coverage\n",
    "- **90.7%** genre coverage (from 0% in original Goodbooks)\n",
    "\n",
    "**Final Datasets:**\n",
    "\n",
    "| Dataset | Location | Purpose | Records |\n",
    "|---------|----------|---------|---------|\n",
    "| `en_supply_catalog.csv` | `outputs/datasets/cleaned/` | BBE supply catalog | 47,452 |\n",
    "| `en_internal_catalog.csv` | `outputs/datasets/cleaned/` | Enriched Goodbooks | 9,940 |\n",
    "| `model_dataset_warm_start.csv` | `outputs/datasets/modeling/` | With external signals | 9,940 |\n",
    "| `model_dataset_cold_start.csv` | `outputs/datasets/modeling/` | Without external signals | 9,940 |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Notebook 04:** Exploratory analysis and genre diversity comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
