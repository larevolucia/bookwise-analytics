{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d6a58f",
   "metadata": {},
   "source": [
    "# Data Enrichment & Dataset Integration\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The purpose of this notebook is to **enrich, align, and integrate the cleaned datasets** to create a unified analytical foundation for modelling book satisfaction and evaluating catalogue diversity.\n",
    "\n",
    "This notebook expands upon prior cleaning work by **adding missing metadata, linking overlapping records across datasets, filtering the dataset to English-language titles, and preparing a model-ready dataset** that combines catalog-level information (BBE) with user-behavioral data (Goodbooks).\n",
    "\n",
    "Ultimately, this notebook enables insights that neither dataset could provide independently, most critically, **genre diversity analysis**, **language-based consistency**, **metadata-enhanced prediction modeling**.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "| Dataset                             | Source                     | Description                                                                                         | Format |\n",
    "| ----------------------------------- | -------------------------- | --------------------------------------------------------------------------------------------------- | ------ |\n",
    "| `bbe_clean_v13.csv`                  | Output from Notebook 02    | Cleaned *Best Books Ever* metadata including title, authors, genres, rating, description, and more. | CSV    |\n",
    "| `books_clean_v7.csv`      | Output from Notebook 02    | Cleaned Goodbooks-10k metadata lacking genre data but containing structural identifiers.            | CSV    |\n",
    "| `ratings_clean_v1.csv`    | Output from Notebook 02    | User–book interaction and aggregated rating data for behavioral modeling.                           | CSV    |\n",
    "| *(Optional)* External API responses | OpenLibrary / Google Books | Supplemental metadata (genres, languages, subjects) for non-overlapping titles.                     | JSON   |\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks in This Notebook\n",
    "\n",
    "This notebook will execute the following enrichment and integration steps:\n",
    "\n",
    "1. **Standardize linking identifiers**\n",
    "   Normalize `isbn_clean`, `goodreads_id`, `title_clean`, and `author_clean` across datasets to ensure reliable cross-dataset merging.\n",
    "\n",
    "2. **Identify overlap between BBE and Goodbooks**\n",
    "   Detect books present in both datasets using multi-key matching and evaluate match quality.\n",
    "\n",
    "3. **Enrich Goodbooks metadata with missing genres**\n",
    "\n",
    "   * Use BBE genre fields for overlapping titles.\n",
    "   * Query external APIs for non-overlapping titles.\n",
    "   * Normalize all genre outputs into a unified taxonomy.\n",
    "\n",
    "4. **Complete and standardize language metadata**\n",
    "   Fill missing values using BBE, APIs, or text-based heuristics, then harmonize language labels and codes.\n",
    "\n",
    "5. **Filter the enriched datasets to English-language books**\n",
    "   Restrict the unified dataset to titles identified as **English-language**, ensuring consistency for:\n",
    "\n",
    "   * genre diversity comparisons\n",
    "   * ratings behavior\n",
    "   * regression modeling\n",
    "\n",
    "   *(Non-English titles will be kept only in the enriched BBE/Goodbooks outputs, but excluded from the model dataset.)*\n",
    "\n",
    "6. **Integrate datasets into a model-ready schemas**\n",
    "   Combine BBE metadata with Goodbooks behavioral features for all overlapping **English-language** books.\n",
    "\n",
    "7. **Validate enrichment and filtering results**\n",
    "\n",
    "   * Assess metadata fields fill rates\n",
    "   * Review API match and success metrics\n",
    "   * Log all imputation and filtering decisions for reproducibility\n",
    "\n",
    "8. **Export enriched and unified datasets**\n",
    "   Produce final English-filtered datasets ready for modeling and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **en_supply_catalog.csv** — enriched metadata for all BBE books, representing the supply catalog\n",
    "* **en_internal_catalog.csv** — enriched metadata for all Goodbooks books, representing the internal catalog\n",
    "* **model_dataset_warm_start.csv** — unified metadata + behavioral dataset filtered to English-language books. Includes external BBE signals for cross platform validation.\n",
    "* **model_dataset_cold_start.csv** — unified metadata + behavioral dataset filtered to English-language books. Excludes external BBE signals for pure internal modeling.\n",
    "* **Enrichment and filtering logs** — documenting imputation sources, API usage, and filtering decisions\n",
    "\n",
    "> **Note:** This notebook focuses on **metadata enrichment, English-language filtering, and dataset integration**. Model development and feature engineering will be performed in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09815b66",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "## Navigate to the Parent Directory\n",
    "\n",
    "Before combining and saving datasets, it’s often helpful to move to a parent directory so that file operations (like loading or saving data) are easier and more organized. \n",
    "\n",
    "Before using the Python’s built-in os module to move one level up from the current working directory, it is advisable to inspect the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f'Current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2117d",
   "metadata": {},
   "source": [
    "To change to parent directory (root folder), run the code below. If you are already in the root folder, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to its parent\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print('Changed directory to parent.')\n",
    "\n",
    "# Get the new current working directory (the parent directory)\n",
    "current_dir = os.getcwd()\n",
    "print(f'New current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cleaning.utils.categories import (\n",
    "    map_subjects_to_genres\n",
    ")\n",
    "from src.cleaning.utils.pipeline import apply_cleaners_selectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b908b97",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install additional packages for this notebook\n",
    "! pip install requests python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4cc03",
   "metadata": {},
   "source": [
    "## Load and Inspect Datasets\n",
    "\n",
    "In this step, we load the previously cleaned datasets: **Goodbooks-10k** (books, ratings) and **Best Books Ever**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load datasets\n",
    "books_clean = pd.read_csv(\n",
    "    'data/interim/goodbooks/books_clean_v7.csv',\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    "    )\n",
    "ratings_clean = pd.read_csv('data/interim/goodbooks/ratings_clean_v1.csv')\n",
    "bbe_clean = pd.read_csv(\n",
    "    \"data/interim/bbe/bbe_clean_v13.csv\",\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    ")\n",
    "\n",
    "# create copies for imputation\n",
    "books_impute = books_clean.copy()\n",
    "ratings_impute = ratings_clean.copy()\n",
    "bbe_impute = bbe_clean.copy()\n",
    "\n",
    "# log samples\n",
    "print(\"BBE dataset columns:\")\n",
    "print(bbe_impute.columns.tolist())\n",
    "print(\"BBE dataset info:\")\n",
    "display(bbe_impute.info())\n",
    "print(\"BBE dataset sample:\")\n",
    "display(bbe_impute.head(3))\n",
    "\n",
    "print(\"Books dataset columns:\")\n",
    "print(books_impute.columns.tolist())\n",
    "print(\"Books dataset info:\")\n",
    "display(books_impute.info())\n",
    "print(\"Books dataset sample:\")\n",
    "display(books_impute.head(3))\n",
    "\n",
    "print(\"Ratings dataset columns:\")\n",
    "print(ratings_impute.columns.tolist())\n",
    "print(\"Ratings dataset info:\")\n",
    "display(ratings_impute.info())\n",
    "print(\"Ratings dataset sample:\")\n",
    "display(ratings_impute.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cff995",
   "metadata": {},
   "source": [
    "# Data Enrichment\n",
    "\n",
    "## Enriching Goodbooks\n",
    "\n",
    "### From BBE overlap\n",
    "\n",
    "To improve the completeness and quality of the Goodbooks-10k dataset, we selectively merge in metadata from the Best Books Ever (BBE) dataset using the shared `goodreads_id_clean` key. Goodbooks is kept as the primary source, while BBE is used to supply additional metadata fields, such as genres and page counts, as well as to fill in missing values for shared attributes like ISBN, publication date, and series.\n",
    "\n",
    "This approach ensures we enhance Goodbooks only where necessary: adding new information where it is absent and completing incomplete entries without overwriting existing data. The resulting `gb_enriched` dataset combines both sources into a more reliable and feature-rich foundation for downstream analytics and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# ENRICH GOODBOOKS (books_impute) WITH BBE DATA\n",
    "# ---------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# columns to enrich ONLY when GB has NaN\n",
    "columns_to_enrich = [\n",
    "    \"publication_date_clean\",\n",
    "    \"series_clean\",\n",
    "    \"isbn_clean\",\n",
    "    \"language_clean\"\n",
    "    ]\n",
    "\n",
    "# columns existent only in BBE\n",
    "bbe_only_columns = [\n",
    "    \"pages_clean\",\n",
    "    \"genres_clean\",\n",
    "    \"genres_simplified\",\n",
    "    \"publisher_clean\",\n",
    "    \"is_major_publisher\",\n",
    "    \"has_award\",\n",
    "    \"description_clean\",\n",
    "    \"description_nlp\"\n",
    "]\n",
    "\n",
    "# merge Goodbooks with the needed BBE columns\n",
    "merge_cols = [\"goodreads_id_clean\"] + columns_to_enrich + bbe_only_columns\n",
    "\n",
    "gb_enriched = books_impute.merge(\n",
    "    bbe_impute[merge_cols].add_suffix(\"_bbe\"),\n",
    "    left_on=\"goodreads_id_clean\",\n",
    "    right_on=\"goodreads_id_clean_bbe\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ENRICH GENRE COLUMNS\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- ENRICHING METADATA ---\")\n",
    "for col in bbe_only_columns:\n",
    "    gb_enriched[col] = gb_enriched[col + \"_bbe\"]\n",
    "    filled = gb_enriched[col].notna().sum()\n",
    "    print(f\"{col}: filled {filled} rows from BBE\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ENRICH SHARED COLUMNS ONLY WHERE GB IS NaN\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- ENRICHING SHARED COLUMNS (GB NaN -> fill from BBE) ---\")\n",
    "for col in columns_to_enrich:\n",
    "    before = gb_enriched[col].isna().sum()\n",
    "    gb_enriched[col] = gb_enriched[col].fillna(gb_enriched[col + \"_bbe\"])\n",
    "    after = gb_enriched[col].isna().sum()\n",
    "    print(f\"{col}: filled {before - after} missing values\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# CLEANUP\n",
    "# ---------------------------------------------\n",
    "gb_enriched = gb_enriched.drop(columns=[c for c in gb_enriched.columns if c.endswith(\"_bbe\")])\n",
    "\n",
    "print(\"\\nEnrichment complete!\")\n",
    "print(\"Final shape:\", gb_enriched.shape)\n",
    "gb_enriched[['isbn_clean','title_clean', 'series_clean', 'genres_clean', 'genres_simplified', 'pages_clean', 'publication_date_clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d474378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 1\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ead17c",
   "metadata": {},
   "source": [
    "### From external APIs\n",
    "\n",
    "To further enrich the Goodbooks-10k dataset, we leverage external APIs such as OpenLibrary and Google Books to fill in missing metadata for titles not covered by the BBE overlap. This process involves querying these APIs using available identifiers (like ISBN or title/author combinations) to retrieve additional information such as genres, page counts, and publication details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_isbn(isbn):\n",
    "    if not isinstance(isbn, str):\n",
    "        return None\n",
    "    isbn = re.sub(r'[^0-9Xx]', '', isbn)\n",
    "    if len(isbn) in [10, 13]:\n",
    "        return isbn\n",
    "    return None\n",
    "\n",
    "gb_enriched['isbn_query'] = gb_enriched['isbn_clean'].apply(clean_isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee431a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mask = (\n",
    "    gb_enriched['language_clean'].isna() |\n",
    "    gb_enriched['language_clean'].isin(['unknown', '', 'None']) |\n",
    "    gb_enriched['pages_clean'].isna() |\n",
    "    gb_enriched['publication_date_clean'].isna()  |\n",
    "    gb_enriched['publisher_clean'].isna() |\n",
    "    gb_enriched['description_clean'].isna()\n",
    ")\n",
    "\n",
    "to_impute = gb_enriched[missing_mask].copy()\n",
    "print(\"Books needing external enrichment:\", len(to_impute))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ab95c",
   "metadata": {},
   "source": [
    "#### Querying OpenLibrary API\n",
    "\n",
    "After enriching Goodbooks with BBE overlap data, we identify **2,249** books still missing critical metadata (ISBN, language, pages, publication date, publisher). We query **OpenLibrary first** because it has no rate limits or API key requirements, making it ideal for bulk enrichment. We create a boolean mask to identify books needing enrichment, then query OpenLibrary's ISBN endpoint for each book, collecting results in a structured format.\n",
    "\n",
    "The results are merged back into `gb_enriched` and saved as **version 2**. This incremental saving strategy ensures we don't lose progress if subsequent API calls fail or exceed quotas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b963e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# cache path for OpenLibrary in data/raw\n",
    "OL_CACHE_PATH = Path(\"data/raw/openlibrary_api_cache.json\")\n",
    "\n",
    "# create directory if it doesn't exist\n",
    "OL_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load existing cache if it exists\n",
    "if OL_CACHE_PATH.exists():\n",
    "    with open(OL_CACHE_PATH, \"r\") as f:\n",
    "        ol_cache = json.load(f)\n",
    "    print(f\"Loaded {len(ol_cache)} cached OpenLibrary entries\")\n",
    "else:\n",
    "    ol_cache = {}\n",
    "    print(\"No existing cache found, starting fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5278cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def query_openlibrary(isbn):\n",
    "    \"\"\"Return OL metadata in a consistent dict format.\"\"\"\n",
    "\n",
    "    isbn_str = str(isbn)\n",
    "    \n",
    "    if isbn_str in ol_cache:\n",
    "        return ol_cache[isbn_str]\n",
    "    \n",
    "    # Default structure to guarantee stable DataFrame columns\n",
    "    result = {\n",
    "        \"pages_openlib\": None,\n",
    "        \"publication_date_openlib\": None,\n",
    "        \"language_openlib\": None,\n",
    "        \"subjects_openlib\": None,\n",
    "        \"publisher_openlib\": None, \n",
    "        \"description_openlib\": None, \n",
    "    }\n",
    "\n",
    "    if isbn is None or pd.isna(isbn) or isbn == \"\":\n",
    "        return result\n",
    "    \n",
    "    url = f\"https://openlibrary.org/isbn/{isbn}.json\"\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return result\n",
    "\n",
    "        data = r.json()\n",
    "\n",
    "        # Pages\n",
    "        result[\"pages_openlib\"] = data.get(\"number_of_pages\")\n",
    "\n",
    "        # Publication date\n",
    "        result[\"publication_date_openlib\"] = data.get(\"publish_date\")\n",
    "\n",
    "        # Language\n",
    "        if \"languages\" in data and isinstance(data[\"languages\"], list):\n",
    "            key = data[\"languages\"][0].get(\"key\", \"\").split(\"/\")[-1]\n",
    "            result[\"language_openlib\"] = key\n",
    "\n",
    "        # Subjects\n",
    "        if \"subjects\" in data:\n",
    "            result[\"subjects_openlib\"] = [s.lower() for s in data[\"subjects\"]]\n",
    "        \n",
    "        # Publisher\n",
    "        if \"publishers\" in data and isinstance(data[\"publishers\"], list):\n",
    "            result[\"publisher_openlib\"] = data[\"publishers\"][0]\n",
    "        \n",
    "        # Description\n",
    "        desc = data.get(\"description\")\n",
    "        if isinstance(desc, dict):\n",
    "            result[\"description_openlib\"] = desc.get(\"value\")\n",
    "        elif isinstance(desc, str):\n",
    "            result[\"description_openlib\"] = desc\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        pass  # keep the default result structure\n",
    "\n",
    "    # Save to cache\n",
    "    ol_cache[isbn_str] = result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for isbn in tqdm(to_impute['isbn_query'], desc=\"Querying OpenLibrary\"):\n",
    "    results.append(query_openlibrary(isbn))\n",
    "    time.sleep(0.2)   # safe rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Save OpenLibrary cache after queries\n",
    "with open(OL_CACHE_PATH, \"w\") as f:\n",
    "    json.dump(ol_cache, f, indent=2)\n",
    "print(f\"OpenLibrary cache saved with {len(ol_cache)} entries\")\n",
    "\n",
    "# convert results to dataframe\n",
    "ol_df = pd.DataFrame(results, index=to_impute.index)\n",
    "print(\"API results summary:\")\n",
    "print(ol_df.notna().sum())\n",
    "\n",
    "# merge back into gb_enriched\n",
    "for col in ol_df.columns:\n",
    "    if col not in gb_enriched.columns:\n",
    "        gb_enriched[col] = None\n",
    "    gb_enriched.loc[ol_df.index, col] = ol_df[col]\n",
    "\n",
    "# verify the merge\n",
    "print(\"\\nAfter merge:\")\n",
    "print(gb_enriched[ol_df.columns].notna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 2\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a12d1a",
   "metadata": {},
   "source": [
    "#### Cleaning and Processing OpenLibrary Data\n",
    "\n",
    "We apply the same cleaning steps used in Notebook 02, compiled into a pipeline, to standardize OpenLibrary API responses. The `apply_cleaners_selectively()` function ensures consistent data types, formats, and validation across all metadata fields. After cleaning, we fill missing values in `gb_enriched` using the cleaned OpenLibrary data.\n",
    "\n",
    "For genre enrichment, we map OpenLibrary subjects to our standardized genre taxonomy using `map_subjects_to_genres()`. This populates `genres_simplified` for books that had subjects but no genre data, significantly improving genre coverage. The enriched dataset is saved as **version 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a19072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean OpenLibrary API data\n",
    "gb_enriched = apply_cleaners_selectively(\n",
    "    gb_enriched,\n",
    "    fields_to_clean=[\n",
    "        'pages',\n",
    "        'publication_date',\n",
    "        'language',\n",
    "        'subjects',\n",
    "        'publisher',\n",
    "        'description'\n",
    "        ],\n",
    "    source_suffix='_openlib',\n",
    "    target_suffix='_openlib_clean',\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# verify cleaning\n",
    "print(\"\\nSample of cleaned OpenLibrary data:\")\n",
    "gb_enriched[[\n",
    "    'title_clean',\n",
    "    'pages_clean',\n",
    "    'pages_openlib',\n",
    "    'pages_openlib_clean',\n",
    "    'publication_date_clean',\n",
    "    'publication_date_openlib',\n",
    "    'publication_date_openlib_clean',\n",
    "    'language_clean',\n",
    "    'language_openlib',\n",
    "    'language_openlib_clean',\n",
    "    'genres_clean',\n",
    "    'genres_simplified',\n",
    "    'subjects_openlib',\n",
    "    'subjects_openlib_clean',\n",
    "    'publisher_clean',\n",
    "    'description_openlib',\n",
    "    'description_clean',\n",
    "    'description_openlib',\n",
    "    'description_openlib_clean'\n",
    "    ]].sample(15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with cleaned OpenLibrary data\n",
    "print(\"\\n--- Filling missing values with cleaned OpenLibrary data ---\")\n",
    "\n",
    "# fill pages_clean\n",
    "before_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "gb_enriched['pages_clean'] = gb_enriched['pages_clean'].fillna(gb_enriched['pages_openlib_clean'])\n",
    "after_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "print(f\"pages_clean: filled {before_pages - after_pages} values\")\n",
    "\n",
    "# fill publication_date_clean\n",
    "before_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "gb_enriched['publication_date_clean'] = gb_enriched['publication_date_clean'].fillna(gb_enriched['publication_date_openlib_clean'])\n",
    "after_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "print(f\"publication_date_clean: filled {before_date - after_date} values\")\n",
    "\n",
    "# fill language_clean\n",
    "# Create mask that catches both NaN and invalid string values\n",
    "before_lang = (gb_enriched['language_clean'].isna() | \n",
    "               gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "mask = (gb_enriched['language_clean'].isna() | \n",
    "        gb_enriched['language_clean'].isin(['unknown', '', 'None']))\n",
    "\n",
    "gb_enriched.loc[mask, 'language_clean'] = gb_enriched.loc[mask, 'language_openlib_clean']\n",
    "\n",
    "after_lang = (gb_enriched['language_clean'].isna() | \n",
    "              gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "print(f\"language_clean: filled {before_lang - after_lang} values\")\n",
    "\n",
    "# fill publisher_clean\n",
    "print(\"\\n--- Filling missing publisher_clean using OpenLibrary data ---\")\n",
    "before_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "gb_enriched['publisher_clean'] = gb_enriched['publisher_clean'].fillna(\n",
    "    gb_enriched['publisher_openlib_clean']\n",
    ")\n",
    "after_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "print(f\"publisher_clean: filled {before_publisher - after_publisher} values\")\n",
    "\n",
    "# fill description_clean\n",
    "print(\"\\n--- Filling missing description_clean using OpenLibrary data ---\")\n",
    "\n",
    "before_desc = gb_enriched['description_clean'].isna().sum()\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].fillna(\n",
    "    gb_enriched['description_openlib_clean']\n",
    ")\n",
    "after_desc = gb_enriched['description_clean'].isna().sum()\n",
    "print(f\"description_clean: filled {before_desc - after_desc} values\")\n",
    "\n",
    "# generate genres_simplified from subjects_openlib_clean\n",
    "print(\"\\n--- Generating genres_simplified from OpenLibrary subjects ---\")\n",
    "\n",
    "# Import genre mapping utilities\n",
    "\n",
    "# Fill genres_simplified for books that have subjects but no genres\n",
    "books_needing_genre_mapping = (\n",
    "    (gb_enriched['genres_simplified'].isna()) & \n",
    "    (gb_enriched['subjects_openlib_clean'].notna())\n",
    ")\n",
    "\n",
    "print(f\"Books with subjects but no genres_simplified: {books_needing_genre_mapping.sum()}\")\n",
    "\n",
    "if books_needing_genre_mapping.sum() > 0:\n",
    "    # Apply genre mapping to subjects\n",
    "    gb_enriched.loc[books_needing_genre_mapping, 'genres_simplified'] = (\n",
    "        gb_enriched.loc[books_needing_genre_mapping, 'subjects_openlib_clean']\n",
    "        .apply(lambda x: map_subjects_to_genres(x) if isinstance(x, list) else None)\n",
    "    )\n",
    "    \n",
    "    filled_genres = (\n",
    "        gb_enriched.loc[books_needing_genre_mapping, 'genres_simplified'].notna().sum()\n",
    "    )\n",
    "    print(f\"genres_simplified: mapped {filled_genres} values from OpenLibrary subjects\")\n",
    "    \n",
    "    # Show sample of newly mapped genres\n",
    "    newly_mapped = gb_enriched[books_needing_genre_mapping & gb_enriched['genres_simplified'].notna()]\n",
    "    if len(newly_mapped) > 0:\n",
    "        print(\"\\nSample of newly mapped genres:\")\n",
    "        print(newly_mapped[['title_clean', 'subjects_openlib_clean', 'genres_simplified']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e72318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- ENRICHMENT SUMMARY ---\")\n",
    "\n",
    "# Show books that received OpenLibrary data\n",
    "books_with_ol_data = gb_enriched[\n",
    "    gb_enriched['subjects_openlib_clean'].notna()\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nTotal books enriched with OpenLibrary data: {len(books_with_ol_data)}\")\n",
    "\n",
    "if len(books_with_ol_data) > 0:\n",
    "    print(\"\\nSample of books enriched from OpenLibrary:\")\n",
    "    display(books_with_ol_data[[\n",
    "        'title_clean',\n",
    "        'author_clean',\n",
    "        'pages_openlib_clean',\n",
    "        'publication_date_openlib_clean',\n",
    "        'language_openlib_clean',\n",
    "        'subjects_openlib_clean',\n",
    "        'genres_simplified',\n",
    "        'publisher_clean',\n",
    "        'description_clean'\n",
    "    ]].head(10))\n",
    "\n",
    "# Show overall genre coverage\n",
    "print(f\"\\n--- GENRE COVERAGE AFTER ENRICHMENT ---\")\n",
    "print(f\"Books with genres_clean: {gb_enriched['genres_clean'].notna().sum()}\")\n",
    "print(f\"Books without genres_clean: {gb_enriched['genres_clean'].isna().sum()}\")\n",
    "print(f\"Genre coverage: {gb_enriched['genres_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\\n\")\n",
    "print(f\"Books with genres_simplified: {gb_enriched['genres_simplified'].notna().sum()}\")\n",
    "print(f\"Books without genres_simplified: {gb_enriched['genres_simplified'].isna().sum()}\")\n",
    "print(f\"Genre simplified coverage: {gb_enriched['genres_simplified'].notna().sum() / len(gb_enriched) * 100:.1f}%\")\n",
    "# Show overall description and publisher coverage\n",
    "print(f\"Books with description_clean: {gb_enriched['description_clean'].notna().sum()}\")\n",
    "print(f\"Books without description_clean: {gb_enriched['description_clean'].isna().sum()}\")\n",
    "print(f\"Books description coverage: {gb_enriched['description_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\")\n",
    "print(f\"Books with publisher_clean: {gb_enriched['publisher_clean'].notna().sum()}\")\n",
    "print(f\"Books without publisher_clean: {gb_enriched['publisher_clean'].isna().sum()}\")\n",
    "print(f\"Books publisher coverage: {gb_enriched['publisher_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e647000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 3\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab0e7c",
   "metadata": {},
   "source": [
    "#### Querying Google Books API (with Quota Management)\n",
    "\n",
    "After OpenLibrary enrichment, we create a new mask to identify remaining gaps: **1,730** books. Google Books API requires an API key and has daily quota limits (1,000 requests/day for free tier), so we implement several strategies: **(1)** process ISBNs in chunks of 1,000, **(2)** add sleep delays between requests, **(3)** cache all results to avoid re-querying, and **(4)** save progress incrementally.\n",
    "\n",
    "We load existing cache if available, query only uncached ISBNs, and update the cache after each session. This approach allows us to spread queries across multiple days if needed while preserving all previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3991bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many books still need enrichment\n",
    "new_missing_mask = (\n",
    "    gb_enriched['language_clean'].isna() |\n",
    "    gb_enriched['language_clean'].isin(['unknown', '', 'None']) |\n",
    "    gb_enriched['pages_clean'].isna() |\n",
    "    gb_enriched['publication_date_clean'].isna() | \n",
    "    gb_enriched['publisher_clean'].isna() |\n",
    "    gb_enriched['description_clean'].isna()\n",
    ")\n",
    "\n",
    "new_to_impute = gb_enriched[new_missing_mask].copy()\n",
    "print(\"Books still needing external enrichment:\", len(new_to_impute))\n",
    "\n",
    "# Show breakdown by field\n",
    "print(\"\\nBreakdown of remaining missing values:\")\n",
    "print(f\"  - Missing pages: {gb_enriched['pages_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing publication_date: {gb_enriched['publication_date_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing/invalid language: {(gb_enriched['language_clean'].isna() | gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()}\")\n",
    "print(f\"  - Missing publisher: {gb_enriched['publisher_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing description: {gb_enriched['description_clean'].isna().sum()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ed215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(data, size=1000):\n",
    "    for i in range(0, len(data), size):\n",
    "        yield data[i:i+size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d16623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define cache path for Google Books in data/raw\n",
    "CACHE_PATH = Path(\"data/raw/google_api_cache.json\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load existing cache if it exists\n",
    "if CACHE_PATH.exists():\n",
    "    with open(CACHE_PATH, \"r\") as f:\n",
    "        google_cache = json.load(f)\n",
    "else:\n",
    "    google_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44817fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_BOOKS_API_KEY\")\n",
    "\n",
    "def query_google_books(isbn):\n",
    "    isbn = str(isbn)\n",
    "\n",
    "    # Check cache\n",
    "    if isbn in google_cache:\n",
    "        return google_cache[isbn]\n",
    "\n",
    "    # Query Google Books with API key\n",
    "    url = (\n",
    "        f\"https://www.googleapis.com/books/v1/volumes?\"\n",
    "        f\"q=isbn:{isbn}&key={GOOGLE_API_KEY}\"\n",
    "    )\n",
    "\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        result = {\"isbn\": isbn, \"error\": f\"HTTP {r.status_code}\"}\n",
    "    else:\n",
    "        data = r.json()\n",
    "        if \"items\" in data and data[\"items\"]:\n",
    "            volume = data[\"items\"][0][\"volumeInfo\"]\n",
    "            result = {\n",
    "                \"isbn\": isbn,\n",
    "                \"title\": volume.get(\"title\"),\n",
    "                \"authors\": volume.get(\"authors\"),\n",
    "                \"publisher\": volume.get(\"publisher\"),\n",
    "                \"publishedDate\": volume.get(\"publishedDate\"),\n",
    "                \"pageCount\": volume.get(\"pageCount\"),\n",
    "                \"categories\": volume.get(\"categories\"),\n",
    "                \"language\": volume.get(\"language\"),\n",
    "                \"description\": volume.get(\"description\"),\n",
    "            }\n",
    "        else:\n",
    "            result = {\"isbn\": isbn, \"error\": \"No results\"}\n",
    "\n",
    "    # Save to cache\n",
    "    google_cache[isbn] = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_isbns = (\n",
    "    new_to_impute['isbn_query']\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "print(f\"Unique ISBNs to query: {len(list_of_isbns)}\")\n",
    "chunks = list(chunk_list(list_of_isbns, size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Choose which chunk you want to process today\n",
    "chunk_to_process = chunks[1]   # run chunk 0 today, 1 tomorrow, etc.\n",
    "results = []\n",
    "for isbn in tqdm(chunk_to_process, desc=\"Querying Google Books\"):\n",
    "    results.append(query_google_books(isbn))\n",
    "    time.sleep(0.1)   # be nice to the API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f955d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cache after ISBN queries\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(google_cache, f, indent=2)\n",
    "print(f\"Cache updated with {len(google_cache)} entries after ISBN queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d7361",
   "metadata": {},
   "source": [
    "#### Handling Books Without ISBNs\n",
    "\n",
    "Some books lack valid ISBNs but can still be enriched using **title and author search**. Google Books API supports `intitle:` and `inauthor:` query parameters, allowing us to find books by bibliographic metadata instead of identifiers. We create cache keys in `\"title|author\"` format to distinguish these from ISBN-based queries.\n",
    "\n",
    "This fallback strategy significantly increases our enrichment coverage, especially for older books, special editions, or records with ISBN errors. Results are cached alongside ISBN queries to maintain a unified enrichment workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad912ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_google_books_by_title(title, author):\n",
    "    \"\"\"Query Google Books API using title and author when ISBN is unavailable.\"\"\"\n",
    "    \n",
    "    # Create cache key\n",
    "    cache_key = f\"{title}|{author}\"\n",
    "    \n",
    "    if cache_key in google_cache:\n",
    "        return google_cache[cache_key]\n",
    "    \n",
    "    # Build query string\n",
    "    query_parts = []\n",
    "    if pd.notna(title):\n",
    "        query_parts.append(f'intitle:\"{title}\"')\n",
    "    if pd.notna(author):\n",
    "        query_parts.append(f'inauthor:\"{author}\"')\n",
    "    \n",
    "    query_string = \"+\".join(query_parts)\n",
    "    \n",
    "    url = (\n",
    "        f\"https://www.googleapis.com/books/v1/volumes?\"\n",
    "        f\"q={query_string}&key={GOOGLE_API_KEY}\"\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        result = {\"title\": title, \"author\": author, \"error\": f\"HTTP {r.status_code}\"}\n",
    "    else:\n",
    "        data = r.json()\n",
    "        if \"items\" in data and data[\"items\"]:\n",
    "            volume = data[\"items\"][0][\"volumeInfo\"]\n",
    "            result = {\n",
    "                \"title\": title,\n",
    "                \"author\": author,\n",
    "                \"pageCount\": volume.get(\"pageCount\"),\n",
    "                \"publisher\": volume.get(\"publisher\"),  \n",
    "                \"publishedDate\": volume.get(\"publishedDate\"),\n",
    "                \"categories\": volume.get(\"categories\"),\n",
    "                \"language\": volume.get(\"language\"),\n",
    "                \"description\": volume.get(\"description\"),\n",
    "            }\n",
    "        else:\n",
    "            result = {\"title\": title, \"author\": author, \"error\": \"No results\"}\n",
    "    \n",
    "    google_cache[cache_key] = result\n",
    "    return result\n",
    "\n",
    "# Process books without ISBN separately\n",
    "books_without_isbn = new_to_impute[new_to_impute['isbn_query'].isna()].copy()\n",
    "print(f\"Books without ISBN to query by title/author: {len(books_without_isbn)}\")\n",
    "\n",
    "results_by_title = []\n",
    "for idx, row in tqdm(books_without_isbn.iterrows(), \n",
    "                     total=len(books_without_isbn),\n",
    "                     desc=\"Querying Google Books by title/author\"):\n",
    "    results_by_title.append(query_google_books_by_title(\n",
    "        row['title_clean'], \n",
    "        row['author_clean']\n",
    "    ))\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cache after title/author queries\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(google_cache, f, indent=2)\n",
    "print(f\"Cache updated with {len(google_cache)} entries after title/author queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25cd3f8",
   "metadata": {},
   "source": [
    "#### Loading and Applying Cached Results\n",
    "\n",
    "The Google Books cache contains results from multiple query sessions, potentially across different days. We load the complete cache and separate ISBN-based results from title/author-based results by checking for the `\"|\"` delimiter in cache keys. This allows us to apply different matching logic for each result type.\n",
    "\n",
    "We then merge cached data back into `gb_enriched`, apply the cleaning pipeline to standardize formats, and fill remaining metadata gaps. The `map_subjects_to_genres()` function maps Google Books categories to our genre taxonomy, further increasing genre coverage. This completes our multi-source enrichment strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the cache\n",
    "CACHE_PATH = Path(\"google_api_cache.json\")\n",
    "\n",
    "with open(CACHE_PATH, \"r\") as f:\n",
    "    google_cache = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(google_cache)} cached entries\")\n",
    "\n",
    "# Separate ISBN-based results from title/author-based results\n",
    "isbn_results = []\n",
    "title_author_results = []\n",
    "\n",
    "for key, value in google_cache.items():\n",
    "    if \"|\" in key:  # Title|Author format\n",
    "        title_author_results.append(value)\n",
    "    else:  # ISBN format\n",
    "        isbn_results.append(value)\n",
    "\n",
    "print(f\"Found {len(isbn_results)} ISBN-based results\")\n",
    "print(f\"Found {len(title_author_results)} title/author-based results\")\n",
    "\n",
    "# add google books data to dataframe\n",
    "google_columns = [\n",
    "    'pageCount_google',\n",
    "    'publishedDate_google',\n",
    "    'categories_google',\n",
    "    'language_google',\n",
    "    'publisher_google',\n",
    "    'description_google'\n",
    "]\n",
    "for col in google_columns:\n",
    "    if col not in gb_enriched.columns:\n",
    "        gb_enriched[col] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741eb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge isbn_results back to gb_enriched\n",
    "if isbn_results:\n",
    "    google_isbn_df = pd.DataFrame(isbn_results)\n",
    "    print(\"\\nISBN results preview:\")\n",
    "    print(google_isbn_df.head())\n",
    "    \n",
    "    # Create ISBN mapping\n",
    "    isbn_to_data = {str(row['isbn']): row for _, row in google_isbn_df.iterrows() \n",
    "                    if 'isbn' in row and pd.notna(row.get('isbn'))}\n",
    "    \n",
    "    # Update gb_enriched\n",
    "    books_with_isbn = new_to_impute[new_to_impute['isbn_query'].notna()].copy()\n",
    "    \n",
    "    for idx in books_with_isbn.index:\n",
    "        isbn = str(books_with_isbn.loc[idx, 'isbn_query'])\n",
    "        if isbn in isbn_to_data:\n",
    "            result = isbn_to_data[isbn]\n",
    "            if pd.isna(result.get('error')):\n",
    "                gb_enriched.loc[idx, 'pageCount_google'] = result.get('pageCount')\n",
    "                gb_enriched.loc[idx, 'publishedDate_google'] = result.get('publishedDate')\n",
    "                gb_enriched.loc[idx, 'categories_google'] = result.get('categories')\n",
    "                gb_enriched.loc[idx, 'language_google'] = result.get('language')\n",
    "                gb_enriched.loc[idx, 'publisher_google'] = result.get('publisher')\n",
    "                gb_enriched.loc[idx, 'description_google'] = result.get('description')\n",
    "    \n",
    "    print(f\"✓ Merged ISBN-based results for {len(isbn_to_data)} books\")\n",
    "\n",
    "\n",
    "# merge title/author results back to gb_enriched\n",
    "\n",
    "if title_author_results:\n",
    "    google_title_df = pd.DataFrame(title_author_results)\n",
    "    print(\"\\nTitle/Author results preview:\")\n",
    "    print(google_title_df.head())\n",
    "    \n",
    "    # Recreate books_without_isbn from new_to_impute\n",
    "    books_without_isbn = new_to_impute[new_to_impute['isbn_query'].isna()].copy()\n",
    "    \n",
    "    # Create title|author key mapping\n",
    "    for i, (idx, row) in enumerate(books_without_isbn.iterrows()):\n",
    "        if i < len(google_title_df):\n",
    "            title_author_key = f\"{row['title_clean']}|{row['author_clean']}\"\n",
    "            if title_author_key in google_cache:\n",
    "                result = google_cache[title_author_key]\n",
    "                if pd.isna(result.get('error')):\n",
    "                    gb_enriched.loc[idx, 'pageCount_google'] = result.get('pageCount')\n",
    "                    gb_enriched.loc[idx, 'publishedDate_google'] = result.get('publishedDate')\n",
    "                    gb_enriched.loc[idx, 'categories_google'] = result.get('categories')\n",
    "                    gb_enriched.loc[idx, 'language_google'] = result.get('language')\n",
    "                    gb_enriched.loc[idx, 'publisher_google'] = result.get('publisher')\n",
    "                    gb_enriched.loc[idx, 'description_google'] = result.get('description')\n",
    "    \n",
    "    print(f\"✓ Merged title/author-based results for {len(books_without_isbn)} books\")\n",
    "\n",
    "# Verify merge\n",
    "print(\"\\nGoogle Books data merged:\")\n",
    "for col in google_columns:\n",
    "    count = gb_enriched[col].notna().sum()\n",
    "    print(f\"  - {col}: {count} values\")\n",
    "\n",
    "# clean Google Books API data\n",
    "from src.cleaning.utils.pipeline import apply_cleaners_selectively\n",
    "\n",
    "gb_enriched = apply_cleaners_selectively(\n",
    "    gb_enriched,\n",
    "    fields_to_clean=[\n",
    "        'pageCount',\n",
    "        'publishedDate',\n",
    "        'language',\n",
    "        'categories',\n",
    "        'publisher',\n",
    "        'description'\n",
    "        ],\n",
    "    source_suffix='_google',\n",
    "    target_suffix='_google_clean',\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# Verify cleaning\n",
    "print(\"\\nSample of cleaned Google Books data:\")\n",
    "display(gb_enriched[[\n",
    "    'title_clean',\n",
    "    'pages_clean',\n",
    "    'pageCount_google',\n",
    "    'pageCount_google_clean',\n",
    "    'publication_date_clean',\n",
    "    'publishedDate_google',\n",
    "    'publishedDate_google_clean',\n",
    "    'language_clean',\n",
    "    'language_google',\n",
    "    'language_google_clean',\n",
    "    'genres_clean',\n",
    "    'categories_google',\n",
    "    'categories_google_clean',\n",
    "    'genres_simplified',\n",
    "    'publisher_google',\n",
    "    'publisher_clean',\n",
    "    'description_google',\n",
    "    'description_clean',\n",
    "]].dropna(subset=['pageCount_google_clean', 'language_google_clean'], how='all').sample(min(15, len(gb_enriched)), random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76157a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after cleaning Google Books data, we'll add genre mapping\n",
    "print(\"\\n--- Generating genres_simplified from Google Books categories ---\")\n",
    "\n",
    "books_needing_google_genre_mapping = (\n",
    "    (gb_enriched['genres_simplified'].isna()) & \n",
    "    (gb_enriched['categories_google_clean'].notna())\n",
    ")\n",
    "\n",
    "print(f\"Books with Google categories but no genres_simplified: {books_needing_google_genre_mapping.sum()}\")\n",
    "\n",
    "if books_needing_google_genre_mapping.sum() > 0:\n",
    "    gb_enriched.loc[books_needing_google_genre_mapping, 'genres_simplified'] = (\n",
    "        gb_enriched.loc[books_needing_google_genre_mapping, 'categories_google_clean']\n",
    "        .apply(lambda x: map_subjects_to_genres(x) if isinstance(x, list) else None)\n",
    "    )\n",
    "    \n",
    "    filled_genres = (\n",
    "        gb_enriched.loc[books_needing_google_genre_mapping, 'genres_simplified'].notna().sum()\n",
    "    )\n",
    "    print(f\"genres_simplified: mapped {filled_genres} values from Google Books categories\")\n",
    "\n",
    "\n",
    "# fill missing values with cleaned Google Books data\n",
    "print(\"\\n--- Filling missing values with cleaned Google Books data ---\")\n",
    "\n",
    "# Fill pages_clean\n",
    "print(\"\\n--- Filling remaining page_clean using Google Books data ---\")\n",
    "before_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "gb_enriched['pages_clean'] = gb_enriched['pages_clean'].fillna(gb_enriched['pageCount_google_clean'])\n",
    "after_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "print(f\"pages_clean: filled {before_pages - after_pages} values from Google Books\")\n",
    "\n",
    "# Fill publication_date_clean\n",
    "print(\"\\n--- Filling remaining publication_date_clean using Google Books data ---\")\n",
    "before_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "gb_enriched['publication_date_clean'] = gb_enriched['publication_date_clean'].fillna(gb_enriched['publishedDate_google_clean'])\n",
    "after_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "print(f\"publication_date_clean: filled {before_date - after_date} values from Google Books\")\n",
    "\n",
    "# Fill language_clean\n",
    "print(\"\\n--- Filling remaining language_clean using Google Books data ---\")\n",
    "before_lang = (gb_enriched['language_clean'].isna() | \n",
    "               gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "mask = (gb_enriched['language_clean'].isna() | \n",
    "        gb_enriched['language_clean'].isin(['unknown', '', 'None']))\n",
    "gb_enriched.loc[mask, 'language_clean'] = gb_enriched.loc[mask, 'language_google_clean']\n",
    "after_lang = (gb_enriched['language_clean'].isna() | \n",
    "              gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "print(f\"language_clean: filled {before_lang - after_lang} values from Google Books\")\n",
    "\n",
    "# Fill publisher_clean\n",
    "print(\"\\n--- Filling remaining publisher_clean using Google Books data ---\")\n",
    "before_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "gb_enriched['publisher_clean'] = gb_enriched['publisher_clean'].fillna(\n",
    "    gb_enriched['publisher_google_clean']\n",
    ")\n",
    "after_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "print(f\"publisher_clean: filled {before_publisher - after_publisher} values from Google Books\")\n",
    "\n",
    "\n",
    "# Fill description_clean\n",
    "print(\"\\n--- Filling remaining description_clean using Google Books data ---\")\n",
    "before_desc_google = gb_enriched['description_clean'].isna().sum()\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].fillna(\n",
    "    gb_enriched['description_google_clean']\n",
    ")\n",
    "after_desc_google = gb_enriched['description_clean'].isna().sum()\n",
    "\n",
    "print(f\"description_clean (Google): filled {before_desc_google - after_desc_google} values\")\n",
    "\n",
    "# final enrichment summary\n",
    "print(\"\\n--- FINAL ENRICHMENT SUMMARY (ALL SOURCES) ---\")\n",
    "\n",
    "print(f\"\\nTotal books enriched with Google Books data: {gb_enriched[gb_enriched['categories_google_clean'].notna()].shape[0]}\")\n",
    "\n",
    "print(\"\\n--- FINAL METADATA COVERAGE ---\")\n",
    "print(f\"Books with pages_clean: {gb_enriched['pages_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['pages_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with publication_date_clean: {gb_enriched['publication_date_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['publication_date_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with publisher_clean: {gb_enriched['publisher_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['publisher_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with description_clean: {gb_enriched['description_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['description_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "valid_language = gb_enriched['language_clean'].notna() & ~gb_enriched['language_clean'].isin(['unknown', '', 'None'])\n",
    "print(f\"Books with valid language_clean: {valid_language.sum()} / {len(gb_enriched)} ({valid_language.sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n--- FINAL GENRE COVERAGE ---\")\n",
    "print(f\"Books with genres_clean: {gb_enriched['genres_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['genres_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with genres_simplified: {gb_enriched['genres_simplified'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['genres_simplified'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 4\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gb_enriched = pd.read_csv('data/cleaned/merge/gb_enriched_v4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14412823",
   "metadata": {},
   "source": [
    "Our multi-source enrichment strategy (BBE → OpenLibrary → Google Books) achieved excellent metadata coverage: **95.1%** for page counts, **100%** for publication dates, **99.4%** for valid language codes, **94.7%** publishers and **85.2%** descriptions. Genre coverage reached **80.8%** for `genres_clean` and **90.7%** for `genres_simplified`, a significant improvement from the original Goodbooks dataset which lacked genre information entirely.\n",
    "\n",
    "This enriched dataset now provides a comprehensive foundation for modeling and analysis. The combination of catalog metadata from BBE, behavioral data from Goodbooks ratings, and API-sourced supplemental information creates a unified dataset that supports both predictive modeling and catalog diversity analysis. The next step is filtering to English-language titles and preparing the final model-ready dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f7600",
   "metadata": {},
   "source": [
    "#### Final Metadata Enrichment Steps\n",
    "\n",
    "After completing API-based enrichment, we perform three final metadata enhancement steps to ensure completeness and consistency across all enriched records:\n",
    "\n",
    "1. **Major Publisher Classification**: We apply the `is_major_publisher` flag to books that received publisher data from APIs but weren't present in the BBE dataset. Using the same publisher pattern matching from Notebook 02, we classify publishers against our curated list of major publishing houses.\n",
    "\n",
    "2. **Awards Flag Completion**: We fill missing `has_award` values with `False` for all books that weren't in the BBE dataset (which contains award metadata). Since API sources don't reliably provide award information, we assume absence of award data means no awards.\n",
    "\n",
    "3. **NLP-Ready Description Generation**: For books enriched with API descriptions, we apply the same NLP cleaning pipeline used in Notebook 02. This converts descriptions into analysis-ready text by removing HTML tags, normalizing whitespace, and standardizing punctuatio, ensuring consistency across BBE and API-sourced descriptions for future text analysis tasks.\n",
    "\n",
    "These steps ensure that all enriched books have the same metadata structure and quality as the original BBE dataset, maintaining consistency across the entire unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load publisher patterns from JSON file\n",
    "with open(\"src/cleaning/mappings/publisher_parent_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    major_publishers = json.load(f)\n",
    "\n",
    "mask_missing_major = gb_enriched['is_major_publisher'].isna()\n",
    "\n",
    "gb_enriched.loc[mask_missing_major, 'is_major_publisher'] = (\n",
    "    gb_enriched.loc[mask_missing_major, 'publisher_clean']\n",
    "        .str.lower()\n",
    "        .apply(lambda x: any(mp in x for mp in major_publishers) if isinstance(x, str) else False)\n",
    ")\n",
    "print(f\"Books without is_major_publisher flag: {gb_enriched['is_major_publisher'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Books without has_awards flag: {gb_enriched['has_award'].isna().sum()}\")\n",
    "gb_enriched['has_award'] = (\n",
    "    gb_enriched['has_award']\n",
    "    .fillna(False)\n",
    "    .astype('bool')\n",
    ")\n",
    "print(f\"Remaining books without has_awards flag: {gb_enriched['has_award'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cleaning.utils.text_cleaning import (\n",
    "    clean_description_nlp\n",
    ")\n",
    "# generate description_nlp from description_clean where API descriptions exist\n",
    "mask_api_desc = (\n",
    "    gb_enriched['description_openlib'].notna() |\n",
    "    gb_enriched['description_google'].notna()\n",
    ")\n",
    "\n",
    "gb_enriched.loc[mask_api_desc, 'description_nlp'] = (\n",
    "    gb_enriched.loc[mask_api_desc, 'description_clean']\n",
    "        .apply(clean_description_nlp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fix_missing_text(col):\n",
    "    return (\n",
    "        gb_enriched[col]\n",
    "        .replace([\"\", \" \", \"None\", \"none\", \"nan\", \"Nan\", \"NAN\"], np.nan)\n",
    "        .replace(r\"^\\s+$\", np.nan, regex=True)\n",
    "    )\n",
    "\n",
    "gb_enriched['description_clean'] = fix_missing_text('description_clean')\n",
    "gb_enriched['description_nlp']   = fix_missing_text('description_nlp')\n",
    "\n",
    "print(\"Missing description_clean:\", gb_enriched['description_clean'].isna().sum())\n",
    "print(\"Missing description_nlp:\", gb_enriched['description_nlp'].isna().sum())\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].astype(\"string\")\n",
    "gb_enriched['description_nlp']   = gb_enriched['description_nlp'].astype(\"string\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c1b3c",
   "metadata": {},
   "source": [
    "### Filtering for English-Language Books\n",
    "\n",
    "To ensure consistency and focus for downstream analysis and modeling, we filter the enriched dataset to include only **English-language books**. This step is critical for:\n",
    "\n",
    "- **Genre diversity analysis**: Comparing genre distributions across a linguistically consistent corpus\n",
    "- **Ratings behavior modeling**: Ensuring user rating patterns reflect a common language context\n",
    "- **Text analysis (stretch)**: Enabling NLP tasks on descriptions without multilingual complexity\n",
    "\n",
    "We create a filtered copy of `gb_enriched` containing only books where `language_clean` is identified as English (using ISO 639 language code`'en'`). This filtered dataset will serve as the primary input for modeling and analysis, while the full enriched dataset (including non-English titles) is preserved for reference.\n",
    "\n",
    "The English-only dataset is saved as the final output, ready for exploratory analysis and model development in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_enriched_en = gb_enriched[gb_enriched['language_clean'] == 'en'].copy()\n",
    "print(\"Filtered (EN only):\", gb_enriched_en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f39309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop intermediate enrichment columns\n",
    "enrichment_columns_to_drop = [\n",
    "    # OpenLibrary raw and intermediate columns\n",
    "    'pages_openlib',\n",
    "    'publication_date_openlib',\n",
    "    'language_openlib',\n",
    "    'subjects_openlib',\n",
    "    'publisher_openlib',\n",
    "    'description_openlib',\n",
    "    'pages_openlib_clean',\n",
    "    'publication_date_openlib_clean',\n",
    "    'language_openlib_clean',\n",
    "    'subjects_openlib_clean',\n",
    "    'publisher_openlib_clean',\n",
    "    'description_openlib_clean',\n",
    "    # Google Books raw and intermediate columns\n",
    "    'pageCount_google',\n",
    "    'publishedDate_google',\n",
    "    'categories_google',\n",
    "    'language_google',\n",
    "    'publisher_google',\n",
    "    'description_google',\n",
    "    'pageCount_google_clean',\n",
    "    'publishedDate_google_clean',\n",
    "    'language_google_clean',\n",
    "    'categories_google_clean',\n",
    "    'publisher_google_clean',\n",
    "    'description_google_clean',\n",
    "    # Query helper column\n",
    "    'isbn_query',\n",
    "    # Other intermediate columns\n",
    "    'isbn13_clean',\n",
    "]\n",
    "\n",
    "# drop enrichment columns\n",
    "gb_enriched_en = gb_enriched_en.drop(columns=enrichment_columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Columns after dropping enrichment data: {len(gb_enriched_en.columns)}\")\n",
    "print(f\"Shape before dropping duplicates: {gb_enriched_en.shape}\")\n",
    "\n",
    "# drop duplicate rows based on goodreads_id_clean (keep first occurrence)\n",
    "gb_enriched_en = gb_enriched_en.drop_duplicates(subset=['goodreads_id_clean'], keep='first')\n",
    "\n",
    "print(f\"Shape after dropping duplicates: {gb_enriched_en.shape}\")\n",
    "print(f\"Duplicates removed: {gb_enriched_en.shape[0]}\")\n",
    "\n",
    "# Verify final columns\n",
    "print(\"\\nFinal columns for analysis:\")\n",
    "print(gb_enriched_en.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# FINAL DATA QUALITY CHECKS BEFORE SAVING\n",
    "# ================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL DATA QUALITY CHECKS - gb_enriched_en\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# check for duplicates\n",
    "print(\"\\n1. DUPLICATE CHECK\")\n",
    "print(f\"Total rows: {len(gb_enriched_en)}\")\n",
    "duplicates = gb_enriched_en.duplicated(subset=['goodreads_id_clean']).sum()\n",
    "print(f\"Duplicate goodreads_id_clean: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"⚠️ WARNING: Duplicates found!\")\n",
    "    display(gb_enriched_en[gb_enriched_en.duplicated(subset=['goodreads_id_clean'], keep=False)])\n",
    "\n",
    "# check for null values in critical columns\n",
    "print(\"\\n2. NULL VALUES IN CRITICAL COLUMNS\")\n",
    "critical_cols = ['goodreads_id_clean', 'title_clean', 'author_clean', 'language_clean']\n",
    "for col in critical_cols:\n",
    "    null_count = gb_enriched_en[col].isna().sum()\n",
    "    print(f\"{col}: {null_count} nulls ({null_count/len(gb_enriched_en)*100:.2f}%)\")\n",
    "    if null_count > 0:\n",
    "        print(f\"⚠️ WARNING: Nulls found in {col}!\")\n",
    "\n",
    "# verify language filter worked\n",
    "print(\"\\n3. LANGUAGE CONSISTENCY CHECK\")\n",
    "unique_langs = gb_enriched_en['language_clean'].unique()\n",
    "print(f\"Unique languages: {unique_langs}\")\n",
    "if len(unique_langs) > 1 or unique_langs[0] != 'en':\n",
    "    print(\"⚠️ WARNING: Non-English books found after filtering!\")\n",
    "\n",
    "# check data types\n",
    "print(\"\\n4. DATA TYPE CHECK\")\n",
    "print(gb_enriched_en.dtypes)\n",
    "\n",
    "# check for empty strings or whitespace-only values\n",
    "print(\"\\n5. EMPTY STRING CHECK\")\n",
    "text_cols = ['title_clean', 'author_clean', 'publisher_clean', 'description_clean']\n",
    "for col in text_cols:\n",
    "    if col in gb_enriched_en.columns:\n",
    "        empty = (gb_enriched_en[col] == '').sum()\n",
    "        whitespace = gb_enriched_en[col].str.strip().eq('').sum()\n",
    "        print(f\"{col}: {empty} empty strings, {whitespace} whitespace-only\")\n",
    "\n",
    "# check metadata coverage\n",
    "print(\"\\n6. METADATA COVERAGE\")\n",
    "metadata_cols = {\n",
    "    'pages_clean': 'Pages',\n",
    "    'publication_date_clean': 'Publication Date',\n",
    "    'publisher_clean': 'Publisher',\n",
    "    'genres_simplified': 'Genres',\n",
    "    'description_clean': 'Description'\n",
    "}\n",
    "for col, label in metadata_cols.items():\n",
    "    if col in gb_enriched_en.columns:\n",
    "        coverage = gb_enriched_en[col].notna().sum() / len(gb_enriched_en) * 100\n",
    "        print(f\"{label}: {coverage:.1f}% coverage\")\n",
    "\n",
    "# check for unexpected enrichment columns still present\n",
    "print(\"\\n7. ENRICHMENT COLUMN CHECK\")\n",
    "enrichment_patterns = ['_openlib', '_google', '_bbe', 'isbn_query']\n",
    "leftover_cols = [col for col in gb_enriched_en.columns \n",
    "                 if any(pattern in col for pattern in enrichment_patterns)]\n",
    "if leftover_cols:\n",
    "    print(f\"WARNING: Leftover enrichment columns found: {leftover_cols}\")\n",
    "else:\n",
    "    print(\"✓ No enrichment columns remaining\")\n",
    "\n",
    "# aummary statistics\n",
    "print(\"\\n8. SUMMARY STATISTICS\")\n",
    "print(f\"Final dataset shape: {gb_enriched_en.shape}\")\n",
    "print(f\"Columns: {len(gb_enriched_en.columns)}\")\n",
    "print(f\"Memory usage: {gb_enriched_en.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHECKS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff346538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'en_internal_catalog'\n",
    "clean_path = Path(\"outputs/datasets/cleaned\")\n",
    "clean_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gb_enriched_en.to_csv(clean_path / f\"{file_name}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} saved successfully in {clean_path} directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33cddfc",
   "metadata": {},
   "source": [
    "## BBE English Only \n",
    "\n",
    "The BBE dataset has already been cleaned in the previous steps. Since we are not enriching it further, we only need to filter it to English-language books for consistency with the Goodbooks dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efe737",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_supply_catalog = bbe_clean[ bbe_clean[\"language_clean\"] == \"en\" ]\n",
    "\n",
    "# check shape after filtering\n",
    "print(f\"Original BBE dataset shape: {bbe_clean.shape}\")\n",
    "print(f\"English-only BBE dataset shape: {en_supply_catalog.shape}\")\n",
    "print(f\"Books filtered out (non-English): {bbe_clean.shape[0] - en_supply_catalog.shape[0]}\")\n",
    "print(f\"English books percentage: {(en_supply_catalog.shape[0] / bbe_clean.shape[0]) * 100:.1f}%\")\n",
    "\n",
    "# display sample\n",
    "print(\"\\nSample of English-only BBE dataset:\")\n",
    "display(en_supply_catalog.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd469b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL DATA QUALITY CHECKS - en_supply_catalog\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# check for duplicates\n",
    "print(\"\\n1. DUPLICATE CHECK\")\n",
    "print(f\"Total rows: {len(en_supply_catalog)}\")\n",
    "duplicates_bbe = en_supply_catalog.duplicated(subset=['goodreads_id_clean']).sum()\n",
    "print(f\"Duplicate goodreads_id_clean: {duplicates_bbe}\")\n",
    "\n",
    "# verify language filter\n",
    "print(\"\\n2. LANGUAGE CONSISTENCY CHECK\")\n",
    "unique_langs_bbe = en_supply_catalog['language_clean'].unique()\n",
    "print(f\"Unique languages: {unique_langs_bbe}\")\n",
    "\n",
    "# metadata coverage\n",
    "print(\"\\n3. METADATA COVERAGE\")\n",
    "for col, label in metadata_cols.items():\n",
    "    if col in en_supply_catalog.columns:\n",
    "        coverage = en_supply_catalog[col].notna().sum() / len(en_supply_catalog) * 100\n",
    "        print(f\"{label}: {coverage:.1f}% coverage\")\n",
    "\n",
    "print(f\"\\nFinal BBE dataset shape: {en_supply_catalog.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'en_supply_catalog'\n",
    "clean_path = Path(\"outputs/datasets/cleaned\")\n",
    "clean_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "en_supply_catalog.to_csv(clean_path / f\"{file_name}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} saved successfully in {clean_path} directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ef1cf",
   "metadata": {},
   "source": [
    "# Dataset Merge Strategy\n",
    "We merge Goodbooks (internal catalog + ratings) with BBE (external supply catalog) on goodreads_id_clean to create two modeling variants:\n",
    "\n",
    "## Warm Start Dataset\n",
    "Includes external BBE signals (ratings, votes, liked %) for **cross-platform validation**:\n",
    "\n",
    "- Do books popular on BBE also perform well on Goodbooks?\n",
    "- Useful for analyzing rating transfer patterns across platforms\n",
    "\n",
    "## Cold Start Dataset\n",
    "Excludes all external behavioral features to prevent leakage:\n",
    "\n",
    "- Trains only on intrinsic book metadata (genre, author, publisher, etc.)\n",
    "- Simulates **new book scenarios** where external platform data is unavailable\n",
    "- Production-ready for fair model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5588c3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goodreads_id_clean'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create renaming dictionaries for merging datasets\n",
    "rename_gb = {\n",
    "    'book_id': 'gb_book_id',\n",
    "    'work_text_reviews_count': 'gb_work_text_reviews_count',\n",
    "    'ratings_1': 'gb_ratings_1',\n",
    "    'ratings_2': 'gb_ratings_2',\n",
    "    'ratings_3': 'gb_ratings_3',\n",
    "    'ratings_4': 'gb_ratings_4',\n",
    "    'ratings_5': 'gb_ratings_5',\n",
    "\n",
    "    # KEEP MERGE KEYS AS IS:\n",
    "    # 'goodreads_id_clean': 'goodreads_id_clean',\n",
    "\n",
    "    'best_book_id_clean': 'gb_best_book_id_clean',\n",
    "    'work_id_clean': 'gb_work_id_clean',\n",
    "    'authors_list': 'gb_authors_list',\n",
    "    'author_clean': 'gb_author_clean',\n",
    "    'language_clean': 'gb_language_clean',\n",
    "    'publication_date_clean': 'gb_publication_date_clean',\n",
    "    'isbn_clean': 'gb_isbn_clean',\n",
    "    'rating_clean': 'gb_rating_clean',                  # TARGET\n",
    "    'numRatings_clean': 'gb_numRatings_clean',\n",
    "    'numRatings_log': 'gb_numRatings_log',\n",
    "    'ratings_1_share': 'gb_ratings_1_share',\n",
    "    'ratings_2_share': 'gb_ratings_2_share',\n",
    "    'ratings_3_share': 'gb_ratings_3_share',\n",
    "    'ratings_4_share': 'gb_ratings_4_share',\n",
    "    'ratings_5_share': 'gb_ratings_5_share',\n",
    "    'work_text_reviews_log': 'gb_work_text_reviews_log',\n",
    "    'series_clean': 'gb_series_clean',\n",
    "    'title_clean': 'gb_title_clean',\n",
    "    'pages_clean': 'gb_pages_clean',\n",
    "    'genres_clean': 'gb_genres_clean',\n",
    "    'genres_simplified': 'gb_genres_simplified',\n",
    "    'publisher_clean': 'gb_publisher_clean',\n",
    "    'is_major_publisher': 'gb_is_major_publisher',\n",
    "    'has_award': 'gb_has_award',\n",
    "    'description_clean': 'gb_description_clean',\n",
    "    'description_nlp': 'gb_description_nlp'\n",
    "}\n",
    "\n",
    "rename_bbe = {\n",
    "    # shared key remains untouched\n",
    "    # 'goodreads_id_clean': 'goodreads_id_clean'\n",
    "\n",
    "    'authors_list': 'bbe_authors_list',\n",
    "    'author_clean': 'bbe_author_clean',\n",
    "    'title_clean': 'bbe_title_clean',\n",
    "    'isbn_clean': 'bbe_isbn_clean',\n",
    "    'language_clean': 'bbe_language_clean',\n",
    "    'publication_date_clean': 'bbe_publication_date_clean',\n",
    "    'publisher_clean': 'bbe_publisher_clean',\n",
    "    'is_major_publisher': 'bbe_is_major_publisher',\n",
    "    'bookFormat_clean': 'bbe_bookFormat_clean',\n",
    "\n",
    "    'rating_clean': 'bbe_rating_clean',  # EXTERNAL RATING (predictive feature for model 1)\n",
    "    'numRatings_clean': 'bbe_numRatings_clean',\n",
    "    'numRatings_log': 'bbe_numRatings_log',\n",
    "\n",
    "    'ratings_1': 'bbe_ratings_1',\n",
    "    'ratings_2': 'bbe_ratings_2',\n",
    "    'ratings_3': 'bbe_ratings_3',\n",
    "    'ratings_4': 'bbe_ratings_4',\n",
    "    'ratings_5': 'bbe_ratings_5',\n",
    "\n",
    "    'ratings_1_share': 'bbe_ratings_1_share',\n",
    "    'ratings_2_share': 'bbe_ratings_2_share',\n",
    "    'ratings_3_share': 'bbe_ratings_3_share',\n",
    "    'ratings_4_share': 'bbe_ratings_4_share',\n",
    "    'ratings_5_share': 'bbe_ratings_5_share',\n",
    "\n",
    "    'has_award': 'bbe_has_award',\n",
    "    'genres_clean': 'bbe_genres_clean',\n",
    "    'genres_simplified': 'bbe_genres_simplified',\n",
    "    'description_clean': 'bbe_description_clean',\n",
    "    'description_nlp': 'bbe_description_nlp',\n",
    "    'series_clean': 'bbe_series_clean',\n",
    "    'pages_clean': 'bbe_pages_clean',\n",
    "\n",
    "    'bbeVotes_clean': 'bbe_votes_clean',\n",
    "    'bbeScore_clean': 'bbe_score_clean',\n",
    "    'likedPercent_clean': 'bbe_likedPercent_clean',\n",
    "    'has_likedPercent': 'bbe_has_likedPercent',\n",
    "    'price_clean': 'bbe_price_clean',\n",
    "    'price_flag': 'bbe_price_flag'\n",
    "}\n",
    "\n",
    "# apply renaming\n",
    "internal_catalog = gb_enriched_en.rename(columns=rename_gb)\n",
    "supply = en_supply_catalog.rename(columns=rename_bbe)\n",
    "\n",
    "set(internal_catalog.columns).intersection(set(supply.columns))\n",
    "# should return {'goodreads_id_clean'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "42a74a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (9742, 69)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gb_book_id</th>\n",
       "      <th>gb_work_text_reviews_count</th>\n",
       "      <th>gb_ratings_1</th>\n",
       "      <th>gb_ratings_2</th>\n",
       "      <th>gb_ratings_3</th>\n",
       "      <th>gb_ratings_4</th>\n",
       "      <th>gb_ratings_5</th>\n",
       "      <th>goodreads_id_clean</th>\n",
       "      <th>gb_best_book_id_clean</th>\n",
       "      <th>gb_work_id_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>bbe_description_clean</th>\n",
       "      <th>bbe_description_nlp</th>\n",
       "      <th>bbe_series_clean</th>\n",
       "      <th>bbe_pages_clean</th>\n",
       "      <th>bbe_votes_clean</th>\n",
       "      <th>bbe_score_clean</th>\n",
       "      <th>bbe_likedPercent_clean</th>\n",
       "      <th>bbe_has_likedPercent</th>\n",
       "      <th>bbe_price_clean</th>\n",
       "      <th>bbe_price_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>155254</td>\n",
       "      <td>66715</td>\n",
       "      <td>127936</td>\n",
       "      <td>560092</td>\n",
       "      <td>1481305</td>\n",
       "      <td>2706317</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2792775</td>\n",
       "      <td>...</td>\n",
       "      <td>winning means fame and fortunelosing means cer...</td>\n",
       "      <td>winning means fame and fortunelosing means cer...</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>374.0</td>\n",
       "      <td>30516.0</td>\n",
       "      <td>2993816.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>75867</td>\n",
       "      <td>75504</td>\n",
       "      <td>101676</td>\n",
       "      <td>455024</td>\n",
       "      <td>1156318</td>\n",
       "      <td>3011543</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4640799</td>\n",
       "      <td>...</td>\n",
       "      <td>harry potter's life is miserable his parents a...</td>\n",
       "      <td>harry potter's life is miserable his parents a...</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>309.0</td>\n",
       "      <td>7348.0</td>\n",
       "      <td>691430.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>95009</td>\n",
       "      <td>456191</td>\n",
       "      <td>436802</td>\n",
       "      <td>793319</td>\n",
       "      <td>875073</td>\n",
       "      <td>1355439</td>\n",
       "      <td>41865</td>\n",
       "      <td>41865</td>\n",
       "      <td>3212258</td>\n",
       "      <td>...</td>\n",
       "      <td>about three things i was absolutely positive f...</td>\n",
       "      <td>about three things i was absolutely positive f...</td>\n",
       "      <td>the twilight saga</td>\n",
       "      <td>501.0</td>\n",
       "      <td>14874.0</td>\n",
       "      <td>1459448.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>72586</td>\n",
       "      <td>60427</td>\n",
       "      <td>117415</td>\n",
       "      <td>446835</td>\n",
       "      <td>1001952</td>\n",
       "      <td>1714267</td>\n",
       "      <td>2657</td>\n",
       "      <td>2657</td>\n",
       "      <td>3275794</td>\n",
       "      <td>...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>324.0</td>\n",
       "      <td>23328.0</td>\n",
       "      <td>2269402.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>51992</td>\n",
       "      <td>86236</td>\n",
       "      <td>197621</td>\n",
       "      <td>606158</td>\n",
       "      <td>936012</td>\n",
       "      <td>947718</td>\n",
       "      <td>4671</td>\n",
       "      <td>4671</td>\n",
       "      <td>245494</td>\n",
       "      <td>...</td>\n",
       "      <td>alternate cover edition isbn 0743273567 isbn13...</td>\n",
       "      <td>alternate cover edition isbn 0743273567 isbn13...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.0</td>\n",
       "      <td>8142.0</td>\n",
       "      <td>755074.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gb_book_id  gb_work_text_reviews_count  gb_ratings_1  gb_ratings_2  \\\n",
       "0           1                      155254         66715        127936   \n",
       "1           2                       75867         75504        101676   \n",
       "2           3                       95009        456191        436802   \n",
       "3           4                       72586         60427        117415   \n",
       "4           5                       51992         86236        197621   \n",
       "\n",
       "   gb_ratings_3  gb_ratings_4  gb_ratings_5 goodreads_id_clean  \\\n",
       "0        560092       1481305       2706317            2767052   \n",
       "1        455024       1156318       3011543                  3   \n",
       "2        793319        875073       1355439              41865   \n",
       "3        446835       1001952       1714267               2657   \n",
       "4        606158        936012        947718               4671   \n",
       "\n",
       "   gb_best_book_id_clean  gb_work_id_clean  ...  \\\n",
       "0                2767052           2792775  ...   \n",
       "1                      3           4640799  ...   \n",
       "2                  41865           3212258  ...   \n",
       "3                   2657           3275794  ...   \n",
       "4                   4671            245494  ...   \n",
       "\n",
       "                               bbe_description_clean  \\\n",
       "0  winning means fame and fortunelosing means cer...   \n",
       "1  harry potter's life is miserable his parents a...   \n",
       "2  about three things i was absolutely positive f...   \n",
       "3  the unforgettable novel of a childhood in a sl...   \n",
       "4  alternate cover edition isbn 0743273567 isbn13...   \n",
       "\n",
       "                                 bbe_description_nlp       bbe_series_clean  \\\n",
       "0  winning means fame and fortunelosing means cer...       the hunger games   \n",
       "1  harry potter's life is miserable his parents a...           harry potter   \n",
       "2  about three things i was absolutely positive f...      the twilight saga   \n",
       "3  the unforgettable novel of a childhood in a sl...  to kill a mockingbird   \n",
       "4  alternate cover edition isbn 0743273567 isbn13...                    NaN   \n",
       "\n",
       "  bbe_pages_clean bbe_votes_clean  bbe_score_clean  bbe_likedPercent_clean  \\\n",
       "0           374.0         30516.0        2993816.0                    96.0   \n",
       "1           309.0          7348.0         691430.0                    96.0   \n",
       "2           501.0         14874.0        1459448.0                    78.0   \n",
       "3           324.0         23328.0        2269402.0                    95.0   \n",
       "4           200.0          8142.0         755074.0                    90.0   \n",
       "\n",
       "   bbe_has_likedPercent  bbe_price_clean  bbe_price_flag  \n",
       "0                   1.0             5.09           False  \n",
       "1                   1.0              NaN            True  \n",
       "2                   1.0             2.10           False  \n",
       "3                   1.0              NaN            True  \n",
       "4                   1.0              NaN            True  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure merge key is string type\n",
    "internal_catalog['goodreads_id_clean'] = internal_catalog['goodreads_id_clean'].astype(str)\n",
    "supply['goodreads_id_clean'] = supply['goodreads_id_clean'].astype(str)\n",
    "# perform left merge\n",
    "merged = internal_catalog.merge(\n",
    "    supply,\n",
    "    on=\"goodreads_id_clean\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(f\"Merged dataset shape: {merged.shape}\")\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to extract primary author\n",
    "def extract_primary_author(author_string):\n",
    "    \"\"\"Extract the first author from a comma-separated author string.\"\"\"\n",
    "    if pd.isna(author_string):\n",
    "        return None\n",
    "    # Ensure it is str\n",
    "    author_string = str(author_string)\n",
    "    # Split by comma and return first element\n",
    "    primary = author_string.split(',')[0].strip()\n",
    "    return primary if primary else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b98080e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidation complete.\n",
      "Final columns added: ['title_final', 'author_final', 'authors_list_final', 'language_final', 'publication_date_final', 'isbn_final', 'series_final', 'publisher_final', 'pages_final', 'genres_final', 'genres_simple_final', 'description_final', 'description_nlp_final', 'has_award_final', 'is_major_publisher_final', 'price_flag_final']\n",
      "Final dataset shape: (9742, 106)\n",
      "Dataset columns:\n",
      "['gb_book_id', 'gb_work_text_reviews_count', 'gb_ratings_1', 'gb_ratings_2', 'gb_ratings_3', 'gb_ratings_4', 'gb_ratings_5', 'goodreads_id_clean', 'gb_best_book_id_clean', 'gb_work_id_clean', 'gb_authors_list', 'gb_author_clean', 'gb_language_clean', 'gb_publication_date_clean', 'gb_isbn_clean', 'gb_rating_clean', 'gb_numRatings_clean', 'gb_numRatings_log', 'gb_ratings_1_share', 'gb_ratings_2_share', 'gb_ratings_3_share', 'gb_ratings_4_share', 'gb_ratings_5_share', 'gb_work_text_reviews_log', 'gb_series_clean', 'gb_title_clean', 'gb_pages_clean', 'gb_genres_clean', 'gb_genres_simplified', 'gb_publisher_clean', 'gb_is_major_publisher', 'gb_has_award', 'gb_description_clean', 'gb_description_nlp', 'bbe_authors_list', 'bbe_author_clean', 'bbe_title_clean', 'bbe_isbn_clean', 'bbe_language_clean', 'bbe_publication_date_clean', 'bbe_publisher_clean', 'bbe_is_major_publisher', 'bbe_bookFormat_clean', 'bbe_rating_clean', 'bbe_numRatings_clean', 'bbe_numRatings_log', 'bbe_ratings_1', 'bbe_ratings_2', 'bbe_ratings_3', 'bbe_ratings_4', 'bbe_ratings_5', 'bbe_ratings_1_share', 'bbe_ratings_2_share', 'bbe_ratings_3_share', 'bbe_ratings_4_share', 'bbe_ratings_5_share', 'bbe_has_award', 'bbe_genres_clean', 'bbe_genres_simplified', 'bbe_description_clean', 'bbe_description_nlp', 'bbe_series_clean', 'bbe_pages_clean', 'bbe_votes_clean', 'bbe_score_clean', 'bbe_likedPercent_clean', 'bbe_has_likedPercent', 'bbe_price_clean', 'bbe_price_flag', 'title_final', 'gb_primary_author', 'bbe_primary_author', 'author_final', 'authors_list_final', 'language_final', 'publication_date_final', 'publication_year', 'publication_decade', 'isbn_final', 'series_final', 'publisher_final', 'pages_final', 'genres_final', 'genres_simple_final', 'description_final', 'description_nlp_final', 'has_award_final', 'is_major_publisher_final', 'external_price', 'price_flag_final', 'external_bookformat', 'external_rating', 'external_numratings', 'external_votes', 'external_score', 'external_likedpct', 'external_bbe_ratings_1', 'external_bbe_ratings_2', 'external_bbe_ratings_3', 'external_bbe_ratings_4', 'external_bbe_ratings_5', 'external_bbe_ratings_1_share', 'external_bbe_ratings_2_share', 'external_bbe_ratings_3_share', 'external_bbe_ratings_4_share', 'external_bbe_ratings_5_share']\n"
     ]
    }
   ],
   "source": [
    "# CONSOLIDATION BLOCK: GB + BBE -> final columns\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "# 1) PUBLICATION_DATE as datetime\n",
    "# gb_publication_date_clean is object (string), convert safely\n",
    "merged['gb_publication_date_clean'] = pd.to_datetime(\n",
    "    merged['gb_publication_date_clean'], errors='coerce'\n",
    ")\n",
    "\n",
    "# bbe_publication_date_clean is object (string), convert safely\n",
    "merged['bbe_publication_date_clean'] = pd.to_datetime(\n",
    "    merged['bbe_publication_date_clean'], errors='coerce'\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Consolidate TITLE\n",
    "merged['title_final'] = merged['gb_title_clean'].combine_first(\n",
    "    merged['bbe_title_clean']\n",
    ")\n",
    "\n",
    "# 3) Consolidate AUTHOR (single author)\n",
    "merged['gb_primary_author'] = merged['gb_author_clean'].apply(extract_primary_author)\n",
    "merged['bbe_primary_author'] = merged['bbe_author_clean'].apply(extract_primary_author)\n",
    "\n",
    "\n",
    "merged['author_final'] = merged['gb_primary_author'].combine_first(\n",
    "    merged['bbe_primary_author']\n",
    ")\n",
    "# Standardize author_final to lowercase and strip whitespace\n",
    "merged['author_final'] = merged['author_final'].str.lower().str.strip()\n",
    "\n",
    "\n",
    "# 4) Consolidate AUTHORS LIST (multi-author)\n",
    "merged['authors_list_final'] = merged['gb_authors_list'].combine_first(\n",
    "    merged['bbe_authors_list']\n",
    ")\n",
    "\n",
    "\n",
    "# 5) Consolidate LANGUAGE\n",
    "merged['language_final'] = merged['gb_language_clean'].combine_first(\n",
    "    merged['bbe_language_clean']\n",
    ")\n",
    "\n",
    "\n",
    "# 6) Consolidate PUBLICATION DATE + YEAR + DECADE\n",
    "merged['publication_date_final'] = merged['gb_publication_date_clean'].combine_first(\n",
    "    merged['bbe_publication_date_clean']\n",
    ")\n",
    "\n",
    "# Derived fields:\n",
    "merged['publication_year'] = merged['publication_date_final'].dt.year\n",
    "merged['publication_decade'] = (merged['publication_year'] // 10) * 10\n",
    "\n",
    "\n",
    "# 7) Consolidate ISBN\n",
    "merged['isbn_final'] = merged['gb_isbn_clean'].combine_first(\n",
    "    merged['bbe_isbn_clean']\n",
    ")\n",
    "\n",
    "\n",
    "# 8) Consolidate SERIES\n",
    "merged['series_final'] = merged['gb_series_clean'].combine_first(\n",
    "    merged['bbe_series_clean']\n",
    ")\n",
    "\n",
    "\n",
    "# 9) Consolidate PUBLISHER\n",
    "merged['publisher_final'] = merged['gb_publisher_clean'].combine_first(\n",
    "    merged['bbe_publisher_clean']\n",
    ")\n",
    "\n",
    "\n",
    "# 10) Consolidate PAGES\n",
    "# gb_pages_clean: float\n",
    "# bbe_pages_clean: float\n",
    "merged['pages_final'] = merged['gb_pages_clean'].combine_first(\n",
    "    merged['bbe_pages_clean']\n",
    ")\n",
    "\n",
    "\n",
    "# 11) Consolidate GENRES\n",
    "# objects holding lists or strings\n",
    "merged['genres_final'] = merged['gb_genres_clean'].combine_first(\n",
    "    merged['bbe_genres_clean']\n",
    ")\n",
    "\n",
    "merged['genres_simple_final'] = merged['gb_genres_simplified'].combine_first(\n",
    "    merged['bbe_genres_simplified']\n",
    ")\n",
    "\n",
    "\n",
    "# 12) Consolidate DESCRIPTION\n",
    "# gb_description_clean: pandas string dtype\n",
    "# bbe_description_clean: object dtype\n",
    "merged['description_final'] = merged['gb_description_clean'].combine_first(\n",
    "    merged['bbe_description_clean']\n",
    ")\n",
    "\n",
    "merged['description_nlp_final'] = merged['gb_description_nlp'].combine_first(\n",
    "    merged['bbe_description_nlp']\n",
    ")\n",
    "\n",
    "\n",
    "# 13) Consolidate AWARDS\n",
    "# gb_has_award is bool, bbe_has_award is object (\"True\"/\"False\"/None)\n",
    "# normalize  award info\n",
    "merged['gb_has_award'] = (\n",
    "    merged['gb_has_award']\n",
    "        .replace({'True': True, 'False': False, '': None})\n",
    "        .astype('boolean')\n",
    ")\n",
    "\n",
    "merged['bbe_has_award'] = (\n",
    "    merged['bbe_has_award']\n",
    "        .replace({'True': True, 'False': False, '': None})\n",
    "        .astype('boolean')\n",
    ")\n",
    "\n",
    "# consolidate\n",
    "merged['has_award_final'] = merged['gb_has_award'].combine_first(\n",
    "    merged['bbe_has_award']\n",
    ")\n",
    "\n",
    "# fill any remaining NA with False (no award)\n",
    "merged['has_award_final'] = (\n",
    "    merged['has_award_final']\n",
    "        .fillna(False)\n",
    "        .astype('boolean')\n",
    ")\n",
    "\n",
    "# 14) Consolidate MAJOR PUBLISHER FLAG\n",
    "# gb_is_major_publisher: object (\"True\"/\"False\")\n",
    "# bbe_is_major_publisher: object\n",
    "merged['gb_is_major_publisher'] = merged['gb_is_major_publisher'].replace({\n",
    "    'True': True, 'False': False\n",
    "}).astype('boolean')\n",
    "\n",
    "merged['bbe_is_major_publisher'] = merged['bbe_is_major_publisher'].replace({\n",
    "    'True': True, 'False': False\n",
    "}).astype('boolean')\n",
    "\n",
    "merged['is_major_publisher_final'] = merged['gb_is_major_publisher'].combine_first(\n",
    "    merged['bbe_is_major_publisher']\n",
    ")\n",
    "\n",
    "\n",
    "# 15) Consolidate PRICE (BBE-only)\n",
    "merged['external_price'] = merged['bbe_price_clean']\n",
    "merged['price_flag_final'] = merged['bbe_price_flag']\n",
    "\n",
    "\n",
    "# 16) Consolidate BOOK FORMAT (BBE-only)\n",
    "merged['external_bookformat'] = merged['bbe_bookFormat_clean']\n",
    "\n",
    "# 17). Consolidate EXTERNAL RATINGS (BBE-only)\n",
    "merged['external_rating'] = merged['bbe_rating_clean']\n",
    "merged['external_numratings'] = merged['bbe_numRatings_clean']\n",
    "merged['external_votes'] = merged['bbe_votes_clean']\n",
    "merged['external_score'] = merged['bbe_score_clean']\n",
    "merged['external_likedpct'] = merged['bbe_likedPercent_clean']\n",
    "\n",
    "\n",
    "# 18) Consolidate EXTERNAL RATING DISTRIBUTIONS (BBE-only)\n",
    "bbe_rating_dist_cols = [\n",
    "    'bbe_ratings_1', 'bbe_ratings_2', 'bbe_ratings_3',\n",
    "    'bbe_ratings_4', 'bbe_ratings_5',\n",
    "    'bbe_ratings_1_share', 'bbe_ratings_2_share',\n",
    "    'bbe_ratings_3_share', 'bbe_ratings_4_share',\n",
    "    'bbe_ratings_5_share'\n",
    "]\n",
    "\n",
    "for col in bbe_rating_dist_cols:\n",
    "    merged[f'external_{col}'] = merged[col]\n",
    "\n",
    "print(\"Consolidation complete.\")\n",
    "print(\"Final columns added:\", [col for col in merged.columns if col.endswith('_final')])\n",
    "print(\"Final dataset shape:\", merged.shape)\n",
    "print(\"Dataset columns:\")\n",
    "print(merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c83855a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_warm = [\n",
    "\n",
    "    # INTERNAL LEAKAGE FIELDS\n",
    "    'gb_ratings_1', 'gb_ratings_2', 'gb_ratings_3',\n",
    "    'gb_ratings_4', 'gb_ratings_5',\n",
    "    'gb_ratings_1_share', 'gb_ratings_2_share',\n",
    "    'gb_ratings_3_share', 'gb_ratings_4_share',\n",
    "    'gb_ratings_5_share',\n",
    "    'gb_numRatings_log',\n",
    "    'gb_work_text_reviews_log',\n",
    "    'gb_work_text_reviews_count',\n",
    "    'gb_numRatings_clean',\n",
    "\n",
    "    # REDUNDANT CONSOLIDATION INPUTS\n",
    "    'gb_title_clean', 'bbe_title_clean',\n",
    "    'gb_primary_author', 'bbe_primary_author',\n",
    "    'gb_author_clean', 'bbe_author_clean',\n",
    "    'gb_authors_list', 'bbe_authors_list',\n",
    "    'gb_language_clean', 'bbe_language_clean',\n",
    "    'gb_publication_date_clean', 'bbe_publication_date_clean',\n",
    "    'gb_series_clean', 'bbe_series_clean',\n",
    "    'gb_publisher_clean', 'bbe_publisher_clean',\n",
    "    'gb_pages_clean', 'bbe_pages_clean',\n",
    "    'gb_genres_clean', 'bbe_genres_clean',\n",
    "    'gb_genres_simplified', 'bbe_genres_simplified',\n",
    "    'gb_description_clean', 'bbe_description_clean',\n",
    "    'gb_description_nlp', 'bbe_description_nlp',\n",
    "    'gb_is_major_publisher', 'bbe_is_major_publisher',\n",
    "    'gb_has_award', 'bbe_has_award',\n",
    "    'gb_isbn_clean', 'bbe_isbn_clean',\n",
    "    \n",
    "    # REDUNDANT BBE FLAGS AND FIELDS\n",
    "    'bbe_price_clean',\n",
    "    'bbe_has_likedPercent',\n",
    "    'bbe_bookFormat_clean',\n",
    "\n",
    "    # IDENTIFIERS\n",
    "    'gb_book_id',\n",
    "    'gb_best_book_id_clean',\n",
    "    'gb_work_id_clean',\n",
    "    'isbn_final',\n",
    "    'goodreads_id_clean',\n",
    "\n",
    "    # NON-PREDICTIVE CLEANING ARTIFACTS\n",
    "    'price_flag_final',\n",
    "    'bbe_price_flag', \n",
    "\n",
    "    # DUPLICATES OF external_* FEATURES → drop original BBE fields\n",
    "    'bbe_rating_clean',\n",
    "    'bbe_numRatings_clean',\n",
    "    'bbe_votes_clean',\n",
    "    'bbe_score_clean',\n",
    "    'bbe_likedPercent_clean',\n",
    "    'bbe_numRatings_log',\n",
    "    \n",
    "    # Drop original BBE rating distribution columns (now have external_* versions)\n",
    "    'bbe_ratings_1', 'bbe_ratings_2', 'bbe_ratings_3',\n",
    "    'bbe_ratings_4', 'bbe_ratings_5',\n",
    "    'bbe_ratings_1_share', 'bbe_ratings_2_share',\n",
    "    'bbe_ratings_3_share', 'bbe_ratings_4_share',\n",
    "    'bbe_ratings_5_share',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1a83b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_cold = cols_to_drop_warm + [\n",
    "\n",
    "    # REMOVE engineered external_* features\n",
    "    *[col for col in merged.columns if col.startswith('external_')],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7d60828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 72 columns for WARM START dataset.\n",
      "Final WARM START dataset shape: (9742, 34)\n",
      "Dropped 89 columns for COLD START dataset.\n",
      "Final COLD START dataset shape: (9742, 17)\n",
      "Columns in WARM START but not in COLD START:\n",
      "{'external_bbe_ratings_5', 'external_bbe_ratings_3_share', 'external_bbe_ratings_1_share', 'external_score', 'external_rating', 'external_bbe_ratings_5_share', 'external_likedpct', 'external_bbe_ratings_4_share', 'external_bbe_ratings_3', 'external_bbe_ratings_2_share', 'external_numratings', 'external_bbe_ratings_4', 'external_bbe_ratings_1', 'external_bbe_ratings_2', 'external_bookformat', 'external_votes', 'external_price'}\n"
     ]
    }
   ],
   "source": [
    "merged_clean_warm = merged.drop(columns=cols_to_drop_warm, errors='ignore')\n",
    "print(\"Dropped\", len(cols_to_drop_warm), \"columns for WARM START dataset.\")\n",
    "print(\"Final WARM START dataset shape:\", merged_clean_warm.shape)\n",
    "\n",
    "merged_clean_cold = merged.drop(columns=cols_to_drop_cold, errors='ignore')\n",
    "print(\"Dropped\", len(cols_to_drop_cold), \"columns for COLD START dataset.\")\n",
    "print(\"Final COLD START dataset shape:\", merged_clean_cold.shape)\n",
    "\n",
    "print(\"Columns in WARM START but not in COLD START:\")\n",
    "print(set(merged_clean_warm.columns) - set(merged_clean_cold.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c2d49",
   "metadata": {},
   "source": [
    "## Finalizing and Saving Datasets\n",
    "\n",
    "Both datasets use consolidated `_final` columns (title, author, pages, etc.) where Goodbooks takes precedence and BBE fills gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ec72db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARM START dataset saved: outputs\\datasets\\modeling\\model_dataset_warm_start.csv\n",
      "\n",
      "COLD START dataset saved: outputs\\datasets\\modeling\\model_dataset_cold_start.csv\n",
      "\n",
      "DATASETS SAVED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "WARM START: 9742 books, 34 features\n",
      "COLD START: 9742 books, 17 features\n",
      "\n",
      "Feature difference: 17 external signals\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# create output directory\n",
    "output_path = Path(\"outputs/datasets/modeling\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save WARM START dataset (includes external BBE signals)\n",
    "warm_file = output_path / \"model_dataset_warm_start.csv\"\n",
    "merged_clean_warm.to_csv(warm_file, index=False)\n",
    "print(f\"WARM START dataset saved: {warm_file}\")\n",
    "\n",
    "# save COLD START dataset (no external signals)\n",
    "cold_file = output_path / \"model_dataset_cold_start.csv\"\n",
    "merged_clean_cold.to_csv(cold_file, index=False)\n",
    "print(f\"\\nCOLD START dataset saved: {cold_file}\")\n",
    "\n",
    "# summary\n",
    "print(\"\\nDATASETS SAVED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWARM START: {merged_clean_warm.shape[0]} books, {merged_clean_warm.shape[1]} features\")\n",
    "print(f\"COLD START: {merged_clean_cold.shape[0]} books, {merged_clean_cold.shape[1]} features\")\n",
    "print(f\"\\nFeature difference: {merged_clean_warm.shape[1] - merged_clean_cold.shape[1]} external signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa12dc",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook successfully completed **multi-source data enrichment and dataset integration** for book satisfaction modeling.\n",
    "\n",
    "### Key Results\n",
    "\n",
    "**Enrichment Performance:**\n",
    "- Multi-source strategy: BBE overlap -> OpenLibrary API -> Google Books API\n",
    "- **95.1%** page count coverage, **100%** publication dates, **99.4%** valid languages\n",
    "- **94.7%** publisher coverage, **85.2%** description coverage\n",
    "- **90.7%** genre coverage (from 0% in original Goodbooks)\n",
    "\n",
    "**Final Datasets:**\n",
    "\n",
    "| Dataset | Location | Purpose | Records |\n",
    "|---------|----------|---------|---------|\n",
    "| `en_supply_catalog.csv` | `outputs/datasets/cleaned/` | BBE supply catalog | 47,452 |\n",
    "| `en_internal_catalog.csv` | `outputs/datasets/cleaned/` | Enriched Goodbooks | 9,940 |\n",
    "| `model_dataset_warm_start.csv` | `outputs/datasets/modeling/` | With external signals | 9,940 |\n",
    "| `model_dataset_cold_start.csv` | `outputs/datasets/modeling/` | Without external signals | 9,940 |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Notebook 04:** Exploratory analysis and genre diversity comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
