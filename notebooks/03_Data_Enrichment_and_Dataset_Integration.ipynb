{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d6a58f",
   "metadata": {},
   "source": [
    "# Data Enrichment & Dataset Integration\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The purpose of this notebook is to **enrich, align, and integrate the cleaned datasets** to create a unified analytical foundation for modelling book satisfaction and evaluating catalogue diversity.\n",
    "\n",
    "This notebook expands upon prior cleaning work by **adding missing metadata, linking overlapping records across datasets, filtering the dataset to English-language titles, and preparing a model-ready dataset** that combines catalog-level information (BBE) with user-behavioral data (Goodbooks).\n",
    "\n",
    "Ultimately, this notebook enables insights that neither dataset could provide independently, most critically, **genre diversity analysis**, **language-based consistency**, **metadata-enhanced prediction modeling**.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "| Dataset                             | Source                     | Description                                                                                         | Format |\n",
    "| ----------------------------------- | -------------------------- | --------------------------------------------------------------------------------------------------- | ------ |\n",
    "| `bbe_clean_v13.csv`                  | Output from Notebook 02    | Cleaned *Best Books Ever* metadata including title, authors, genres, rating, description, and more. | CSV    |\n",
    "| `books_clean_v7.csv`      | Output from Notebook 02    | Cleaned Goodbooks-10k metadata lacking genre data but containing structural identifiers.            | CSV    |\n",
    "| `ratings_clean_v1.csv`    | Output from Notebook 02    | User–book interaction and aggregated rating data for behavioral modeling.                           | CSV    |\n",
    "| *(Optional)* External API responses | OpenLibrary / Google Books | Supplemental metadata (genres, languages, subjects) for non-overlapping titles.                     | JSON   |\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks in This Notebook\n",
    "\n",
    "This notebook will execute the following enrichment and integration steps:\n",
    "\n",
    "1. **Standardize linking identifiers**\n",
    "   Normalize `isbn_clean`, `goodreads_id`, `title_clean`, and `author_clean` across datasets to ensure reliable cross-dataset merging.\n",
    "\n",
    "2. **Identify overlap between BBE and Goodbooks**\n",
    "   Detect books present in both datasets using multi-key matching and evaluate match quality.\n",
    "\n",
    "3. **Enrich Goodbooks metadata with missing genres**\n",
    "\n",
    "   * Use BBE genre fields for overlapping titles.\n",
    "   * Query external APIs for non-overlapping titles.\n",
    "   * Normalize all genre outputs into a unified taxonomy.\n",
    "\n",
    "4. **Complete and standardize language metadata**\n",
    "   Fill missing values using BBE, APIs, or text-based heuristics, then harmonize language labels and codes.\n",
    "\n",
    "5. **Filter the enriched datasets to English-language books**\n",
    "   Restrict the unified dataset to titles identified as **English-language**, ensuring consistency for:\n",
    "\n",
    "   * genre diversity comparisons\n",
    "   * ratings behavior\n",
    "   * regression modeling\n",
    "\n",
    "   *(Non-English titles will be kept only in the enriched BBE/Goodbooks outputs, but excluded from the model dataset.)*\n",
    "\n",
    "6. **Integrate datasets into a unified model-ready schema**\n",
    "   Combine BBE metadata with Goodbooks behavioral features for all overlapping **English-language** books.\n",
    "\n",
    "7. **Validate enrichment and filtering results**\n",
    "\n",
    "   * Assess genre and language fill rates\n",
    "   * Review API match and success metrics\n",
    "   * Log all imputation and filtering decisions for reproducibility\n",
    "\n",
    "8. **Export enriched and unified datasets**\n",
    "   Produce final English-filtered datasets ready for modeling and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **BBE_clean_enriched.csv** — enriched metadata for all BBE books\n",
    "* **Goodbooks_books_clean_enriched.csv** — enriched metadata for all Goodbooks books\n",
    "* **model_dataset_overlap_en_only.csv** — unified metadata + behavioral dataset filtered to English-language books\n",
    "* **Enrichment and filtering logs** — documenting imputation sources, API usage, and filtering decisions\n",
    "\n",
    "> **Note:** This notebook focuses on **metadata enrichment, English-language filtering, and dataset integration**. Model development and feature engineering will be performed in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09815b66",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "## Navigate to the Parent Directory\n",
    "\n",
    "Before combining and saving datasets, it’s often helpful to move to a parent directory so that file operations (like loading or saving data) are easier and more organized. \n",
    "\n",
    "Before using the Python’s built-in os module to move one level up from the current working directory, it is advisable to inspect the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f'Current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2117d",
   "metadata": {},
   "source": [
    "To change to parent directory (root folder), run the code below. If you are already in the root folder, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to its parent\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print('Changed directory to parent.')\n",
    "\n",
    "# Get the new current working directory (the parent directory)\n",
    "current_dir = os.getcwd()\n",
    "print(f'New current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4cc03",
   "metadata": {},
   "source": [
    "## Load and Inspect Datasets\n",
    "\n",
    "In this step, we load the previously cleaned datasets: **Goodbooks-10k** (books, ratings) and **Best Books Ever**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load datasets\n",
    "books_clean = pd.read_csv(\n",
    "    'data/interim/goodbooks/books_clean_v7.csv',\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    "    )\n",
    "ratings_clean = pd.read_csv('data/interim/goodbooks/ratings_clean_v1.csv')\n",
    "bbe_clean = pd.read_csv(\n",
    "    \"data/interim/bbe/bbe_clean_v13.csv\",\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    ")\n",
    "\n",
    "# create copies for imputation\n",
    "books_impute = books_clean.copy()\n",
    "ratings_impute = ratings_clean.copy()\n",
    "bbe_impute = bbe_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e14af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log samples\n",
    "print(\"BBE dataset columns:\")\n",
    "print(bbe_impute.columns.tolist())\n",
    "print(\"BBE dataset info:\")\n",
    "display(bbe_impute.info())\n",
    "print(\"BBE dataset sample:\")\n",
    "display(bbe_impute.head(3))\n",
    "\n",
    "print(\"Books dataset columns:\")\n",
    "print(books_impute.columns.tolist())\n",
    "print(\"Books dataset info:\")\n",
    "display(books_impute.info())\n",
    "print(\"Books dataset sample:\")\n",
    "display(books_impute.head(3))\n",
    "\n",
    "print(\"Ratings dataset columns:\")\n",
    "print(ratings_impute.columns.tolist())\n",
    "print(\"Ratings dataset info:\")\n",
    "display(ratings_impute.info())\n",
    "print(\"Ratings dataset sample:\")\n",
    "display(ratings_impute.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cff995",
   "metadata": {},
   "source": [
    "# Data Enrichment\n",
    "\n",
    "## Enriching Goodbooks with BBE Metadata\n",
    "\n",
    "To improve the completeness and quality of the Goodbooks-10k dataset, we selectively merge in metadata from the Best Books Ever (BBE) dataset using the shared `goodreads_id_clean` key. Goodbooks is kept as the primary source, while BBE is used to supply additional metadata fields, such as genres and page counts, as well as to fill in missing values for shared attributes like ISBN, publication date, and series.\n",
    "\n",
    "This approach ensures we enhance Goodbooks only where necessary: adding new information where it is absent and completing incomplete entries without overwriting existing data. The resulting `gb_enriched` dataset combines both sources into a more reliable and feature-rich foundation for downstream analytics and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# ENRICH GOODBOOKS (books_impute) WITH BBE DATA\n",
    "# ---------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# columns to enrich ONLY when GB has NaN\n",
    "columns_to_enrich = [\n",
    "    \"publication_date_clean\",\n",
    "    \"series_clean\",\n",
    "    \"isbn_clean\",\n",
    "    \"language_clean\"\n",
    "    ]\n",
    "\n",
    "# columns existent only in BBE\n",
    "bbe_only_columns = [\n",
    "    \"pages_clean\",\n",
    "    \"genres_clean\",\n",
    "    \"genres_simplified\"\n",
    "]\n",
    "\n",
    "# merge Goodbooks with the needed BBE columns\n",
    "merge_cols = [\"goodreads_id_clean\"] + columns_to_enrich + bbe_only_columns\n",
    "\n",
    "gb_enriched = books_impute.merge(\n",
    "    bbe_impute[merge_cols].add_suffix(\"_bbe\"),\n",
    "    left_on=\"goodreads_id_clean\",\n",
    "    right_on=\"goodreads_id_clean_bbe\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ENRICH GENRE COLUMNS\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- ENRICHING GENRES ---\")\n",
    "for col in bbe_only_columns:\n",
    "    gb_enriched[col] = gb_enriched[col + \"_bbe\"]\n",
    "    filled = gb_enriched[col].notna().sum()\n",
    "    print(f\"{col}: filled {filled} rows from BBE\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ENRICH SHARED COLUMNS ONLY WHERE GB IS NaN\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- ENRICHING SHARED COLUMNS (GB NaN → fill from BBE) ---\")\n",
    "for col in columns_to_enrich:\n",
    "    before = gb_enriched[col].isna().sum()\n",
    "    gb_enriched[col] = gb_enriched[col].fillna(gb_enriched[col + \"_bbe\"])\n",
    "    after = gb_enriched[col].isna().sum()\n",
    "    print(f\"{col}: filled {before - after} missing values\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# CLEANUP\n",
    "# ---------------------------------------------\n",
    "gb_enriched = gb_enriched.drop(columns=[c for c in gb_enriched.columns if c.endswith(\"_bbe\")])\n",
    "\n",
    "print(\"\\nEnrichment complete!\")\n",
    "print(\"Final shape:\", gb_enriched.shape)\n",
    "gb_enriched[['isbn_clean','title_clean', 'series_clean', 'genres_clean', 'genres_simplified', 'pages_clean', 'publication_date_clean']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
