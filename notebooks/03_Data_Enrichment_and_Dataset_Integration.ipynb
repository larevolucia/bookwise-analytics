{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d6a58f",
   "metadata": {},
   "source": [
    "# Data Enrichment & Dataset Integration\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The purpose of this notebook is to **enrich, align, and integrate the cleaned datasets** to create a unified analytical foundation for modelling book satisfaction and evaluating catalogue diversity.\n",
    "\n",
    "This notebook expands upon prior cleaning work by **adding missing metadata, linking overlapping records across datasets, filtering the dataset to English-language titles, and preparing a model-ready dataset** that combines catalog-level information (BBE) with user-behavioral data (Goodbooks).\n",
    "\n",
    "Ultimately, this notebook enables insights that neither dataset could provide independently, most critically, **genre diversity analysis**, **language-based consistency**, **metadata-enhanced prediction modeling**.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "| Dataset                             | Source                     | Description                                                                                         | Format |\n",
    "| ----------------------------------- | -------------------------- | --------------------------------------------------------------------------------------------------- | ------ |\n",
    "| `bbe_clean_v13.csv`                  | Output from Notebook 02    | Cleaned *Best Books Ever* metadata including title, authors, genres, rating, description, and more. | CSV    |\n",
    "| `books_clean_v7.csv`      | Output from Notebook 02    | Cleaned Goodbooks-10k metadata lacking genre data but containing structural identifiers.            | CSV    |\n",
    "| `ratings_clean_v1.csv`    | Output from Notebook 02    | User–book interaction and aggregated rating data for behavioral modeling.                           | CSV    |\n",
    "| *(Optional)* External API responses | OpenLibrary / Google Books | Supplemental metadata (genres, languages, subjects) for non-overlapping titles.                     | JSON   |\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks in This Notebook\n",
    "\n",
    "This notebook will execute the following enrichment and integration steps:\n",
    "\n",
    "1. **Standardize linking identifiers**\n",
    "   Normalize `isbn_clean`, `goodreads_id`, `title_clean`, and `author_clean` across datasets to ensure reliable cross-dataset merging.\n",
    "\n",
    "2. **Identify overlap between BBE and Goodbooks**\n",
    "   Detect books present in both datasets using multi-key matching and evaluate match quality.\n",
    "\n",
    "3. **Enrich Goodbooks metadata with missing genres**\n",
    "\n",
    "   * Use BBE genre fields for overlapping titles.\n",
    "   * Query external APIs for non-overlapping titles.\n",
    "   * Normalize all genre outputs into a unified taxonomy.\n",
    "\n",
    "4. **Complete and standardize language metadata**\n",
    "   Fill missing values using BBE, APIs, or text-based heuristics, then harmonize language labels and codes.\n",
    "\n",
    "5. **Filter the enriched datasets to English-language books**\n",
    "   Restrict the unified dataset to titles identified as **English-language**, ensuring consistency for:\n",
    "\n",
    "   * genre diversity comparisons\n",
    "   * ratings behavior\n",
    "   * regression modeling\n",
    "\n",
    "   *(Non-English titles will be kept only in the enriched BBE/Goodbooks outputs, but excluded from the model dataset.)*\n",
    "\n",
    "6. **Integrate datasets into a unified model-ready schema**\n",
    "   Combine BBE metadata with Goodbooks behavioral features for all overlapping **English-language** books.\n",
    "\n",
    "7. **Validate enrichment and filtering results**\n",
    "\n",
    "   * Assess genre and language fill rates\n",
    "   * Review API match and success metrics\n",
    "   * Log all imputation and filtering decisions for reproducibility\n",
    "\n",
    "8. **Export enriched and unified datasets**\n",
    "   Produce final English-filtered datasets ready for modeling and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **BBE_clean_enriched.csv** — enriched metadata for all BBE books\n",
    "* **Goodbooks_books_clean_enriched.csv** — enriched metadata for all Goodbooks books\n",
    "* **model_dataset_overlap_en_only.csv** — unified metadata + behavioral dataset filtered to English-language books\n",
    "* **Enrichment and filtering logs** — documenting imputation sources, API usage, and filtering decisions\n",
    "\n",
    "> **Note:** This notebook focuses on **metadata enrichment, English-language filtering, and dataset integration**. Model development and feature engineering will be performed in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09815b66",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "## Navigate to the Parent Directory\n",
    "\n",
    "Before combining and saving datasets, it’s often helpful to move to a parent directory so that file operations (like loading or saving data) are easier and more organized. \n",
    "\n",
    "Before using the Python’s built-in os module to move one level up from the current working directory, it is advisable to inspect the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19d6dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\reisl\\OneDrive\\Documents\\GitHub\\bookwise-analytics\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f'Current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2117d",
   "metadata": {},
   "source": [
    "To change to parent directory (root folder), run the code below. If you are already in the root folder, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fc58fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to parent.\n",
      "New current directory: c:\\Users\\reisl\\OneDrive\\Documents\\GitHub\\bookwise-analytics\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory to its parent\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print('Changed directory to parent.')\n",
    "\n",
    "# Get the new current working directory (the parent directory)\n",
    "current_dir = os.getcwd()\n",
    "print(f'New current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cleaning.utils.categories import (\n",
    "    map_subjects_to_genres\n",
    ")\n",
    "from src.cleaning.utils.pipeline import apply_cleaners_selectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b908b97",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install additional packages for this notebook\n",
    "! pip install requests python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4cc03",
   "metadata": {},
   "source": [
    "## Load and Inspect Datasets\n",
    "\n",
    "In this step, we load the previously cleaned datasets: **Goodbooks-10k** (books, ratings) and **Best Books Ever**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "770d789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBE dataset columns:\n",
      "['goodreads_id_clean', 'authors_list', 'author_clean', 'title_clean', 'isbn_clean', 'language_clean', 'publication_date_clean', 'publisher_clean', 'is_major_publisher', 'bookFormat_clean', 'rating_clean', 'numRatings_clean', 'numRatings_log', 'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5', 'ratings_1_share', 'ratings_2_share', 'ratings_3_share', 'ratings_4_share', 'ratings_5_share', 'has_award', 'genres_clean', 'genres_simplified', 'description_clean', 'description_nlp', 'series_clean', 'pages_clean', 'bbeVotes_clean', 'bbeScore_clean', 'likedPercent_clean', 'has_likedPercent', 'price_clean', 'price_flag']\n",
      "BBE dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52424 entries, 0 to 52423\n",
      "Data columns (total 36 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   goodreads_id_clean      52424 non-null  string \n",
      " 1   authors_list            52424 non-null  object \n",
      " 2   author_clean            52424 non-null  object \n",
      " 3   title_clean             52424 non-null  object \n",
      " 4   isbn_clean              43338 non-null  string \n",
      " 5   language_clean          48413 non-null  object \n",
      " 6   publication_date_clean  46983 non-null  object \n",
      " 7   publisher_clean         48725 non-null  object \n",
      " 8   is_major_publisher      52424 non-null  bool   \n",
      " 9   bookFormat_clean        52424 non-null  object \n",
      " 10  rating_clean            52353 non-null  float64\n",
      " 11  numRatings_clean        52424 non-null  int64  \n",
      " 12  numRatings_log          52424 non-null  float64\n",
      " 13  ratings_1               52353 non-null  float64\n",
      " 14  ratings_2               52353 non-null  float64\n",
      " 15  ratings_3               52353 non-null  float64\n",
      " 16  ratings_4               52353 non-null  float64\n",
      " 17  ratings_5               52353 non-null  float64\n",
      " 18  ratings_1_share         52353 non-null  float64\n",
      " 19  ratings_2_share         52353 non-null  float64\n",
      " 20  ratings_3_share         52353 non-null  float64\n",
      " 21  ratings_4_share         52353 non-null  float64\n",
      " 22  ratings_5_share         52353 non-null  float64\n",
      " 23  has_award               52424 non-null  bool   \n",
      " 24  genres_clean            47804 non-null  object \n",
      " 25  genres_simplified       52424 non-null  object \n",
      " 26  description_clean       50268 non-null  object \n",
      " 27  description_nlp         50268 non-null  object \n",
      " 28  series_clean            23441 non-null  object \n",
      " 29  pages_clean             49747 non-null  float64\n",
      " 30  bbeVotes_clean          52424 non-null  int64  \n",
      " 31  bbeScore_clean          52424 non-null  int64  \n",
      " 32  likedPercent_clean      51803 non-null  float64\n",
      " 33  has_likedPercent        52424 non-null  int64  \n",
      " 34  price_clean             38068 non-null  float64\n",
      " 35  price_flag              52424 non-null  bool   \n",
      "dtypes: bool(3), float64(15), int64(4), object(12), string(2)\n",
      "memory usage: 13.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBE dataset sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goodreads_id_clean</th>\n",
       "      <th>authors_list</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>isbn_clean</th>\n",
       "      <th>language_clean</th>\n",
       "      <th>publication_date_clean</th>\n",
       "      <th>publisher_clean</th>\n",
       "      <th>is_major_publisher</th>\n",
       "      <th>bookFormat_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>description_nlp</th>\n",
       "      <th>series_clean</th>\n",
       "      <th>pages_clean</th>\n",
       "      <th>bbeVotes_clean</th>\n",
       "      <th>bbeScore_clean</th>\n",
       "      <th>likedPercent_clean</th>\n",
       "      <th>has_likedPercent</th>\n",
       "      <th>price_clean</th>\n",
       "      <th>price_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2767052</td>\n",
       "      <td>['suzanne collins']</td>\n",
       "      <td>suzanne collins</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>9780439023481</td>\n",
       "      <td>en</td>\n",
       "      <td>2008-09-14</td>\n",
       "      <td>scholastic</td>\n",
       "      <td>True</td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>winning means fame and fortunelosing means cer...</td>\n",
       "      <td>winning means fame and fortunelosing means cer...</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>374.0</td>\n",
       "      <td>30516</td>\n",
       "      <td>2993816</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['jk rowling', 'mary grandpre']</td>\n",
       "      <td>jk rowling, mary grandpre</td>\n",
       "      <td>harry potter and the order of the phoenix</td>\n",
       "      <td>9780439358071</td>\n",
       "      <td>en</td>\n",
       "      <td>2003-06-21</td>\n",
       "      <td>scholastic</td>\n",
       "      <td>True</td>\n",
       "      <td>paperback</td>\n",
       "      <td>...</td>\n",
       "      <td>there is a door at the end of a silent corrido...</td>\n",
       "      <td>there is a door at the end of a silent corrido...</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>870.0</td>\n",
       "      <td>26923</td>\n",
       "      <td>2632233</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.38</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2657</td>\n",
       "      <td>['harper lee']</td>\n",
       "      <td>harper lee</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>harpercollins</td>\n",
       "      <td>True</td>\n",
       "      <td>paperback</td>\n",
       "      <td>...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>324.0</td>\n",
       "      <td>23328</td>\n",
       "      <td>2269402</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  goodreads_id_clean                     authors_list  \\\n",
       "0            2767052              ['suzanne collins']   \n",
       "1                  2  ['jk rowling', 'mary grandpre']   \n",
       "2               2657                   ['harper lee']   \n",
       "\n",
       "                author_clean                                title_clean  \\\n",
       "0            suzanne collins                           the hunger games   \n",
       "1  jk rowling, mary grandpre  harry potter and the order of the phoenix   \n",
       "2                 harper lee                      to kill a mockingbird   \n",
       "\n",
       "      isbn_clean language_clean publication_date_clean publisher_clean  \\\n",
       "0  9780439023481             en             2008-09-14      scholastic   \n",
       "1  9780439358071             en             2003-06-21      scholastic   \n",
       "2           <NA>             en                    NaN   harpercollins   \n",
       "\n",
       "   is_major_publisher bookFormat_clean  ...  \\\n",
       "0                True        hardcover  ...   \n",
       "1                True        paperback  ...   \n",
       "2                True        paperback  ...   \n",
       "\n",
       "                                   description_clean  \\\n",
       "0  winning means fame and fortunelosing means cer...   \n",
       "1  there is a door at the end of a silent corrido...   \n",
       "2  the unforgettable novel of a childhood in a sl...   \n",
       "\n",
       "                                     description_nlp           series_clean  \\\n",
       "0  winning means fame and fortunelosing means cer...       the hunger games   \n",
       "1  there is a door at the end of a silent corrido...           harry potter   \n",
       "2  the unforgettable novel of a childhood in a sl...  to kill a mockingbird   \n",
       "\n",
       "   pages_clean  bbeVotes_clean  bbeScore_clean  likedPercent_clean  \\\n",
       "0        374.0           30516         2993816                96.0   \n",
       "1        870.0           26923         2632233                98.0   \n",
       "2        324.0           23328         2269402                95.0   \n",
       "\n",
       "   has_likedPercent  price_clean  price_flag  \n",
       "0                 1         5.09       False  \n",
       "1                 1         7.38       False  \n",
       "2                 1          NaN        True  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books dataset columns:\n",
      "['book_id', 'work_text_reviews_count', 'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5', 'goodreads_id_clean', 'best_book_id_clean', 'work_id_clean', 'authors_list', 'author_clean', 'language_clean', 'publication_date_clean', 'isbn_clean', 'isbn13_clean', 'rating_clean', 'numRatings_clean', 'numRatings_log', 'ratings_1_share', 'ratings_2_share', 'ratings_3_share', 'ratings_4_share', 'ratings_5_share', 'work_text_reviews_log', 'series_clean', 'title_clean']\n",
      "Books dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 27 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   book_id                  10000 non-null  int64  \n",
      " 1   work_text_reviews_count  10000 non-null  int64  \n",
      " 2   ratings_1                10000 non-null  int64  \n",
      " 3   ratings_2                10000 non-null  int64  \n",
      " 4   ratings_3                10000 non-null  int64  \n",
      " 5   ratings_4                10000 non-null  int64  \n",
      " 6   ratings_5                10000 non-null  int64  \n",
      " 7   goodreads_id_clean       10000 non-null  string \n",
      " 8   best_book_id_clean       10000 non-null  int64  \n",
      " 9   work_id_clean            10000 non-null  int64  \n",
      " 10  authors_list             10000 non-null  object \n",
      " 11  author_clean             10000 non-null  object \n",
      " 12  language_clean           8916 non-null   object \n",
      " 13  publication_date_clean   9887 non-null   object \n",
      " 14  isbn_clean               8251 non-null   string \n",
      " 15  isbn13_clean             9415 non-null   float64\n",
      " 16  rating_clean             10000 non-null  float64\n",
      " 17  numRatings_clean         10000 non-null  int64  \n",
      " 18  numRatings_log           10000 non-null  float64\n",
      " 19  ratings_1_share          10000 non-null  float64\n",
      " 20  ratings_2_share          10000 non-null  float64\n",
      " 21  ratings_3_share          10000 non-null  float64\n",
      " 22  ratings_4_share          10000 non-null  float64\n",
      " 23  ratings_5_share          10000 non-null  float64\n",
      " 24  work_text_reviews_log    10000 non-null  float64\n",
      " 25  series_clean             3947 non-null   object \n",
      " 26  title_clean              10000 non-null  object \n",
      "dtypes: float64(9), int64(10), object(6), string(2)\n",
      "memory usage: 2.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books dataset sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>work_text_reviews_count</th>\n",
       "      <th>ratings_1</th>\n",
       "      <th>ratings_2</th>\n",
       "      <th>ratings_3</th>\n",
       "      <th>ratings_4</th>\n",
       "      <th>ratings_5</th>\n",
       "      <th>goodreads_id_clean</th>\n",
       "      <th>best_book_id_clean</th>\n",
       "      <th>work_id_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>numRatings_clean</th>\n",
       "      <th>numRatings_log</th>\n",
       "      <th>ratings_1_share</th>\n",
       "      <th>ratings_2_share</th>\n",
       "      <th>ratings_3_share</th>\n",
       "      <th>ratings_4_share</th>\n",
       "      <th>ratings_5_share</th>\n",
       "      <th>work_text_reviews_log</th>\n",
       "      <th>series_clean</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>155254</td>\n",
       "      <td>66715</td>\n",
       "      <td>127936</td>\n",
       "      <td>560092</td>\n",
       "      <td>1481305</td>\n",
       "      <td>2706317</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2767052</td>\n",
       "      <td>2792775</td>\n",
       "      <td>...</td>\n",
       "      <td>4942365</td>\n",
       "      <td>15.413355</td>\n",
       "      <td>0.013499</td>\n",
       "      <td>0.025886</td>\n",
       "      <td>0.113325</td>\n",
       "      <td>0.299716</td>\n",
       "      <td>0.547575</td>\n",
       "      <td>11.952824</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>the hunger games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>75867</td>\n",
       "      <td>75504</td>\n",
       "      <td>101676</td>\n",
       "      <td>455024</td>\n",
       "      <td>1156318</td>\n",
       "      <td>3011543</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4640799</td>\n",
       "      <td>...</td>\n",
       "      <td>4800065</td>\n",
       "      <td>15.384140</td>\n",
       "      <td>0.015730</td>\n",
       "      <td>0.021182</td>\n",
       "      <td>0.094795</td>\n",
       "      <td>0.240896</td>\n",
       "      <td>0.627396</td>\n",
       "      <td>11.236750</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>harry potter and the sorcerer's stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>95009</td>\n",
       "      <td>456191</td>\n",
       "      <td>436802</td>\n",
       "      <td>793319</td>\n",
       "      <td>875073</td>\n",
       "      <td>1355439</td>\n",
       "      <td>41865</td>\n",
       "      <td>41865</td>\n",
       "      <td>3212258</td>\n",
       "      <td>...</td>\n",
       "      <td>3916824</td>\n",
       "      <td>15.180792</td>\n",
       "      <td>0.116470</td>\n",
       "      <td>0.111519</td>\n",
       "      <td>0.202541</td>\n",
       "      <td>0.223414</td>\n",
       "      <td>0.346056</td>\n",
       "      <td>11.461737</td>\n",
       "      <td>twilight</td>\n",
       "      <td>twilight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id  work_text_reviews_count  ratings_1  ratings_2  ratings_3  \\\n",
       "0        1                   155254      66715     127936     560092   \n",
       "1        2                    75867      75504     101676     455024   \n",
       "2        3                    95009     456191     436802     793319   \n",
       "\n",
       "   ratings_4  ratings_5 goodreads_id_clean  best_book_id_clean  work_id_clean  \\\n",
       "0    1481305    2706317            2767052             2767052        2792775   \n",
       "1    1156318    3011543                  3                   3        4640799   \n",
       "2     875073    1355439              41865               41865        3212258   \n",
       "\n",
       "   ... numRatings_clean numRatings_log ratings_1_share ratings_2_share  \\\n",
       "0  ...          4942365      15.413355        0.013499        0.025886   \n",
       "1  ...          4800065      15.384140        0.015730        0.021182   \n",
       "2  ...          3916824      15.180792        0.116470        0.111519   \n",
       "\n",
       "  ratings_3_share  ratings_4_share  ratings_5_share  work_text_reviews_log  \\\n",
       "0        0.113325         0.299716         0.547575              11.952824   \n",
       "1        0.094795         0.240896         0.627396              11.236750   \n",
       "2        0.202541         0.223414         0.346056              11.461737   \n",
       "\n",
       "       series_clean                            title_clean  \n",
       "0  the hunger games                       the hunger games  \n",
       "1      harry potter  harry potter and the sorcerer's stone  \n",
       "2          twilight                               twilight  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings dataset columns:\n",
      "['user_id', 'book_id', 'rating']\n",
      "Ratings dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5976479 entries, 0 to 5976478\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Dtype\n",
      "---  ------   -----\n",
      " 0   user_id  int64\n",
      " 1   book_id  int64\n",
      " 2   rating   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 136.8 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings dataset sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4081</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id  rating\n",
       "0        1      258       5\n",
       "1        2     4081       4\n",
       "2        2      260       5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load datasets\n",
    "books_clean = pd.read_csv(\n",
    "    'data/interim/goodbooks/books_clean_v7.csv',\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    "    )\n",
    "ratings_clean = pd.read_csv('data/interim/goodbooks/ratings_clean_v1.csv')\n",
    "bbe_clean = pd.read_csv(\n",
    "    \"data/interim/bbe/bbe_clean_v13.csv\",\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    ")\n",
    "\n",
    "# create copies for imputation\n",
    "books_impute = books_clean.copy()\n",
    "ratings_impute = ratings_clean.copy()\n",
    "bbe_impute = bbe_clean.copy()\n",
    "\n",
    "# log samples\n",
    "print(\"BBE dataset columns:\")\n",
    "print(bbe_impute.columns.tolist())\n",
    "print(\"BBE dataset info:\")\n",
    "display(bbe_impute.info())\n",
    "print(\"BBE dataset sample:\")\n",
    "display(bbe_impute.head(3))\n",
    "\n",
    "print(\"Books dataset columns:\")\n",
    "print(books_impute.columns.tolist())\n",
    "print(\"Books dataset info:\")\n",
    "display(books_impute.info())\n",
    "print(\"Books dataset sample:\")\n",
    "display(books_impute.head(3))\n",
    "\n",
    "print(\"Ratings dataset columns:\")\n",
    "print(ratings_impute.columns.tolist())\n",
    "print(\"Ratings dataset info:\")\n",
    "display(ratings_impute.info())\n",
    "print(\"Ratings dataset sample:\")\n",
    "display(ratings_impute.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cff995",
   "metadata": {},
   "source": [
    "# Data Enrichment\n",
    "\n",
    "## Enriching Goodbooks\n",
    "\n",
    "### From BBE overlap\n",
    "\n",
    "To improve the completeness and quality of the Goodbooks-10k dataset, we selectively merge in metadata from the Best Books Ever (BBE) dataset using the shared `goodreads_id_clean` key. Goodbooks is kept as the primary source, while BBE is used to supply additional metadata fields, such as genres and page counts, as well as to fill in missing values for shared attributes like ISBN, publication date, and series.\n",
    "\n",
    "This approach ensures we enhance Goodbooks only where necessary: adding new information where it is absent and completing incomplete entries without overwriting existing data. The resulting `gb_enriched` dataset combines both sources into a more reliable and feature-rich foundation for downstream analytics and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# ENRICH GOODBOOKS (books_impute) WITH BBE DATA\n",
    "# ---------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# columns to enrich ONLY when GB has NaN\n",
    "columns_to_enrich = [\n",
    "    \"publication_date_clean\",\n",
    "    \"series_clean\",\n",
    "    \"isbn_clean\",\n",
    "    \"language_clean\"\n",
    "    ]\n",
    "\n",
    "# columns existent only in BBE\n",
    "bbe_only_columns = [\n",
    "    \"pages_clean\",\n",
    "    \"genres_clean\",\n",
    "    \"genres_simplified\",\n",
    "    \"publisher_clean\",\n",
    "    \"is_major_publisher\",\n",
    "    \"has_award\",\n",
    "    \"description_clean\",\n",
    "    \"description_nlp\"\n",
    "]\n",
    "\n",
    "# merge Goodbooks with the needed BBE columns\n",
    "merge_cols = [\"goodreads_id_clean\"] + columns_to_enrich + bbe_only_columns\n",
    "\n",
    "gb_enriched = books_impute.merge(\n",
    "    bbe_impute[merge_cols].add_suffix(\"_bbe\"),\n",
    "    left_on=\"goodreads_id_clean\",\n",
    "    right_on=\"goodreads_id_clean_bbe\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ENRICH GENRE COLUMNS\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- ENRICHING METADATA ---\")\n",
    "for col in bbe_only_columns:\n",
    "    gb_enriched[col] = gb_enriched[col + \"_bbe\"]\n",
    "    filled = gb_enriched[col].notna().sum()\n",
    "    print(f\"{col}: filled {filled} rows from BBE\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ENRICH SHARED COLUMNS ONLY WHERE GB IS NaN\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- ENRICHING SHARED COLUMNS (GB NaN -> fill from BBE) ---\")\n",
    "for col in columns_to_enrich:\n",
    "    before = gb_enriched[col].isna().sum()\n",
    "    gb_enriched[col] = gb_enriched[col].fillna(gb_enriched[col + \"_bbe\"])\n",
    "    after = gb_enriched[col].isna().sum()\n",
    "    print(f\"{col}: filled {before - after} missing values\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# CLEANUP\n",
    "# ---------------------------------------------\n",
    "gb_enriched = gb_enriched.drop(columns=[c for c in gb_enriched.columns if c.endswith(\"_bbe\")])\n",
    "\n",
    "print(\"\\nEnrichment complete!\")\n",
    "print(\"Final shape:\", gb_enriched.shape)\n",
    "gb_enriched[['isbn_clean','title_clean', 'series_clean', 'genres_clean', 'genres_simplified', 'pages_clean', 'publication_date_clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d474378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 1\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ead17c",
   "metadata": {},
   "source": [
    "### From external APIs\n",
    "\n",
    "To further enrich the Goodbooks-10k dataset, we leverage external APIs such as OpenLibrary and Google Books to fill in missing metadata for titles not covered by the BBE overlap. This process involves querying these APIs using available identifiers (like ISBN or title/author combinations) to retrieve additional information such as genres, page counts, and publication details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_isbn(isbn):\n",
    "    if not isinstance(isbn, str):\n",
    "        return None\n",
    "    isbn = re.sub(r'[^0-9Xx]', '', isbn)\n",
    "    if len(isbn) in [10, 13]:\n",
    "        return isbn\n",
    "    return None\n",
    "\n",
    "gb_enriched['isbn_query'] = gb_enriched['isbn_clean'].apply(clean_isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee431a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mask = (\n",
    "    gb_enriched['language_clean'].isna() |\n",
    "    gb_enriched['language_clean'].isin(['unknown', '', 'None']) |\n",
    "    gb_enriched['pages_clean'].isna() |\n",
    "    gb_enriched['publication_date_clean'].isna()  |\n",
    "    gb_enriched['publisher_clean'].isna() |\n",
    "    gb_enriched['description_clean'].isna()\n",
    ")\n",
    "\n",
    "to_impute = gb_enriched[missing_mask].copy()\n",
    "print(\"Books needing external enrichment:\", len(to_impute))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ab95c",
   "metadata": {},
   "source": [
    "#### Querying OpenLibrary API\n",
    "\n",
    "After enriching Goodbooks with BBE overlap data, we identify **2,249** books still missing critical metadata (ISBN, language, pages, publication date, publisher). We query **OpenLibrary first** because it has no rate limits or API key requirements, making it ideal for bulk enrichment. We create a boolean mask to identify books needing enrichment, then query OpenLibrary's ISBN endpoint for each book, collecting results in a structured format.\n",
    "\n",
    "The results are merged back into `gb_enriched` and saved as **version 2**. This incremental saving strategy ensures we don't lose progress if subsequent API calls fail or exceed quotas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b963e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# cache path for OpenLibrary in data/raw\n",
    "OL_CACHE_PATH = Path(\"data/raw/openlibrary_api_cache.json\")\n",
    "\n",
    "# create directory if it doesn't exist\n",
    "OL_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load existing cache if it exists\n",
    "if OL_CACHE_PATH.exists():\n",
    "    with open(OL_CACHE_PATH, \"r\") as f:\n",
    "        ol_cache = json.load(f)\n",
    "    print(f\"Loaded {len(ol_cache)} cached OpenLibrary entries\")\n",
    "else:\n",
    "    ol_cache = {}\n",
    "    print(\"No existing cache found, starting fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5278cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def query_openlibrary(isbn):\n",
    "    \"\"\"Return OL metadata in a consistent dict format.\"\"\"\n",
    "\n",
    "    isbn_str = str(isbn)\n",
    "    \n",
    "    if isbn_str in ol_cache:\n",
    "        return ol_cache[isbn_str]\n",
    "    \n",
    "    # Default structure to guarantee stable DataFrame columns\n",
    "    result = {\n",
    "        \"pages_openlib\": None,\n",
    "        \"publication_date_openlib\": None,\n",
    "        \"language_openlib\": None,\n",
    "        \"subjects_openlib\": None,\n",
    "        \"publisher_openlib\": None, \n",
    "        \"description_openlib\": None, \n",
    "    }\n",
    "\n",
    "    if isbn is None or pd.isna(isbn) or isbn == \"\":\n",
    "        return result\n",
    "    \n",
    "    url = f\"https://openlibrary.org/isbn/{isbn}.json\"\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return result\n",
    "\n",
    "        data = r.json()\n",
    "\n",
    "        # Pages\n",
    "        result[\"pages_openlib\"] = data.get(\"number_of_pages\")\n",
    "\n",
    "        # Publication date\n",
    "        result[\"publication_date_openlib\"] = data.get(\"publish_date\")\n",
    "\n",
    "        # Language\n",
    "        if \"languages\" in data and isinstance(data[\"languages\"], list):\n",
    "            key = data[\"languages\"][0].get(\"key\", \"\").split(\"/\")[-1]\n",
    "            result[\"language_openlib\"] = key\n",
    "\n",
    "        # Subjects\n",
    "        if \"subjects\" in data:\n",
    "            result[\"subjects_openlib\"] = [s.lower() for s in data[\"subjects\"]]\n",
    "        \n",
    "        # Publisher\n",
    "        if \"publishers\" in data and isinstance(data[\"publishers\"], list):\n",
    "            result[\"publisher_openlib\"] = data[\"publishers\"][0]\n",
    "        \n",
    "        # Description\n",
    "        desc = data.get(\"description\")\n",
    "        if isinstance(desc, dict):\n",
    "            result[\"description_openlib\"] = desc.get(\"value\")\n",
    "        elif isinstance(desc, str):\n",
    "            result[\"description_openlib\"] = desc\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        pass  # keep the default result structure\n",
    "\n",
    "    # Save to cache\n",
    "    ol_cache[isbn_str] = result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for isbn in tqdm(to_impute['isbn_query'], desc=\"Querying OpenLibrary\"):\n",
    "    results.append(query_openlibrary(isbn))\n",
    "    time.sleep(0.2)   # safe rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Save OpenLibrary cache after queries\n",
    "with open(OL_CACHE_PATH, \"w\") as f:\n",
    "    json.dump(ol_cache, f, indent=2)\n",
    "print(f\"OpenLibrary cache saved with {len(ol_cache)} entries\")\n",
    "\n",
    "# convert results to dataframe\n",
    "ol_df = pd.DataFrame(results, index=to_impute.index)\n",
    "print(\"API results summary:\")\n",
    "print(ol_df.notna().sum())\n",
    "\n",
    "# merge back into gb_enriched\n",
    "for col in ol_df.columns:\n",
    "    if col not in gb_enriched.columns:\n",
    "        gb_enriched[col] = None\n",
    "    gb_enriched.loc[ol_df.index, col] = ol_df[col]\n",
    "\n",
    "# verify the merge\n",
    "print(\"\\nAfter merge:\")\n",
    "print(gb_enriched[ol_df.columns].notna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 2\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a12d1a",
   "metadata": {},
   "source": [
    "#### Cleaning and Processing OpenLibrary Data\n",
    "\n",
    "We apply the same cleaning steps used in Notebook 02, compiled into a pipeline, to standardize OpenLibrary API responses. The `apply_cleaners_selectively()` function ensures consistent data types, formats, and validation across all metadata fields. After cleaning, we fill missing values in `gb_enriched` using the cleaned OpenLibrary data.\n",
    "\n",
    "For genre enrichment, we map OpenLibrary subjects to our standardized genre taxonomy using `map_subjects_to_genres()`. This populates `genres_simplified` for books that had subjects but no genre data, significantly improving genre coverage. The enriched dataset is saved as **version 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a19072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean OpenLibrary API data\n",
    "gb_enriched = apply_cleaners_selectively(\n",
    "    gb_enriched,\n",
    "    fields_to_clean=[\n",
    "        'pages',\n",
    "        'publication_date',\n",
    "        'language',\n",
    "        'subjects',\n",
    "        'publisher',\n",
    "        'description'\n",
    "        ],\n",
    "    source_suffix='_openlib',\n",
    "    target_suffix='_openlib_clean',\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# verify cleaning\n",
    "print(\"\\nSample of cleaned OpenLibrary data:\")\n",
    "gb_enriched[[\n",
    "    'title_clean',\n",
    "    'pages_clean',\n",
    "    'pages_openlib',\n",
    "    'pages_openlib_clean',\n",
    "    'publication_date_clean',\n",
    "    'publication_date_openlib',\n",
    "    'publication_date_openlib_clean',\n",
    "    'language_clean',\n",
    "    'language_openlib',\n",
    "    'language_openlib_clean',\n",
    "    'genres_clean',\n",
    "    'genres_simplified',\n",
    "    'subjects_openlib',\n",
    "    'subjects_openlib_clean',\n",
    "    'publisher_clean',\n",
    "    'description_openlib',\n",
    "    'description_clean',\n",
    "    'description_openlib',\n",
    "    'description_openlib_clean'\n",
    "    ]].sample(15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with cleaned OpenLibrary data\n",
    "print(\"\\n--- Filling missing values with cleaned OpenLibrary data ---\")\n",
    "\n",
    "# fill pages_clean\n",
    "before_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "gb_enriched['pages_clean'] = gb_enriched['pages_clean'].fillna(gb_enriched['pages_openlib_clean'])\n",
    "after_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "print(f\"pages_clean: filled {before_pages - after_pages} values\")\n",
    "\n",
    "# fill publication_date_clean\n",
    "before_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "gb_enriched['publication_date_clean'] = gb_enriched['publication_date_clean'].fillna(gb_enriched['publication_date_openlib_clean'])\n",
    "after_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "print(f\"publication_date_clean: filled {before_date - after_date} values\")\n",
    "\n",
    "# fill language_clean\n",
    "# Create mask that catches both NaN and invalid string values\n",
    "before_lang = (gb_enriched['language_clean'].isna() | \n",
    "               gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "mask = (gb_enriched['language_clean'].isna() | \n",
    "        gb_enriched['language_clean'].isin(['unknown', '', 'None']))\n",
    "\n",
    "gb_enriched.loc[mask, 'language_clean'] = gb_enriched.loc[mask, 'language_openlib_clean']\n",
    "\n",
    "after_lang = (gb_enriched['language_clean'].isna() | \n",
    "              gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "print(f\"language_clean: filled {before_lang - after_lang} values\")\n",
    "\n",
    "# fill publisher_clean\n",
    "print(\"\\n--- Filling missing publisher_clean using OpenLibrary data ---\")\n",
    "before_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "gb_enriched['publisher_clean'] = gb_enriched['publisher_clean'].fillna(\n",
    "    gb_enriched['publisher_openlib_clean']\n",
    ")\n",
    "after_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "print(f\"publisher_clean: filled {before_publisher - after_publisher} values\")\n",
    "\n",
    "# fill description_clean\n",
    "print(\"\\n--- Filling missing description_clean using OpenLibrary data ---\")\n",
    "\n",
    "before_desc = gb_enriched['description_clean'].isna().sum()\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].fillna(\n",
    "    gb_enriched['description_openlib_clean']\n",
    ")\n",
    "after_desc = gb_enriched['description_clean'].isna().sum()\n",
    "print(f\"description_clean: filled {before_desc - after_desc} values\")\n",
    "\n",
    "# generate genres_simplified from subjects_openlib_clean\n",
    "print(\"\\n--- Generating genres_simplified from OpenLibrary subjects ---\")\n",
    "\n",
    "# Import genre mapping utilities\n",
    "\n",
    "# Fill genres_simplified for books that have subjects but no genres\n",
    "books_needing_genre_mapping = (\n",
    "    (gb_enriched['genres_simplified'].isna()) & \n",
    "    (gb_enriched['subjects_openlib_clean'].notna())\n",
    ")\n",
    "\n",
    "print(f\"Books with subjects but no genres_simplified: {books_needing_genre_mapping.sum()}\")\n",
    "\n",
    "if books_needing_genre_mapping.sum() > 0:\n",
    "    # Apply genre mapping to subjects\n",
    "    gb_enriched.loc[books_needing_genre_mapping, 'genres_simplified'] = (\n",
    "        gb_enriched.loc[books_needing_genre_mapping, 'subjects_openlib_clean']\n",
    "        .apply(lambda x: map_subjects_to_genres(x) if isinstance(x, list) else None)\n",
    "    )\n",
    "    \n",
    "    filled_genres = (\n",
    "        gb_enriched.loc[books_needing_genre_mapping, 'genres_simplified'].notna().sum()\n",
    "    )\n",
    "    print(f\"genres_simplified: mapped {filled_genres} values from OpenLibrary subjects\")\n",
    "    \n",
    "    # Show sample of newly mapped genres\n",
    "    newly_mapped = gb_enriched[books_needing_genre_mapping & gb_enriched['genres_simplified'].notna()]\n",
    "    if len(newly_mapped) > 0:\n",
    "        print(\"\\nSample of newly mapped genres:\")\n",
    "        print(newly_mapped[['title_clean', 'subjects_openlib_clean', 'genres_simplified']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e72318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- ENRICHMENT SUMMARY ---\")\n",
    "\n",
    "# Show books that received OpenLibrary data\n",
    "books_with_ol_data = gb_enriched[\n",
    "    gb_enriched['subjects_openlib_clean'].notna()\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nTotal books enriched with OpenLibrary data: {len(books_with_ol_data)}\")\n",
    "\n",
    "if len(books_with_ol_data) > 0:\n",
    "    print(\"\\nSample of books enriched from OpenLibrary:\")\n",
    "    display(books_with_ol_data[[\n",
    "        'title_clean',\n",
    "        'author_clean',\n",
    "        'pages_openlib_clean',\n",
    "        'publication_date_openlib_clean',\n",
    "        'language_openlib_clean',\n",
    "        'subjects_openlib_clean',\n",
    "        'genres_simplified',\n",
    "        'publisher_clean',\n",
    "        'description_clean'\n",
    "    ]].head(10))\n",
    "\n",
    "# Show overall genre coverage\n",
    "print(f\"\\n--- GENRE COVERAGE AFTER ENRICHMENT ---\")\n",
    "print(f\"Books with genres_clean: {gb_enriched['genres_clean'].notna().sum()}\")\n",
    "print(f\"Books without genres_clean: {gb_enriched['genres_clean'].isna().sum()}\")\n",
    "print(f\"Genre coverage: {gb_enriched['genres_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\\n\")\n",
    "print(f\"Books with genres_simplified: {gb_enriched['genres_simplified'].notna().sum()}\")\n",
    "print(f\"Books without genres_simplified: {gb_enriched['genres_simplified'].isna().sum()}\")\n",
    "print(f\"Genre simplified coverage: {gb_enriched['genres_simplified'].notna().sum() / len(gb_enriched) * 100:.1f}%\")\n",
    "# Show overall description and publisher coverage\n",
    "print(f\"Books with description_clean: {gb_enriched['description_clean'].notna().sum()}\")\n",
    "print(f\"Books without description_clean: {gb_enriched['description_clean'].isna().sum()}\")\n",
    "print(f\"Books description coverage: {gb_enriched['description_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\")\n",
    "print(f\"Books with publisher_clean: {gb_enriched['publisher_clean'].notna().sum()}\")\n",
    "print(f\"Books without publisher_clean: {gb_enriched['publisher_clean'].isna().sum()}\")\n",
    "print(f\"Books publisher coverage: {gb_enriched['publisher_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e647000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 3\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab0e7c",
   "metadata": {},
   "source": [
    "#### Querying Google Books API (with Quota Management)\n",
    "\n",
    "After OpenLibrary enrichment, we create a new mask to identify remaining gaps: **1,730** books. Google Books API requires an API key and has daily quota limits (1,000 requests/day for free tier), so we implement several strategies: **(1)** process ISBNs in chunks of 1,000, **(2)** add sleep delays between requests, **(3)** cache all results to avoid re-querying, and **(4)** save progress incrementally.\n",
    "\n",
    "We load existing cache if available, query only uncached ISBNs, and update the cache after each session. This approach allows us to spread queries across multiple days if needed while preserving all previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3991bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many books still need enrichment\n",
    "new_missing_mask = (\n",
    "    gb_enriched['language_clean'].isna() |\n",
    "    gb_enriched['language_clean'].isin(['unknown', '', 'None']) |\n",
    "    gb_enriched['pages_clean'].isna() |\n",
    "    gb_enriched['publication_date_clean'].isna() | \n",
    "    gb_enriched['publisher_clean'].isna() |\n",
    "    gb_enriched['description_clean'].isna()\n",
    ")\n",
    "\n",
    "new_to_impute = gb_enriched[new_missing_mask].copy()\n",
    "print(\"Books still needing external enrichment:\", len(new_to_impute))\n",
    "\n",
    "# Show breakdown by field\n",
    "print(\"\\nBreakdown of remaining missing values:\")\n",
    "print(f\"  - Missing pages: {gb_enriched['pages_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing publication_date: {gb_enriched['publication_date_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing/invalid language: {(gb_enriched['language_clean'].isna() | gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()}\")\n",
    "print(f\"  - Missing publisher: {gb_enriched['publisher_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing description: {gb_enriched['description_clean'].isna().sum()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ed215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(data, size=1000):\n",
    "    for i in range(0, len(data), size):\n",
    "        yield data[i:i+size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d16623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define cache path for Google Books in data/raw\n",
    "CACHE_PATH = Path(\"data/raw/google_api_cache.json\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load existing cache if it exists\n",
    "if CACHE_PATH.exists():\n",
    "    with open(CACHE_PATH, \"r\") as f:\n",
    "        google_cache = json.load(f)\n",
    "else:\n",
    "    google_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44817fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_BOOKS_API_KEY\")\n",
    "\n",
    "def query_google_books(isbn):\n",
    "    isbn = str(isbn)\n",
    "\n",
    "    # Check cache\n",
    "    if isbn in google_cache:\n",
    "        return google_cache[isbn]\n",
    "\n",
    "    # Query Google Books with API key\n",
    "    url = (\n",
    "        f\"https://www.googleapis.com/books/v1/volumes?\"\n",
    "        f\"q=isbn:{isbn}&key={GOOGLE_API_KEY}\"\n",
    "    )\n",
    "\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        result = {\"isbn\": isbn, \"error\": f\"HTTP {r.status_code}\"}\n",
    "    else:\n",
    "        data = r.json()\n",
    "        if \"items\" in data and data[\"items\"]:\n",
    "            volume = data[\"items\"][0][\"volumeInfo\"]\n",
    "            result = {\n",
    "                \"isbn\": isbn,\n",
    "                \"title\": volume.get(\"title\"),\n",
    "                \"authors\": volume.get(\"authors\"),\n",
    "                \"publisher\": volume.get(\"publisher\"),\n",
    "                \"publishedDate\": volume.get(\"publishedDate\"),\n",
    "                \"pageCount\": volume.get(\"pageCount\"),\n",
    "                \"categories\": volume.get(\"categories\"),\n",
    "                \"language\": volume.get(\"language\"),\n",
    "                \"description\": volume.get(\"description\"),\n",
    "            }\n",
    "        else:\n",
    "            result = {\"isbn\": isbn, \"error\": \"No results\"}\n",
    "\n",
    "    # Save to cache\n",
    "    google_cache[isbn] = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_isbns = (\n",
    "    new_to_impute['isbn_query']\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "print(f\"Unique ISBNs to query: {len(list_of_isbns)}\")\n",
    "chunks = list(chunk_list(list_of_isbns, size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Choose which chunk you want to process today\n",
    "chunk_to_process = chunks[1]   # run chunk 0 today, 1 tomorrow, etc.\n",
    "results = []\n",
    "for isbn in tqdm(chunk_to_process, desc=\"Querying Google Books\"):\n",
    "    results.append(query_google_books(isbn))\n",
    "    time.sleep(0.1)   # be nice to the API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f955d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cache after ISBN queries\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(google_cache, f, indent=2)\n",
    "print(f\"Cache updated with {len(google_cache)} entries after ISBN queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d7361",
   "metadata": {},
   "source": [
    "#### Handling Books Without ISBNs\n",
    "\n",
    "Some books lack valid ISBNs but can still be enriched using **title and author search**. Google Books API supports `intitle:` and `inauthor:` query parameters, allowing us to find books by bibliographic metadata instead of identifiers. We create cache keys in `\"title|author\"` format to distinguish these from ISBN-based queries.\n",
    "\n",
    "This fallback strategy significantly increases our enrichment coverage, especially for older books, special editions, or records with ISBN errors. Results are cached alongside ISBN queries to maintain a unified enrichment workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad912ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_google_books_by_title(title, author):\n",
    "    \"\"\"Query Google Books API using title and author when ISBN is unavailable.\"\"\"\n",
    "    \n",
    "    # Create cache key\n",
    "    cache_key = f\"{title}|{author}\"\n",
    "    \n",
    "    if cache_key in google_cache:\n",
    "        return google_cache[cache_key]\n",
    "    \n",
    "    # Build query string\n",
    "    query_parts = []\n",
    "    if pd.notna(title):\n",
    "        query_parts.append(f'intitle:\"{title}\"')\n",
    "    if pd.notna(author):\n",
    "        query_parts.append(f'inauthor:\"{author}\"')\n",
    "    \n",
    "    query_string = \"+\".join(query_parts)\n",
    "    \n",
    "    url = (\n",
    "        f\"https://www.googleapis.com/books/v1/volumes?\"\n",
    "        f\"q={query_string}&key={GOOGLE_API_KEY}\"\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        result = {\"title\": title, \"author\": author, \"error\": f\"HTTP {r.status_code}\"}\n",
    "    else:\n",
    "        data = r.json()\n",
    "        if \"items\" in data and data[\"items\"]:\n",
    "            volume = data[\"items\"][0][\"volumeInfo\"]\n",
    "            result = {\n",
    "                \"title\": title,\n",
    "                \"author\": author,\n",
    "                \"pageCount\": volume.get(\"pageCount\"),\n",
    "                \"publisher\": volume.get(\"publisher\"),  \n",
    "                \"publishedDate\": volume.get(\"publishedDate\"),\n",
    "                \"categories\": volume.get(\"categories\"),\n",
    "                \"language\": volume.get(\"language\"),\n",
    "                \"description\": volume.get(\"description\"),\n",
    "            }\n",
    "        else:\n",
    "            result = {\"title\": title, \"author\": author, \"error\": \"No results\"}\n",
    "    \n",
    "    google_cache[cache_key] = result\n",
    "    return result\n",
    "\n",
    "# Process books without ISBN separately\n",
    "books_without_isbn = new_to_impute[new_to_impute['isbn_query'].isna()].copy()\n",
    "print(f\"Books without ISBN to query by title/author: {len(books_without_isbn)}\")\n",
    "\n",
    "results_by_title = []\n",
    "for idx, row in tqdm(books_without_isbn.iterrows(), \n",
    "                     total=len(books_without_isbn),\n",
    "                     desc=\"Querying Google Books by title/author\"):\n",
    "    results_by_title.append(query_google_books_by_title(\n",
    "        row['title_clean'], \n",
    "        row['author_clean']\n",
    "    ))\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cache after title/author queries\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(google_cache, f, indent=2)\n",
    "print(f\"Cache updated with {len(google_cache)} entries after title/author queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25cd3f8",
   "metadata": {},
   "source": [
    "#### Loading and Applying Cached Results\n",
    "\n",
    "The Google Books cache contains results from multiple query sessions, potentially across different days. We load the complete cache and separate ISBN-based results from title/author-based results by checking for the `\"|\"` delimiter in cache keys. This allows us to apply different matching logic for each result type.\n",
    "\n",
    "We then merge cached data back into `gb_enriched`, apply the cleaning pipeline to standardize formats, and fill remaining metadata gaps. The `map_subjects_to_genres()` function maps Google Books categories to our genre taxonomy, further increasing genre coverage. This completes our multi-source enrichment strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the cache\n",
    "CACHE_PATH = Path(\"google_api_cache.json\")\n",
    "\n",
    "with open(CACHE_PATH, \"r\") as f:\n",
    "    google_cache = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(google_cache)} cached entries\")\n",
    "\n",
    "# Separate ISBN-based results from title/author-based results\n",
    "isbn_results = []\n",
    "title_author_results = []\n",
    "\n",
    "for key, value in google_cache.items():\n",
    "    if \"|\" in key:  # Title|Author format\n",
    "        title_author_results.append(value)\n",
    "    else:  # ISBN format\n",
    "        isbn_results.append(value)\n",
    "\n",
    "print(f\"Found {len(isbn_results)} ISBN-based results\")\n",
    "print(f\"Found {len(title_author_results)} title/author-based results\")\n",
    "\n",
    "# add google books data to dataframe\n",
    "google_columns = [\n",
    "    'pageCount_google',\n",
    "    'publishedDate_google',\n",
    "    'categories_google',\n",
    "    'language_google',\n",
    "    'publisher_google',\n",
    "    'description_google'\n",
    "]\n",
    "for col in google_columns:\n",
    "    if col not in gb_enriched.columns:\n",
    "        gb_enriched[col] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741eb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge isbn_results back to gb_enriched\n",
    "if isbn_results:\n",
    "    google_isbn_df = pd.DataFrame(isbn_results)\n",
    "    print(\"\\nISBN results preview:\")\n",
    "    print(google_isbn_df.head())\n",
    "    \n",
    "    # Create ISBN mapping\n",
    "    isbn_to_data = {str(row['isbn']): row for _, row in google_isbn_df.iterrows() \n",
    "                    if 'isbn' in row and pd.notna(row.get('isbn'))}\n",
    "    \n",
    "    # Update gb_enriched\n",
    "    books_with_isbn = new_to_impute[new_to_impute['isbn_query'].notna()].copy()\n",
    "    \n",
    "    for idx in books_with_isbn.index:\n",
    "        isbn = str(books_with_isbn.loc[idx, 'isbn_query'])\n",
    "        if isbn in isbn_to_data:\n",
    "            result = isbn_to_data[isbn]\n",
    "            if pd.isna(result.get('error')):\n",
    "                gb_enriched.loc[idx, 'pageCount_google'] = result.get('pageCount')\n",
    "                gb_enriched.loc[idx, 'publishedDate_google'] = result.get('publishedDate')\n",
    "                gb_enriched.loc[idx, 'categories_google'] = result.get('categories')\n",
    "                gb_enriched.loc[idx, 'language_google'] = result.get('language')\n",
    "                gb_enriched.loc[idx, 'publisher_google'] = result.get('publisher')\n",
    "                gb_enriched.loc[idx, 'description_google'] = result.get('description')\n",
    "    \n",
    "    print(f\"✓ Merged ISBN-based results for {len(isbn_to_data)} books\")\n",
    "\n",
    "\n",
    "# merge title/author results back to gb_enriched\n",
    "\n",
    "if title_author_results:\n",
    "    google_title_df = pd.DataFrame(title_author_results)\n",
    "    print(\"\\nTitle/Author results preview:\")\n",
    "    print(google_title_df.head())\n",
    "    \n",
    "    # Recreate books_without_isbn from new_to_impute\n",
    "    books_without_isbn = new_to_impute[new_to_impute['isbn_query'].isna()].copy()\n",
    "    \n",
    "    # Create title|author key mapping\n",
    "    for i, (idx, row) in enumerate(books_without_isbn.iterrows()):\n",
    "        if i < len(google_title_df):\n",
    "            title_author_key = f\"{row['title_clean']}|{row['author_clean']}\"\n",
    "            if title_author_key in google_cache:\n",
    "                result = google_cache[title_author_key]\n",
    "                if pd.isna(result.get('error')):\n",
    "                    gb_enriched.loc[idx, 'pageCount_google'] = result.get('pageCount')\n",
    "                    gb_enriched.loc[idx, 'publishedDate_google'] = result.get('publishedDate')\n",
    "                    gb_enriched.loc[idx, 'categories_google'] = result.get('categories')\n",
    "                    gb_enriched.loc[idx, 'language_google'] = result.get('language')\n",
    "                    gb_enriched.loc[idx, 'publisher_google'] = result.get('publisher')\n",
    "                    gb_enriched.loc[idx, 'description_google'] = result.get('description')\n",
    "    \n",
    "    print(f\"✓ Merged title/author-based results for {len(books_without_isbn)} books\")\n",
    "\n",
    "# Verify merge\n",
    "print(\"\\nGoogle Books data merged:\")\n",
    "for col in google_columns:\n",
    "    count = gb_enriched[col].notna().sum()\n",
    "    print(f\"  - {col}: {count} values\")\n",
    "\n",
    "# clean Google Books API data\n",
    "from src.cleaning.utils.pipeline import apply_cleaners_selectively\n",
    "\n",
    "gb_enriched = apply_cleaners_selectively(\n",
    "    gb_enriched,\n",
    "    fields_to_clean=[\n",
    "        'pageCount',\n",
    "        'publishedDate',\n",
    "        'language',\n",
    "        'categories',\n",
    "        'publisher',\n",
    "        'description'\n",
    "        ],\n",
    "    source_suffix='_google',\n",
    "    target_suffix='_google_clean',\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# Verify cleaning\n",
    "print(\"\\nSample of cleaned Google Books data:\")\n",
    "display(gb_enriched[[\n",
    "    'title_clean',\n",
    "    'pages_clean',\n",
    "    'pageCount_google',\n",
    "    'pageCount_google_clean',\n",
    "    'publication_date_clean',\n",
    "    'publishedDate_google',\n",
    "    'publishedDate_google_clean',\n",
    "    'language_clean',\n",
    "    'language_google',\n",
    "    'language_google_clean',\n",
    "    'genres_clean',\n",
    "    'categories_google',\n",
    "    'categories_google_clean',\n",
    "    'genres_simplified',\n",
    "    'publisher_google',\n",
    "    'publisher_clean',\n",
    "    'description_google',\n",
    "    'description_clean',\n",
    "]].dropna(subset=['pageCount_google_clean', 'language_google_clean'], how='all').sample(min(15, len(gb_enriched)), random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76157a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after cleaning Google Books data, we'll add genre mapping\n",
    "print(\"\\n--- Generating genres_simplified from Google Books categories ---\")\n",
    "\n",
    "books_needing_google_genre_mapping = (\n",
    "    (gb_enriched['genres_simplified'].isna()) & \n",
    "    (gb_enriched['categories_google_clean'].notna())\n",
    ")\n",
    "\n",
    "print(f\"Books with Google categories but no genres_simplified: {books_needing_google_genre_mapping.sum()}\")\n",
    "\n",
    "if books_needing_google_genre_mapping.sum() > 0:\n",
    "    gb_enriched.loc[books_needing_google_genre_mapping, 'genres_simplified'] = (\n",
    "        gb_enriched.loc[books_needing_google_genre_mapping, 'categories_google_clean']\n",
    "        .apply(lambda x: map_subjects_to_genres(x) if isinstance(x, list) else None)\n",
    "    )\n",
    "    \n",
    "    filled_genres = (\n",
    "        gb_enriched.loc[books_needing_google_genre_mapping, 'genres_simplified'].notna().sum()\n",
    "    )\n",
    "    print(f\"genres_simplified: mapped {filled_genres} values from Google Books categories\")\n",
    "\n",
    "\n",
    "# fill missing values with cleaned Google Books data\n",
    "print(\"\\n--- Filling missing values with cleaned Google Books data ---\")\n",
    "\n",
    "# Fill pages_clean\n",
    "print(\"\\n--- Filling remaining page_clean using Google Books data ---\")\n",
    "before_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "gb_enriched['pages_clean'] = gb_enriched['pages_clean'].fillna(gb_enriched['pageCount_google_clean'])\n",
    "after_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "print(f\"pages_clean: filled {before_pages - after_pages} values from Google Books\")\n",
    "\n",
    "# Fill publication_date_clean\n",
    "print(\"\\n--- Filling remaining publication_date_clean using Google Books data ---\")\n",
    "before_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "gb_enriched['publication_date_clean'] = gb_enriched['publication_date_clean'].fillna(gb_enriched['publishedDate_google_clean'])\n",
    "after_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "print(f\"publication_date_clean: filled {before_date - after_date} values from Google Books\")\n",
    "\n",
    "# Fill language_clean\n",
    "print(\"\\n--- Filling remaining language_clean using Google Books data ---\")\n",
    "before_lang = (gb_enriched['language_clean'].isna() | \n",
    "               gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "mask = (gb_enriched['language_clean'].isna() | \n",
    "        gb_enriched['language_clean'].isin(['unknown', '', 'None']))\n",
    "gb_enriched.loc[mask, 'language_clean'] = gb_enriched.loc[mask, 'language_google_clean']\n",
    "after_lang = (gb_enriched['language_clean'].isna() | \n",
    "              gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "print(f\"language_clean: filled {before_lang - after_lang} values from Google Books\")\n",
    "\n",
    "# Fill publisher_clean\n",
    "print(\"\\n--- Filling remaining publisher_clean using Google Books data ---\")\n",
    "before_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "gb_enriched['publisher_clean'] = gb_enriched['publisher_clean'].fillna(\n",
    "    gb_enriched['publisher_google_clean']\n",
    ")\n",
    "after_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "print(f\"publisher_clean: filled {before_publisher - after_publisher} values from Google Books\")\n",
    "\n",
    "\n",
    "# Fill description_clean\n",
    "print(\"\\n--- Filling remaining description_clean using Google Books data ---\")\n",
    "before_desc_google = gb_enriched['description_clean'].isna().sum()\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].fillna(\n",
    "    gb_enriched['description_google_clean']\n",
    ")\n",
    "after_desc_google = gb_enriched['description_clean'].isna().sum()\n",
    "\n",
    "print(f\"description_clean (Google): filled {before_desc_google - after_desc_google} values\")\n",
    "\n",
    "# final enrichment summary\n",
    "print(\"\\n--- FINAL ENRICHMENT SUMMARY (ALL SOURCES) ---\")\n",
    "\n",
    "print(f\"\\nTotal books enriched with Google Books data: {gb_enriched[gb_enriched['categories_google_clean'].notna()].shape[0]}\")\n",
    "\n",
    "print(\"\\n--- FINAL METADATA COVERAGE ---\")\n",
    "print(f\"Books with pages_clean: {gb_enriched['pages_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['pages_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with publication_date_clean: {gb_enriched['publication_date_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['publication_date_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with publisher_clean: {gb_enriched['publisher_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['publisher_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with description_clean: {gb_enriched['description_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['description_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "valid_language = gb_enriched['language_clean'].notna() & ~gb_enriched['language_clean'].isin(['unknown', '', 'None'])\n",
    "print(f\"Books with valid language_clean: {valid_language.sum()} / {len(gb_enriched)} ({valid_language.sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n--- FINAL GENRE COVERAGE ---\")\n",
    "print(f\"Books with genres_clean: {gb_enriched['genres_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['genres_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with genres_simplified: {gb_enriched['genres_simplified'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['genres_simplified'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 4\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdb70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gb_enriched = pd.read_csv('data/cleaned/merge/gb_enriched_v4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14412823",
   "metadata": {},
   "source": [
    "Our multi-source enrichment strategy (BBE → OpenLibrary → Google Books) achieved excellent metadata coverage: **95.1%** for page counts, **100%** for publication dates, **99.4%** for valid language codes, **94.7%** publishers and **85.2%** descriptions. Genre coverage reached **80.8%** for `genres_clean` and **90.7%** for `genres_simplified`, a significant improvement from the original Goodbooks dataset which lacked genre information entirely.\n",
    "\n",
    "This enriched dataset now provides a comprehensive foundation for modeling and analysis. The combination of catalog metadata from BBE, behavioral data from Goodbooks ratings, and API-sourced supplemental information creates a unified dataset that supports both predictive modeling and catalog diversity analysis. The next step is filtering to English-language titles and preparing the final model-ready dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f7600",
   "metadata": {},
   "source": [
    "#### Final Metadata Enrichment Steps\n",
    "\n",
    "After completing API-based enrichment, we perform three final metadata enhancement steps to ensure completeness and consistency across all enriched records:\n",
    "\n",
    "1. **Major Publisher Classification**: We apply the `is_major_publisher` flag to books that received publisher data from APIs but weren't present in the BBE dataset. Using the same publisher pattern matching from Notebook 02, we classify publishers against our curated list of major publishing houses.\n",
    "\n",
    "2. **Awards Flag Completion**: We fill missing `has_award` values with `False` for all books that weren't in the BBE dataset (which contains award metadata). Since API sources don't reliably provide award information, we assume absence of award data means no awards.\n",
    "\n",
    "3. **NLP-Ready Description Generation**: For books enriched with API descriptions, we apply the same NLP cleaning pipeline used in Notebook 02. This converts descriptions into analysis-ready text by removing HTML tags, normalizing whitespace, and standardizing punctuatio, ensuring consistency across BBE and API-sourced descriptions for future text analysis tasks.\n",
    "\n",
    "These steps ensure that all enriched books have the same metadata structure and quality as the original BBE dataset, maintaining consistency across the entire unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813f551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books without is_major_publisher flag: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# load publisher patterns from JSON file\n",
    "with open(\"src/cleaning/mappings/publisher_parent_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    major_publishers = json.load(f)\n",
    "\n",
    "mask_missing_major = gb_enriched['is_major_publisher'].isna()\n",
    "\n",
    "gb_enriched.loc[mask_missing_major, 'is_major_publisher'] = (\n",
    "    gb_enriched.loc[mask_missing_major, 'publisher_clean']\n",
    "        .str.lower()\n",
    "        .apply(lambda x: any(mp in x for mp in major_publishers) if isinstance(x, str) else False)\n",
    ")\n",
    "print(f\"Books without is_major_publisher flag: {gb_enriched['is_major_publisher'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books without has_awards flag: 1918\n",
      "Remaining books without has_awards flag: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reisl\\AppData\\Local\\Temp\\ipykernel_35384\\1980202524.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .fillna(False)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Books without has_awards flag: {gb_enriched['has_award'].isna().sum()}\")\n",
    "gb_enriched['has_award'] = (\n",
    "    gb_enriched['has_award']\n",
    "    .fillna(False)\n",
    "    .astype('bool')\n",
    ")\n",
    "print(f\"Remaining books without has_awards flag: {gb_enriched['has_award'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cleaning.utils.text_cleaning import (\n",
    "    clean_description_nlp\n",
    ")\n",
    "# generate description_nlp from description_clean where API descriptions exist\n",
    "mask_api_desc = (\n",
    "    gb_enriched['description_openlib'].notna() |\n",
    "    gb_enriched['description_google'].notna()\n",
    ")\n",
    "\n",
    "gb_enriched.loc[mask_api_desc, 'description_nlp'] = (\n",
    "    gb_enriched.loc[mask_api_desc, 'description_clean']\n",
    "        .apply(clean_description_nlp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77e3add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing description_clean: 1484\n",
      "Missing description_nlp: 1484\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fix_missing_text(col):\n",
    "    return (\n",
    "        gb_enriched[col]\n",
    "        .replace([\"\", \" \", \"None\", \"none\", \"nan\", \"Nan\", \"NAN\"], np.nan)\n",
    "        .replace(r\"^\\s+$\", np.nan, regex=True)\n",
    "    )\n",
    "\n",
    "gb_enriched['description_clean'] = fix_missing_text('description_clean')\n",
    "gb_enriched['description_nlp']   = fix_missing_text('description_nlp')\n",
    "\n",
    "print(\"Missing description_clean:\", gb_enriched['description_clean'].isna().sum())\n",
    "print(\"Missing description_nlp:\", gb_enriched['description_nlp'].isna().sum())\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].astype(\"string\")\n",
    "gb_enriched['description_nlp']   = gb_enriched['description_nlp'].astype(\"string\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c1b3c",
   "metadata": {},
   "source": [
    "### Filtering for English-Language Books\n",
    "\n",
    "To ensure consistency and focus for downstream analysis and modeling, we filter the enriched dataset to include only **English-language books**. This step is critical for:\n",
    "\n",
    "- **Genre diversity analysis**: Comparing genre distributions across a linguistically consistent corpus\n",
    "- **Ratings behavior modeling**: Ensuring user rating patterns reflect a common language context\n",
    "- **Text analysis (stretch)**: Enabling NLP tasks on descriptions without multilingual complexity\n",
    "\n",
    "We create a filtered copy of `gb_enriched` containing only books where `language_clean` is identified as English (using ISO 639 language code`'en'`). This filtered dataset will serve as the primary input for modeling and analysis, while the full enriched dataset (including non-English titles) is preserved for reference.\n",
    "\n",
    "The English-only dataset is saved as the final output, ready for exploratory analysis and model development in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c3ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered (EN only): (9742, 60)\n"
     ]
    }
   ],
   "source": [
    "gb_enriched_en = gb_enriched[gb_enriched['language_clean'] == 'en'].copy()\n",
    "print(\"Filtered (EN only):\", gb_enriched_en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30f39309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after dropping enrichment data: 34\n",
      "Shape before dropping duplicates: (9742, 34)\n",
      "Shape after dropping duplicates: (9742, 34)\n",
      "Duplicates removed: 9742\n",
      "\n",
      "Final columns for analysis:\n",
      "['book_id', 'work_text_reviews_count', 'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5', 'goodreads_id_clean', 'best_book_id_clean', 'work_id_clean', 'authors_list', 'author_clean', 'language_clean', 'publication_date_clean', 'isbn_clean', 'rating_clean', 'numRatings_clean', 'numRatings_log', 'ratings_1_share', 'ratings_2_share', 'ratings_3_share', 'ratings_4_share', 'ratings_5_share', 'work_text_reviews_log', 'series_clean', 'title_clean', 'pages_clean', 'genres_clean', 'genres_simplified', 'publisher_clean', 'is_major_publisher', 'has_award', 'description_clean', 'description_nlp']\n"
     ]
    }
   ],
   "source": [
    "# drop intermediate enrichment columns\n",
    "enrichment_columns_to_drop = [\n",
    "    # OpenLibrary raw and intermediate columns\n",
    "    'pages_openlib',\n",
    "    'publication_date_openlib',\n",
    "    'language_openlib',\n",
    "    'subjects_openlib',\n",
    "    'publisher_openlib',\n",
    "    'description_openlib',\n",
    "    'pages_openlib_clean',\n",
    "    'publication_date_openlib_clean',\n",
    "    'language_openlib_clean',\n",
    "    'subjects_openlib_clean',\n",
    "    'publisher_openlib_clean',\n",
    "    'description_openlib_clean',\n",
    "    # Google Books raw and intermediate columns\n",
    "    'pageCount_google',\n",
    "    'publishedDate_google',\n",
    "    'categories_google',\n",
    "    'language_google',\n",
    "    'publisher_google',\n",
    "    'description_google',\n",
    "    'pageCount_google_clean',\n",
    "    'publishedDate_google_clean',\n",
    "    'language_google_clean',\n",
    "    'categories_google_clean',\n",
    "    'publisher_google_clean',\n",
    "    'description_google_clean',\n",
    "    # Query helper column\n",
    "    'isbn_query',\n",
    "    # Other intermediate columns\n",
    "    'isbn13_clean',\n",
    "]\n",
    "\n",
    "# drop enrichment columns\n",
    "gb_enriched_en = gb_enriched_en.drop(columns=enrichment_columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Columns after dropping enrichment data: {len(gb_enriched_en.columns)}\")\n",
    "print(f\"Shape before dropping duplicates: {gb_enriched_en.shape}\")\n",
    "\n",
    "# drop duplicate rows based on goodreads_id_clean (keep first occurrence)\n",
    "gb_enriched_en = gb_enriched_en.drop_duplicates(subset=['goodreads_id_clean'], keep='first')\n",
    "\n",
    "print(f\"Shape after dropping duplicates: {gb_enriched_en.shape}\")\n",
    "print(f\"Duplicates removed: {gb_enriched_en.shape[0]}\")\n",
    "\n",
    "# Verify final columns\n",
    "print(\"\\nFinal columns for analysis:\")\n",
    "print(gb_enriched_en.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c01d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL DATA QUALITY CHECKS - gb_enriched_en\n",
      "============================================================\n",
      "\n",
      "1. DUPLICATE CHECK\n",
      "Total rows: 9742\n",
      "Duplicate goodreads_id_clean: 0\n",
      "\n",
      "2. NULL VALUES IN CRITICAL COLUMNS\n",
      "goodreads_id_clean: 0 nulls (0.00%)\n",
      "title_clean: 0 nulls (0.00%)\n",
      "author_clean: 0 nulls (0.00%)\n",
      "language_clean: 0 nulls (0.00%)\n",
      "\n",
      "3. LANGUAGE CONSISTENCY CHECK\n",
      "Unique languages: ['en']\n",
      "\n",
      "4. DATA TYPE CHECK\n",
      "book_id                             int64\n",
      "work_text_reviews_count             int64\n",
      "ratings_1                           int64\n",
      "ratings_2                           int64\n",
      "ratings_3                           int64\n",
      "ratings_4                           int64\n",
      "ratings_5                           int64\n",
      "goodreads_id_clean                  int64\n",
      "best_book_id_clean                  int64\n",
      "work_id_clean                       int64\n",
      "authors_list                       object\n",
      "author_clean                       object\n",
      "language_clean                     object\n",
      "publication_date_clean             object\n",
      "isbn_clean                         object\n",
      "rating_clean                      float64\n",
      "numRatings_clean                    int64\n",
      "numRatings_log                    float64\n",
      "ratings_1_share                   float64\n",
      "ratings_2_share                   float64\n",
      "ratings_3_share                   float64\n",
      "ratings_4_share                   float64\n",
      "ratings_5_share                   float64\n",
      "work_text_reviews_log             float64\n",
      "series_clean                       object\n",
      "title_clean                        object\n",
      "pages_clean                       float64\n",
      "genres_clean                       object\n",
      "genres_simplified                  object\n",
      "publisher_clean                    object\n",
      "is_major_publisher                 object\n",
      "has_award                            bool\n",
      "description_clean          string[python]\n",
      "description_nlp            string[python]\n",
      "dtype: object\n",
      "\n",
      "5. EMPTY STRING CHECK\n",
      "title_clean: 0 empty strings, 0 whitespace-only\n",
      "author_clean: 0 empty strings, 0 whitespace-only\n",
      "publisher_clean: 0 empty strings, 0 whitespace-only\n",
      "description_clean: 0 empty strings, 0 whitespace-only\n",
      "\n",
      "6. METADATA COVERAGE\n",
      "Pages: 95.5% coverage\n",
      "Publication Date: 100.0% coverage\n",
      "Publisher: 95.0% coverage\n",
      "Genres: 91.2% coverage\n",
      "Description: 86.0% coverage\n",
      "\n",
      "7. ENRICHMENT COLUMN CHECK\n",
      "✓ No enrichment columns remaining\n",
      "\n",
      "8. SUMMARY STATISTICS\n",
      "Final dataset shape: (9742, 34)\n",
      "Columns: 34\n",
      "Memory usage: 23.64 MB\n",
      "\n",
      "============================================================\n",
      "CHECKS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# FINAL DATA QUALITY CHECKS BEFORE SAVING\n",
    "# ================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL DATA QUALITY CHECKS - gb_enriched_en\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# check for duplicates\n",
    "print(\"\\n1. DUPLICATE CHECK\")\n",
    "print(f\"Total rows: {len(gb_enriched_en)}\")\n",
    "duplicates = gb_enriched_en.duplicated(subset=['goodreads_id_clean']).sum()\n",
    "print(f\"Duplicate goodreads_id_clean: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(\"⚠️ WARNING: Duplicates found!\")\n",
    "    display(gb_enriched_en[gb_enriched_en.duplicated(subset=['goodreads_id_clean'], keep=False)])\n",
    "\n",
    "# check for null values in critical columns\n",
    "print(\"\\n2. NULL VALUES IN CRITICAL COLUMNS\")\n",
    "critical_cols = ['goodreads_id_clean', 'title_clean', 'author_clean', 'language_clean']\n",
    "for col in critical_cols:\n",
    "    null_count = gb_enriched_en[col].isna().sum()\n",
    "    print(f\"{col}: {null_count} nulls ({null_count/len(gb_enriched_en)*100:.2f}%)\")\n",
    "    if null_count > 0:\n",
    "        print(f\"⚠️ WARNING: Nulls found in {col}!\")\n",
    "\n",
    "# verify language filter worked\n",
    "print(\"\\n3. LANGUAGE CONSISTENCY CHECK\")\n",
    "unique_langs = gb_enriched_en['language_clean'].unique()\n",
    "print(f\"Unique languages: {unique_langs}\")\n",
    "if len(unique_langs) > 1 or unique_langs[0] != 'en':\n",
    "    print(\"⚠️ WARNING: Non-English books found after filtering!\")\n",
    "\n",
    "# check data types\n",
    "print(\"\\n4. DATA TYPE CHECK\")\n",
    "print(gb_enriched_en.dtypes)\n",
    "\n",
    "# check for empty strings or whitespace-only values\n",
    "print(\"\\n5. EMPTY STRING CHECK\")\n",
    "text_cols = ['title_clean', 'author_clean', 'publisher_clean', 'description_clean']\n",
    "for col in text_cols:\n",
    "    if col in gb_enriched_en.columns:\n",
    "        empty = (gb_enriched_en[col] == '').sum()\n",
    "        whitespace = gb_enriched_en[col].str.strip().eq('').sum()\n",
    "        print(f\"{col}: {empty} empty strings, {whitespace} whitespace-only\")\n",
    "\n",
    "# check metadata coverage\n",
    "print(\"\\n6. METADATA COVERAGE\")\n",
    "metadata_cols = {\n",
    "    'pages_clean': 'Pages',\n",
    "    'publication_date_clean': 'Publication Date',\n",
    "    'publisher_clean': 'Publisher',\n",
    "    'genres_simplified': 'Genres',\n",
    "    'description_clean': 'Description'\n",
    "}\n",
    "for col, label in metadata_cols.items():\n",
    "    if col in gb_enriched_en.columns:\n",
    "        coverage = gb_enriched_en[col].notna().sum() / len(gb_enriched_en) * 100\n",
    "        print(f\"{label}: {coverage:.1f}% coverage\")\n",
    "\n",
    "# check for unexpected enrichment columns still present\n",
    "print(\"\\n7. ENRICHMENT COLUMN CHECK\")\n",
    "enrichment_patterns = ['_openlib', '_google', '_bbe', 'isbn_query']\n",
    "leftover_cols = [col for col in gb_enriched_en.columns \n",
    "                 if any(pattern in col for pattern in enrichment_patterns)]\n",
    "if leftover_cols:\n",
    "    print(f\"WARNING: Leftover enrichment columns found: {leftover_cols}\")\n",
    "else:\n",
    "    print(\"✓ No enrichment columns remaining\")\n",
    "\n",
    "# aummary statistics\n",
    "print(\"\\n8. SUMMARY STATISTICS\")\n",
    "print(f\"Final dataset shape: {gb_enriched_en.shape}\")\n",
    "print(f\"Columns: {len(gb_enriched_en.columns)}\")\n",
    "print(f\"Memory usage: {gb_enriched_en.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHECKS COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff346538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_internal_catalog saved successfully in outputs\\datasets\\cleaned directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'en_internal_catalog'\n",
    "clean_path = Path(\"outputs/datasets/cleaned\")\n",
    "clean_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gb_enriched_en.to_csv(clean_path / f\"{file_name}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} saved successfully in {clean_path} directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33cddfc",
   "metadata": {},
   "source": [
    "## BBE English Only \n",
    "\n",
    "The BBE dataset has already been cleaned in the previous steps. Since we are not enriching it further, we only need to filter it to English-language books for consistency with the Goodbooks dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31efe737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original BBE dataset shape: (52424, 36)\n",
      "English-only BBE dataset shape: (42634, 36)\n",
      "Books filtered out (non-English): 9790\n",
      "English books percentage: 81.3%\n",
      "\n",
      "Sample of English-only BBE dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goodreads_id_clean</th>\n",
       "      <th>authors_list</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>isbn_clean</th>\n",
       "      <th>language_clean</th>\n",
       "      <th>publication_date_clean</th>\n",
       "      <th>publisher_clean</th>\n",
       "      <th>is_major_publisher</th>\n",
       "      <th>bookFormat_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>description_clean</th>\n",
       "      <th>description_nlp</th>\n",
       "      <th>series_clean</th>\n",
       "      <th>pages_clean</th>\n",
       "      <th>bbeVotes_clean</th>\n",
       "      <th>bbeScore_clean</th>\n",
       "      <th>likedPercent_clean</th>\n",
       "      <th>has_likedPercent</th>\n",
       "      <th>price_clean</th>\n",
       "      <th>price_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2767052</td>\n",
       "      <td>['suzanne collins']</td>\n",
       "      <td>suzanne collins</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>9780439023481</td>\n",
       "      <td>en</td>\n",
       "      <td>2008-09-14</td>\n",
       "      <td>scholastic</td>\n",
       "      <td>True</td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>winning means fame and fortunelosing means cer...</td>\n",
       "      <td>winning means fame and fortunelosing means cer...</td>\n",
       "      <td>the hunger games</td>\n",
       "      <td>374.0</td>\n",
       "      <td>30516</td>\n",
       "      <td>2993816</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['jk rowling', 'mary grandpre']</td>\n",
       "      <td>jk rowling, mary grandpre</td>\n",
       "      <td>harry potter and the order of the phoenix</td>\n",
       "      <td>9780439358071</td>\n",
       "      <td>en</td>\n",
       "      <td>2003-06-21</td>\n",
       "      <td>scholastic</td>\n",
       "      <td>True</td>\n",
       "      <td>paperback</td>\n",
       "      <td>...</td>\n",
       "      <td>there is a door at the end of a silent corrido...</td>\n",
       "      <td>there is a door at the end of a silent corrido...</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>870.0</td>\n",
       "      <td>26923</td>\n",
       "      <td>2632233</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.38</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2657</td>\n",
       "      <td>['harper lee']</td>\n",
       "      <td>harper lee</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>harpercollins</td>\n",
       "      <td>True</td>\n",
       "      <td>paperback</td>\n",
       "      <td>...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>the unforgettable novel of a childhood in a sl...</td>\n",
       "      <td>to kill a mockingbird</td>\n",
       "      <td>324.0</td>\n",
       "      <td>23328</td>\n",
       "      <td>2269402</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  goodreads_id_clean                     authors_list  \\\n",
       "0            2767052              ['suzanne collins']   \n",
       "1                  2  ['jk rowling', 'mary grandpre']   \n",
       "2               2657                   ['harper lee']   \n",
       "\n",
       "                author_clean                                title_clean  \\\n",
       "0            suzanne collins                           the hunger games   \n",
       "1  jk rowling, mary grandpre  harry potter and the order of the phoenix   \n",
       "2                 harper lee                      to kill a mockingbird   \n",
       "\n",
       "      isbn_clean language_clean publication_date_clean publisher_clean  \\\n",
       "0  9780439023481             en             2008-09-14      scholastic   \n",
       "1  9780439358071             en             2003-06-21      scholastic   \n",
       "2           <NA>             en                    NaN   harpercollins   \n",
       "\n",
       "   is_major_publisher bookFormat_clean  ...  \\\n",
       "0                True        hardcover  ...   \n",
       "1                True        paperback  ...   \n",
       "2                True        paperback  ...   \n",
       "\n",
       "                                   description_clean  \\\n",
       "0  winning means fame and fortunelosing means cer...   \n",
       "1  there is a door at the end of a silent corrido...   \n",
       "2  the unforgettable novel of a childhood in a sl...   \n",
       "\n",
       "                                     description_nlp           series_clean  \\\n",
       "0  winning means fame and fortunelosing means cer...       the hunger games   \n",
       "1  there is a door at the end of a silent corrido...           harry potter   \n",
       "2  the unforgettable novel of a childhood in a sl...  to kill a mockingbird   \n",
       "\n",
       "   pages_clean  bbeVotes_clean  bbeScore_clean  likedPercent_clean  \\\n",
       "0        374.0           30516         2993816                96.0   \n",
       "1        870.0           26923         2632233                98.0   \n",
       "2        324.0           23328         2269402                95.0   \n",
       "\n",
       "   has_likedPercent  price_clean  price_flag  \n",
       "0                 1         5.09       False  \n",
       "1                 1         7.38       False  \n",
       "2                 1          NaN        True  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_supply_catalog = bbe_clean[ bbe_clean[\"language_clean\"] == \"en\" ]\n",
    "\n",
    "# check shape after filtering\n",
    "print(f\"Original BBE dataset shape: {bbe_clean.shape}\")\n",
    "print(f\"English-only BBE dataset shape: {en_supply_catalog.shape}\")\n",
    "print(f\"Books filtered out (non-English): {bbe_clean.shape[0] - en_supply_catalog.shape[0]}\")\n",
    "print(f\"English books percentage: {(en_supply_catalog.shape[0] / bbe_clean.shape[0]) * 100:.1f}%\")\n",
    "\n",
    "# display sample\n",
    "print(\"\\nSample of English-only BBE dataset:\")\n",
    "display(en_supply_catalog.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8dd469b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL DATA QUALITY CHECKS - en_supply_catalog\n",
      "============================================================\n",
      "\n",
      "1. DUPLICATE CHECK\n",
      "Total rows: 42634\n",
      "Duplicate goodreads_id_clean: 0\n",
      "\n",
      "2. LANGUAGE CONSISTENCY CHECK\n",
      "Unique languages: ['en']\n",
      "\n",
      "3. METADATA COVERAGE\n",
      "Pages: 96.3% coverage\n",
      "Publication Date: 90.9% coverage\n",
      "Publisher: 94.3% coverage\n",
      "Genres: 100.0% coverage\n",
      "Description: 98.8% coverage\n",
      "\n",
      "Final BBE dataset shape: (42634, 36)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL DATA QUALITY CHECKS - en_supply_catalog\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# check for duplicates\n",
    "print(\"\\n1. DUPLICATE CHECK\")\n",
    "print(f\"Total rows: {len(en_supply_catalog)}\")\n",
    "duplicates_bbe = en_supply_catalog.duplicated(subset=['goodreads_id_clean']).sum()\n",
    "print(f\"Duplicate goodreads_id_clean: {duplicates_bbe}\")\n",
    "\n",
    "# verify language filter\n",
    "print(\"\\n2. LANGUAGE CONSISTENCY CHECK\")\n",
    "unique_langs_bbe = en_supply_catalog['language_clean'].unique()\n",
    "print(f\"Unique languages: {unique_langs_bbe}\")\n",
    "\n",
    "# metadata coverage\n",
    "print(\"\\n3. METADATA COVERAGE\")\n",
    "for col, label in metadata_cols.items():\n",
    "    if col in en_supply_catalog.columns:\n",
    "        coverage = en_supply_catalog[col].notna().sum() / len(en_supply_catalog) * 100\n",
    "        print(f\"{label}: {coverage:.1f}% coverage\")\n",
    "\n",
    "print(f\"\\nFinal BBE dataset shape: {en_supply_catalog.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d447fbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_supply_catalog saved successfully in outputs\\datasets\\cleaned directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'en_supply_catalog'\n",
    "clean_path = Path(\"outputs/datasets/cleaned\")\n",
    "clean_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "en_supply_catalog.to_csv(clean_path / f\"{file_name}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} saved successfully in {clean_path} directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
