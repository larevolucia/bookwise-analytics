{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d6a58f",
   "metadata": {},
   "source": [
    "# Data Enrichment & Dataset Integration\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The purpose of this notebook is to **enrich, align, and integrate the cleaned datasets** to create a unified analytical foundation for modelling book satisfaction and evaluating catalogue diversity.\n",
    "\n",
    "This notebook expands upon prior cleaning work by **adding missing metadata, linking overlapping records across datasets, filtering the dataset to English-language titles, and preparing a model-ready dataset** that combines catalog-level information (BBE) with user-behavioral data (Goodbooks).\n",
    "\n",
    "Ultimately, this notebook enables insights that neither dataset could provide independently, most critically, **genre diversity analysis**, **language-based consistency**, **metadata-enhanced prediction modeling**.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "\n",
    "| Dataset                             | Source                     | Description                                                                                         | Format |\n",
    "| ----------------------------------- | -------------------------- | --------------------------------------------------------------------------------------------------- | ------ |\n",
    "| `bbe_clean_v13.csv`                  | Output from Notebook 02    | Cleaned *Best Books Ever* metadata including title, authors, genres, rating, description, and more. | CSV    |\n",
    "| `books_clean_v7.csv`      | Output from Notebook 02    | Cleaned Goodbooks-10k metadata lacking genre data but containing structural identifiers.            | CSV    |\n",
    "| `ratings_clean_v1.csv`    | Output from Notebook 02    | User–book interaction and aggregated rating data for behavioral modeling.                           | CSV    |\n",
    "| *(Optional)* External API responses | OpenLibrary / Google Books | Supplemental metadata (genres, languages, subjects) for non-overlapping titles.                     | JSON   |\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks in This Notebook\n",
    "\n",
    "This notebook will execute the following enrichment and integration steps:\n",
    "\n",
    "1. **Standardize linking identifiers**\n",
    "   Normalize `isbn_clean`, `goodreads_id`, `title_clean`, and `author_clean` across datasets to ensure reliable cross-dataset merging.\n",
    "\n",
    "2. **Identify overlap between BBE and Goodbooks**\n",
    "   Detect books present in both datasets using multi-key matching and evaluate match quality.\n",
    "\n",
    "3. **Enrich Goodbooks metadata with missing genres**\n",
    "\n",
    "   * Use BBE genre fields for overlapping titles.\n",
    "   * Query external APIs for non-overlapping titles.\n",
    "   * Normalize all genre outputs into a unified taxonomy.\n",
    "\n",
    "4. **Complete and standardize language metadata**\n",
    "   Fill missing values using BBE, APIs, or text-based heuristics, then harmonize language labels and codes.\n",
    "\n",
    "5. **Filter the enriched datasets to English-language books**\n",
    "   Restrict the unified dataset to titles identified as **English-language**, ensuring consistency for:\n",
    "\n",
    "   * genre diversity comparisons\n",
    "   * ratings behavior\n",
    "   * regression modeling\n",
    "\n",
    "   *(Non-English titles will be kept only in the enriched BBE/Goodbooks outputs, but excluded from the model dataset.)*\n",
    "\n",
    "6. **Integrate datasets into a unified model-ready schema**\n",
    "   Combine BBE metadata with Goodbooks behavioral features for all overlapping **English-language** books.\n",
    "\n",
    "7. **Validate enrichment and filtering results**\n",
    "\n",
    "   * Assess genre and language fill rates\n",
    "   * Review API match and success metrics\n",
    "   * Log all imputation and filtering decisions for reproducibility\n",
    "\n",
    "8. **Export enriched and unified datasets**\n",
    "   Produce final English-filtered datasets ready for modeling and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* **BBE_clean_enriched.csv** — enriched metadata for all BBE books\n",
    "* **Goodbooks_books_clean_enriched.csv** — enriched metadata for all Goodbooks books\n",
    "* **model_dataset_overlap_en_only.csv** — unified metadata + behavioral dataset filtered to English-language books\n",
    "* **Enrichment and filtering logs** — documenting imputation sources, API usage, and filtering decisions\n",
    "\n",
    "> **Note:** This notebook focuses on **metadata enrichment, English-language filtering, and dataset integration**. Model development and feature engineering will be performed in later notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09815b66",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "## Navigate to the Parent Directory\n",
    "\n",
    "Before combining and saving datasets, it’s often helpful to move to a parent directory so that file operations (like loading or saving data) are easier and more organized. \n",
    "\n",
    "Before using the Python’s built-in os module to move one level up from the current working directory, it is advisable to inspect the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19d6dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\reisl\\OneDrive\\Documents\\GitHub\\bookwise-analytics\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(f'Current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2117d",
   "metadata": {},
   "source": [
    "To change to parent directory (root folder), run the code below. If you are already in the root folder, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fc58fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to parent.\n",
      "New current directory: c:\\Users\\reisl\\OneDrive\\Documents\\GitHub\\bookwise-analytics\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory to its parent\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "print('Changed directory to parent.')\n",
    "\n",
    "# Get the new current working directory (the parent directory)\n",
    "current_dir = os.getcwd()\n",
    "print(f'New current directory: {current_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cleaning.utils.categories import (\n",
    "    map_subjects_to_genres\n",
    ")\n",
    "from src.cleaning.utils.pipeline import apply_cleaners_selectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b908b97",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Install additional packages for this notebook\n",
    "! pip install requests python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4cc03",
   "metadata": {},
   "source": [
    "## Load and Inspect Datasets\n",
    "\n",
    "In this step, we load the previously cleaned datasets: **Goodbooks-10k** (books, ratings) and **Best Books Ever**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load datasets\n",
    "books_clean = pd.read_csv(\n",
    "    'data/interim/goodbooks/books_clean_v7.csv',\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    "    )\n",
    "ratings_clean = pd.read_csv('data/interim/goodbooks/ratings_clean_v1.csv')\n",
    "bbe_clean = pd.read_csv(\n",
    "    \"data/interim/bbe/bbe_clean_v13.csv\",\n",
    "    dtype={\"isbn_clean\": \"string\", \"goodreads_id_clean\": \"string\"}\n",
    ")\n",
    "\n",
    "# create copies for imputation\n",
    "books_impute = books_clean.copy()\n",
    "ratings_impute = ratings_clean.copy()\n",
    "bbe_impute = bbe_clean.copy()\n",
    "\n",
    "# log samples\n",
    "print(\"BBE dataset columns:\")\n",
    "print(bbe_impute.columns.tolist())\n",
    "print(\"BBE dataset info:\")\n",
    "display(bbe_impute.info())\n",
    "print(\"BBE dataset sample:\")\n",
    "display(bbe_impute.head(3))\n",
    "\n",
    "print(\"Books dataset columns:\")\n",
    "print(books_impute.columns.tolist())\n",
    "print(\"Books dataset info:\")\n",
    "display(books_impute.info())\n",
    "print(\"Books dataset sample:\")\n",
    "display(books_impute.head(3))\n",
    "\n",
    "print(\"Ratings dataset columns:\")\n",
    "print(ratings_impute.columns.tolist())\n",
    "print(\"Ratings dataset info:\")\n",
    "display(ratings_impute.info())\n",
    "print(\"Ratings dataset sample:\")\n",
    "display(ratings_impute.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cff995",
   "metadata": {},
   "source": [
    "# Data Enrichment\n",
    "\n",
    "## Enriching Goodbooks with Genre and Page Count\n",
    "\n",
    "### From BBE overlap\n",
    "\n",
    "To improve the completeness and quality of the Goodbooks-10k dataset, we selectively merge in metadata from the Best Books Ever (BBE) dataset using the shared `goodreads_id_clean` key. Goodbooks is kept as the primary source, while BBE is used to supply additional metadata fields, such as genres and page counts, as well as to fill in missing values for shared attributes like ISBN, publication date, and series.\n",
    "\n",
    "This approach ensures we enhance Goodbooks only where necessary: adding new information where it is absent and completing incomplete entries without overwriting existing data. The resulting `gb_enriched` dataset combines both sources into a more reliable and feature-rich foundation for downstream analytics and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# ENRICH GOODBOOKS (books_impute) WITH BBE DATA\n",
    "# ---------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# columns to enrich ONLY when GB has NaN\n",
    "columns_to_enrich = [\n",
    "    \"publication_date_clean\",\n",
    "    \"series_clean\",\n",
    "    \"isbn_clean\",\n",
    "    \"language_clean\"\n",
    "    ]\n",
    "\n",
    "# columns existent only in BBE\n",
    "bbe_only_columns = [\n",
    "    \"pages_clean\",\n",
    "    \"genres_clean\",\n",
    "    \"genres_simplified\",\n",
    "    \"publisher_clean\",\n",
    "    \"is_major_publisher\",\n",
    "    \"has_award\",\n",
    "    \"description_clean\",\n",
    "    \"description_nlp\"\n",
    "]\n",
    "\n",
    "# merge Goodbooks with the needed BBE columns\n",
    "merge_cols = [\"goodreads_id_clean\"] + columns_to_enrich + bbe_only_columns\n",
    "\n",
    "gb_enriched = books_impute.merge(\n",
    "    bbe_impute[merge_cols].add_suffix(\"_bbe\"),\n",
    "    left_on=\"goodreads_id_clean\",\n",
    "    right_on=\"goodreads_id_clean_bbe\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ENRICH GENRE COLUMNS\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- ENRICHING METADATA ---\")\n",
    "for col in bbe_only_columns:\n",
    "    gb_enriched[col] = gb_enriched[col + \"_bbe\"]\n",
    "    filled = gb_enriched[col].notna().sum()\n",
    "    print(f\"{col}: filled {filled} rows from BBE\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ENRICH SHARED COLUMNS ONLY WHERE GB IS NaN\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- ENRICHING SHARED COLUMNS (GB NaN -> fill from BBE) ---\")\n",
    "for col in columns_to_enrich:\n",
    "    before = gb_enriched[col].isna().sum()\n",
    "    gb_enriched[col] = gb_enriched[col].fillna(gb_enriched[col + \"_bbe\"])\n",
    "    after = gb_enriched[col].isna().sum()\n",
    "    print(f\"{col}: filled {before - after} missing values\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# CLEANUP\n",
    "# ---------------------------------------------\n",
    "gb_enriched = gb_enriched.drop(columns=[c for c in gb_enriched.columns if c.endswith(\"_bbe\")])\n",
    "\n",
    "print(\"\\nEnrichment complete!\")\n",
    "print(\"Final shape:\", gb_enriched.shape)\n",
    "gb_enriched[['isbn_clean','title_clean', 'series_clean', 'genres_clean', 'genres_simplified', 'pages_clean', 'publication_date_clean']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d474378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 1\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ead17c",
   "metadata": {},
   "source": [
    "### From external APIs\n",
    "\n",
    "To further enrich the Goodbooks-10k dataset, we leverage external APIs such as OpenLibrary and Google Books to fill in missing metadata for titles not covered by the BBE overlap. This process involves querying these APIs using available identifiers (like ISBN or title/author combinations) to retrieve additional information such as genres, page counts, and publication details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_isbn(isbn):\n",
    "    if not isinstance(isbn, str):\n",
    "        return None\n",
    "    isbn = re.sub(r'[^0-9Xx]', '', isbn)\n",
    "    if len(isbn) in [10, 13]:\n",
    "        return isbn\n",
    "    return None\n",
    "\n",
    "gb_enriched['isbn_query'] = gb_enriched['isbn_clean'].apply(clean_isbn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee431a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mask = (\n",
    "    gb_enriched['language_clean'].isna() |\n",
    "    gb_enriched['language_clean'].isin(['unknown', '', 'None']) |\n",
    "    gb_enriched['pages_clean'].isna() |\n",
    "    gb_enriched['publication_date_clean'].isna()  |\n",
    "    gb_enriched['publisher_clean'].isna() |\n",
    "    gb_enriched['description_clean'].isna()\n",
    ")\n",
    "\n",
    "to_impute = gb_enriched[missing_mask].copy()\n",
    "print(\"Books needing external enrichment:\", len(to_impute))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ab95c",
   "metadata": {},
   "source": [
    "#### Querying OpenLibrary API\n",
    "\n",
    "After enriching Goodbooks with BBE overlap data, we identify **2,249** books still missing critical metadata (ISBN, language, pages, publication date, publisher). We query **OpenLibrary first** because it has no rate limits or API key requirements, making it ideal for bulk enrichment. We create a boolean mask to identify books needing enrichment, then query OpenLibrary's ISBN endpoint for each book, collecting results in a structured format.\n",
    "\n",
    "The results are merged back into `gb_enriched` and saved as **version 2**. This incremental saving strategy ensures we don't lose progress if subsequent API calls fail or exceed quotas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b963e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# cache path for OpenLibrary in data/raw\n",
    "OL_CACHE_PATH = Path(\"data/raw/openlibrary_api_cache.json\")\n",
    "\n",
    "# create directory if it doesn't exist\n",
    "OL_CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load existing cache if it exists\n",
    "if OL_CACHE_PATH.exists():\n",
    "    with open(OL_CACHE_PATH, \"r\") as f:\n",
    "        ol_cache = json.load(f)\n",
    "    print(f\"Loaded {len(ol_cache)} cached OpenLibrary entries\")\n",
    "else:\n",
    "    ol_cache = {}\n",
    "    print(\"No existing cache found, starting fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5278cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def query_openlibrary(isbn):\n",
    "    \"\"\"Return OL metadata in a consistent dict format.\"\"\"\n",
    "\n",
    "    isbn_str = str(isbn)\n",
    "    \n",
    "    if isbn_str in ol_cache:\n",
    "        return ol_cache[isbn_str]\n",
    "    \n",
    "    # Default structure to guarantee stable DataFrame columns\n",
    "    result = {\n",
    "        \"pages_openlib\": None,\n",
    "        \"publication_date_openlib\": None,\n",
    "        \"language_openlib\": None,\n",
    "        \"subjects_openlib\": None,\n",
    "        \"publisher_openlib\": None, \n",
    "        \"description_openlib\": None, \n",
    "    }\n",
    "\n",
    "    if isbn is None or pd.isna(isbn) or isbn == \"\":\n",
    "        return result\n",
    "    \n",
    "    url = f\"https://openlibrary.org/isbn/{isbn}.json\"\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return result\n",
    "\n",
    "        data = r.json()\n",
    "\n",
    "        # Pages\n",
    "        result[\"pages_openlib\"] = data.get(\"number_of_pages\")\n",
    "\n",
    "        # Publication date\n",
    "        result[\"publication_date_openlib\"] = data.get(\"publish_date\")\n",
    "\n",
    "        # Language\n",
    "        if \"languages\" in data and isinstance(data[\"languages\"], list):\n",
    "            key = data[\"languages\"][0].get(\"key\", \"\").split(\"/\")[-1]\n",
    "            result[\"language_openlib\"] = key\n",
    "\n",
    "        # Subjects\n",
    "        if \"subjects\" in data:\n",
    "            result[\"subjects_openlib\"] = [s.lower() for s in data[\"subjects\"]]\n",
    "        \n",
    "        # Publisher\n",
    "        if \"publishers\" in data and isinstance(data[\"publishers\"], list):\n",
    "            result[\"publisher_openlib\"] = data[\"publishers\"][0]\n",
    "        \n",
    "        # Description\n",
    "        desc = data.get(\"description\")\n",
    "        if isinstance(desc, dict):\n",
    "            result[\"description_openlib\"] = desc.get(\"value\")\n",
    "        elif isinstance(desc, str):\n",
    "            result[\"description_openlib\"] = desc\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        pass  # keep the default result structure\n",
    "\n",
    "    # Save to cache\n",
    "    ol_cache[isbn_str] = result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for isbn in tqdm(to_impute['isbn_query'], desc=\"Querying OpenLibrary\"):\n",
    "    results.append(query_openlibrary(isbn))\n",
    "    time.sleep(0.2)   # safe rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Save OpenLibrary cache after queries\n",
    "with open(OL_CACHE_PATH, \"w\") as f:\n",
    "    json.dump(ol_cache, f, indent=2)\n",
    "print(f\"OpenLibrary cache saved with {len(ol_cache)} entries\")\n",
    "\n",
    "# convert results to dataframe\n",
    "ol_df = pd.DataFrame(results, index=to_impute.index)\n",
    "print(\"API results summary:\")\n",
    "print(ol_df.notna().sum())\n",
    "\n",
    "# merge back into gb_enriched\n",
    "for col in ol_df.columns:\n",
    "    if col not in gb_enriched.columns:\n",
    "        gb_enriched[col] = None\n",
    "    gb_enriched.loc[ol_df.index, col] = ol_df[col]\n",
    "\n",
    "# verify the merge\n",
    "print(\"\\nAfter merge:\")\n",
    "print(gb_enriched[ol_df.columns].notna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 2\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a12d1a",
   "metadata": {},
   "source": [
    "#### Cleaning and Processing OpenLibrary Data\n",
    "\n",
    "We apply the same cleaning steps used in Notebook 02, compiled into a pipeline, to standardize OpenLibrary API responses. The `apply_cleaners_selectively()` function ensures consistent data types, formats, and validation across all metadata fields. After cleaning, we fill missing values in `gb_enriched` using the cleaned OpenLibrary data.\n",
    "\n",
    "For genre enrichment, we map OpenLibrary subjects to our standardized genre taxonomy using `map_subjects_to_genres()`. This populates `genres_simplified` for books that had subjects but no genre data, significantly improving genre coverage. The enriched dataset is saved as **version 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a19072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean OpenLibrary API data\n",
    "gb_enriched = apply_cleaners_selectively(\n",
    "    gb_enriched,\n",
    "    fields_to_clean=[\n",
    "        'pages',\n",
    "        'publication_date',\n",
    "        'language',\n",
    "        'subjects',\n",
    "        'publisher',\n",
    "        'description'\n",
    "        ],\n",
    "    source_suffix='_openlib',\n",
    "    target_suffix='_openlib_clean',\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# verify cleaning\n",
    "print(\"\\nSample of cleaned OpenLibrary data:\")\n",
    "gb_enriched[[\n",
    "    'title_clean',\n",
    "    'pages_clean',\n",
    "    'pages_openlib',\n",
    "    'pages_openlib_clean',\n",
    "    'publication_date_clean',\n",
    "    'publication_date_openlib',\n",
    "    'publication_date_openlib_clean',\n",
    "    'language_clean',\n",
    "    'language_openlib',\n",
    "    'language_openlib_clean',\n",
    "    'genres_clean',\n",
    "    'genres_simplified',\n",
    "    'subjects_openlib',\n",
    "    'subjects_openlib_clean',\n",
    "    'publisher_clean',\n",
    "    'description_openlib',\n",
    "    'description_clean',\n",
    "    'description_openlib',\n",
    "    'description_openlib_clean'\n",
    "    ]].sample(15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with cleaned OpenLibrary data\n",
    "print(\"\\n--- Filling missing values with cleaned OpenLibrary data ---\")\n",
    "\n",
    "# fill pages_clean\n",
    "before_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "gb_enriched['pages_clean'] = gb_enriched['pages_clean'].fillna(gb_enriched['pages_openlib_clean'])\n",
    "after_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "print(f\"pages_clean: filled {before_pages - after_pages} values\")\n",
    "\n",
    "# fill publication_date_clean\n",
    "before_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "gb_enriched['publication_date_clean'] = gb_enriched['publication_date_clean'].fillna(gb_enriched['publication_date_openlib_clean'])\n",
    "after_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "print(f\"publication_date_clean: filled {before_date - after_date} values\")\n",
    "\n",
    "# fill language_clean\n",
    "# Create mask that catches both NaN and invalid string values\n",
    "before_lang = (gb_enriched['language_clean'].isna() | \n",
    "               gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "mask = (gb_enriched['language_clean'].isna() | \n",
    "        gb_enriched['language_clean'].isin(['unknown', '', 'None']))\n",
    "\n",
    "gb_enriched.loc[mask, 'language_clean'] = gb_enriched.loc[mask, 'language_openlib_clean']\n",
    "\n",
    "after_lang = (gb_enriched['language_clean'].isna() | \n",
    "              gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "print(f\"language_clean: filled {before_lang - after_lang} values\")\n",
    "\n",
    "# fill publisher_clean\n",
    "print(\"\\n--- Filling missing publisher_clean using OpenLibrary data ---\")\n",
    "before_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "gb_enriched['publisher_clean'] = gb_enriched['publisher_clean'].fillna(\n",
    "    gb_enriched['publisher_openlib_clean']\n",
    ")\n",
    "after_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "print(f\"publisher_clean: filled {before_publisher - after_publisher} values\")\n",
    "\n",
    "# fill description_clean\n",
    "print(\"\\n--- Filling missing description_clean using OpenLibrary data ---\")\n",
    "\n",
    "before_desc = gb_enriched['description_clean'].isna().sum()\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].fillna(\n",
    "    gb_enriched['description_openlib_clean']\n",
    ")\n",
    "after_desc = gb_enriched['description_clean'].isna().sum()\n",
    "print(f\"description_clean: filled {before_desc - after_desc} values\")\n",
    "\n",
    "# generate genres_simplified from subjects_openlib_clean\n",
    "print(\"\\n--- Generating genres_simplified from OpenLibrary subjects ---\")\n",
    "\n",
    "# Import genre mapping utilities\n",
    "\n",
    "# Fill genres_simplified for books that have subjects but no genres\n",
    "books_needing_genre_mapping = (\n",
    "    (gb_enriched['genres_simplified'].isna()) & \n",
    "    (gb_enriched['subjects_openlib_clean'].notna())\n",
    ")\n",
    "\n",
    "print(f\"Books with subjects but no genres_simplified: {books_needing_genre_mapping.sum()}\")\n",
    "\n",
    "if books_needing_genre_mapping.sum() > 0:\n",
    "    # Apply genre mapping to subjects\n",
    "    gb_enriched.loc[books_needing_genre_mapping, 'genres_simplified'] = (\n",
    "        gb_enriched.loc[books_needing_genre_mapping, 'subjects_openlib_clean']\n",
    "        .apply(lambda x: map_subjects_to_genres(x) if isinstance(x, list) else None)\n",
    "    )\n",
    "    \n",
    "    filled_genres = (\n",
    "        gb_enriched.loc[books_needing_genre_mapping, 'genres_simplified'].notna().sum()\n",
    "    )\n",
    "    print(f\"genres_simplified: mapped {filled_genres} values from OpenLibrary subjects\")\n",
    "    \n",
    "    # Show sample of newly mapped genres\n",
    "    newly_mapped = gb_enriched[books_needing_genre_mapping & gb_enriched['genres_simplified'].notna()]\n",
    "    if len(newly_mapped) > 0:\n",
    "        print(\"\\nSample of newly mapped genres:\")\n",
    "        print(newly_mapped[['title_clean', 'subjects_openlib_clean', 'genres_simplified']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e72318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- ENRICHMENT SUMMARY ---\")\n",
    "\n",
    "# Show books that received OpenLibrary data\n",
    "books_with_ol_data = gb_enriched[\n",
    "    gb_enriched['subjects_openlib_clean'].notna()\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nTotal books enriched with OpenLibrary data: {len(books_with_ol_data)}\")\n",
    "\n",
    "if len(books_with_ol_data) > 0:\n",
    "    print(\"\\nSample of books enriched from OpenLibrary:\")\n",
    "    display(books_with_ol_data[[\n",
    "        'title_clean',\n",
    "        'author_clean',\n",
    "        'pages_openlib_clean',\n",
    "        'publication_date_openlib_clean',\n",
    "        'language_openlib_clean',\n",
    "        'subjects_openlib_clean',\n",
    "        'genres_simplified',\n",
    "        'publisher_clean',\n",
    "        'description_clean'\n",
    "    ]].head(10))\n",
    "\n",
    "# Show overall genre coverage\n",
    "print(f\"\\n--- GENRE COVERAGE AFTER ENRICHMENT ---\")\n",
    "print(f\"Books with genres_clean: {gb_enriched['genres_clean'].notna().sum()}\")\n",
    "print(f\"Books without genres_clean: {gb_enriched['genres_clean'].isna().sum()}\")\n",
    "print(f\"Genre coverage: {gb_enriched['genres_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\\n\")\n",
    "print(f\"Books with genres_simplified: {gb_enriched['genres_simplified'].notna().sum()}\")\n",
    "print(f\"Books without genres_simplified: {gb_enriched['genres_simplified'].isna().sum()}\")\n",
    "print(f\"Genre simplified coverage: {gb_enriched['genres_simplified'].notna().sum() / len(gb_enriched) * 100:.1f}%\")\n",
    "# Show overall description and publisher coverage\n",
    "print(f\"Books with description_clean: {gb_enriched['description_clean'].notna().sum()}\")\n",
    "print(f\"Books without description_clean: {gb_enriched['description_clean'].isna().sum()}\")\n",
    "print(f\"Books description coverage: {gb_enriched['description_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\")\n",
    "print(f\"Books with publisher_clean: {gb_enriched['publisher_clean'].notna().sum()}\")\n",
    "print(f\"Books without publisher_clean: {gb_enriched['publisher_clean'].isna().sum()}\")\n",
    "print(f\"Books publisher coverage: {gb_enriched['publisher_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e647000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 3\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab0e7c",
   "metadata": {},
   "source": [
    "#### Querying Google Books API (with Quota Management)\n",
    "\n",
    "After OpenLibrary enrichment, we create a new mask to identify remaining gaps: **1,730** books. Google Books API requires an API key and has daily quota limits (1,000 requests/day for free tier), so we implement several strategies: **(1)** process ISBNs in chunks of 1,000, **(2)** add sleep delays between requests, **(3)** cache all results to avoid re-querying, and **(4)** save progress incrementally.\n",
    "\n",
    "We load existing cache if available, query only uncached ISBNs, and update the cache after each session. This approach allows us to spread queries across multiple days if needed while preserving all previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3991bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many books still need enrichment\n",
    "new_missing_mask = (\n",
    "    gb_enriched['language_clean'].isna() |\n",
    "    gb_enriched['language_clean'].isin(['unknown', '', 'None']) |\n",
    "    gb_enriched['pages_clean'].isna() |\n",
    "    gb_enriched['publication_date_clean'].isna() | \n",
    "    gb_enriched['publisher_clean'].isna() |\n",
    "    gb_enriched['description_clean'].isna()\n",
    ")\n",
    "\n",
    "new_to_impute = gb_enriched[new_missing_mask].copy()\n",
    "print(\"Books still needing external enrichment:\", len(new_to_impute))\n",
    "\n",
    "# Show breakdown by field\n",
    "print(\"\\nBreakdown of remaining missing values:\")\n",
    "print(f\"  - Missing pages: {gb_enriched['pages_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing publication_date: {gb_enriched['publication_date_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing/invalid language: {(gb_enriched['language_clean'].isna() | gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()}\")\n",
    "print(f\"  - Missing publisher: {gb_enriched['publisher_clean'].isna().sum()}\")\n",
    "print(f\"  - Missing description: {gb_enriched['description_clean'].isna().sum()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ed215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(data, size=1000):\n",
    "    for i in range(0, len(data), size):\n",
    "        yield data[i:i+size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d16623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define cache path for Google Books in data/raw\n",
    "CACHE_PATH = Path(\"data/raw/google_api_cache.json\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load existing cache if it exists\n",
    "if CACHE_PATH.exists():\n",
    "    with open(CACHE_PATH, \"r\") as f:\n",
    "        google_cache = json.load(f)\n",
    "else:\n",
    "    google_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44817fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_BOOKS_API_KEY\")\n",
    "\n",
    "def query_google_books(isbn):\n",
    "    isbn = str(isbn)\n",
    "\n",
    "    # Check cache\n",
    "    if isbn in google_cache:\n",
    "        return google_cache[isbn]\n",
    "\n",
    "    # Query Google Books with API key\n",
    "    url = (\n",
    "        f\"https://www.googleapis.com/books/v1/volumes?\"\n",
    "        f\"q=isbn:{isbn}&key={GOOGLE_API_KEY}\"\n",
    "    )\n",
    "\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        result = {\"isbn\": isbn, \"error\": f\"HTTP {r.status_code}\"}\n",
    "    else:\n",
    "        data = r.json()\n",
    "        if \"items\" in data and data[\"items\"]:\n",
    "            volume = data[\"items\"][0][\"volumeInfo\"]\n",
    "            result = {\n",
    "                \"isbn\": isbn,\n",
    "                \"title\": volume.get(\"title\"),\n",
    "                \"authors\": volume.get(\"authors\"),\n",
    "                \"publisher\": volume.get(\"publisher\"),\n",
    "                \"publishedDate\": volume.get(\"publishedDate\"),\n",
    "                \"pageCount\": volume.get(\"pageCount\"),\n",
    "                \"categories\": volume.get(\"categories\"),\n",
    "                \"language\": volume.get(\"language\"),\n",
    "                \"description\": volume.get(\"description\"),\n",
    "            }\n",
    "        else:\n",
    "            result = {\"isbn\": isbn, \"error\": \"No results\"}\n",
    "\n",
    "    # Save to cache\n",
    "    google_cache[isbn] = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf3933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_isbns = (\n",
    "    new_to_impute['isbn_query']\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "print(f\"Unique ISBNs to query: {len(list_of_isbns)}\")\n",
    "chunks = list(chunk_list(list_of_isbns, size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Choose which chunk you want to process today\n",
    "chunk_to_process = chunks[1]   # run chunk 0 today, 1 tomorrow, etc.\n",
    "results = []\n",
    "for isbn in tqdm(chunk_to_process, desc=\"Querying Google Books\"):\n",
    "    results.append(query_google_books(isbn))\n",
    "    time.sleep(0.1)   # be nice to the API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f955d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cache after ISBN queries\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(google_cache, f, indent=2)\n",
    "print(f\"Cache updated with {len(google_cache)} entries after ISBN queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d7361",
   "metadata": {},
   "source": [
    "#### Handling Books Without ISBNs\n",
    "\n",
    "Some books lack valid ISBNs but can still be enriched using **title and author search**. Google Books API supports `intitle:` and `inauthor:` query parameters, allowing us to find books by bibliographic metadata instead of identifiers. We create cache keys in `\"title|author\"` format to distinguish these from ISBN-based queries.\n",
    "\n",
    "This fallback strategy significantly increases our enrichment coverage, especially for older books, special editions, or records with ISBN errors. Results are cached alongside ISBN queries to maintain a unified enrichment workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad912ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_google_books_by_title(title, author):\n",
    "    \"\"\"Query Google Books API using title and author when ISBN is unavailable.\"\"\"\n",
    "    \n",
    "    # Create cache key\n",
    "    cache_key = f\"{title}|{author}\"\n",
    "    \n",
    "    if cache_key in google_cache:\n",
    "        return google_cache[cache_key]\n",
    "    \n",
    "    # Build query string\n",
    "    query_parts = []\n",
    "    if pd.notna(title):\n",
    "        query_parts.append(f'intitle:\"{title}\"')\n",
    "    if pd.notna(author):\n",
    "        query_parts.append(f'inauthor:\"{author}\"')\n",
    "    \n",
    "    query_string = \"+\".join(query_parts)\n",
    "    \n",
    "    url = (\n",
    "        f\"https://www.googleapis.com/books/v1/volumes?\"\n",
    "        f\"q={query_string}&key={GOOGLE_API_KEY}\"\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        result = {\"title\": title, \"author\": author, \"error\": f\"HTTP {r.status_code}\"}\n",
    "    else:\n",
    "        data = r.json()\n",
    "        if \"items\" in data and data[\"items\"]:\n",
    "            volume = data[\"items\"][0][\"volumeInfo\"]\n",
    "            result = {\n",
    "                \"title\": title,\n",
    "                \"author\": author,\n",
    "                \"pageCount\": volume.get(\"pageCount\"),\n",
    "                \"publisher\": volume.get(\"publisher\"),  \n",
    "                \"publishedDate\": volume.get(\"publishedDate\"),\n",
    "                \"categories\": volume.get(\"categories\"),\n",
    "                \"language\": volume.get(\"language\"),\n",
    "                \"description\": volume.get(\"description\"),\n",
    "            }\n",
    "        else:\n",
    "            result = {\"title\": title, \"author\": author, \"error\": \"No results\"}\n",
    "    \n",
    "    google_cache[cache_key] = result\n",
    "    return result\n",
    "\n",
    "# Process books without ISBN separately\n",
    "books_without_isbn = new_to_impute[new_to_impute['isbn_query'].isna()].copy()\n",
    "print(f\"Books without ISBN to query by title/author: {len(books_without_isbn)}\")\n",
    "\n",
    "results_by_title = []\n",
    "for idx, row in tqdm(books_without_isbn.iterrows(), \n",
    "                     total=len(books_without_isbn),\n",
    "                     desc=\"Querying Google Books by title/author\"):\n",
    "    results_by_title.append(query_google_books_by_title(\n",
    "        row['title_clean'], \n",
    "        row['author_clean']\n",
    "    ))\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cache after title/author queries\n",
    "with open(CACHE_PATH, \"w\") as f:\n",
    "    json.dump(google_cache, f, indent=2)\n",
    "print(f\"Cache updated with {len(google_cache)} entries after title/author queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25cd3f8",
   "metadata": {},
   "source": [
    "#### Loading and Applying Cached Results\n",
    "\n",
    "The Google Books cache contains results from multiple query sessions, potentially across different days. We load the complete cache and separate ISBN-based results from title/author-based results by checking for the `\"|\"` delimiter in cache keys. This allows us to apply different matching logic for each result type.\n",
    "\n",
    "We then merge cached data back into `gb_enriched`, apply the cleaning pipeline to standardize formats, and fill remaining metadata gaps. The `map_subjects_to_genres()` function maps Google Books categories to our genre taxonomy, further increasing genre coverage. This completes our multi-source enrichment strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the cache\n",
    "CACHE_PATH = Path(\"google_api_cache.json\")\n",
    "\n",
    "with open(CACHE_PATH, \"r\") as f:\n",
    "    google_cache = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(google_cache)} cached entries\")\n",
    "\n",
    "# Separate ISBN-based results from title/author-based results\n",
    "isbn_results = []\n",
    "title_author_results = []\n",
    "\n",
    "for key, value in google_cache.items():\n",
    "    if \"|\" in key:  # Title|Author format\n",
    "        title_author_results.append(value)\n",
    "    else:  # ISBN format\n",
    "        isbn_results.append(value)\n",
    "\n",
    "print(f\"Found {len(isbn_results)} ISBN-based results\")\n",
    "print(f\"Found {len(title_author_results)} title/author-based results\")\n",
    "\n",
    "# add google books data to dataframe\n",
    "google_columns = [\n",
    "    'pageCount_google',\n",
    "    'publishedDate_google',\n",
    "    'categories_google',\n",
    "    'language_google',\n",
    "    'publisher_google',\n",
    "    'description_google'\n",
    "]\n",
    "for col in google_columns:\n",
    "    if col not in gb_enriched.columns:\n",
    "        gb_enriched[col] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741eb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge isbn_results back to gb_enriched\n",
    "if isbn_results:\n",
    "    google_isbn_df = pd.DataFrame(isbn_results)\n",
    "    print(\"\\nISBN results preview:\")\n",
    "    print(google_isbn_df.head())\n",
    "    \n",
    "    # Create ISBN mapping\n",
    "    isbn_to_data = {str(row['isbn']): row for _, row in google_isbn_df.iterrows() \n",
    "                    if 'isbn' in row and pd.notna(row.get('isbn'))}\n",
    "    \n",
    "    # Update gb_enriched\n",
    "    books_with_isbn = new_to_impute[new_to_impute['isbn_query'].notna()].copy()\n",
    "    \n",
    "    for idx in books_with_isbn.index:\n",
    "        isbn = str(books_with_isbn.loc[idx, 'isbn_query'])\n",
    "        if isbn in isbn_to_data:\n",
    "            result = isbn_to_data[isbn]\n",
    "            if pd.isna(result.get('error')):\n",
    "                gb_enriched.loc[idx, 'pageCount_google'] = result.get('pageCount')\n",
    "                gb_enriched.loc[idx, 'publishedDate_google'] = result.get('publishedDate')\n",
    "                gb_enriched.loc[idx, 'categories_google'] = result.get('categories')\n",
    "                gb_enriched.loc[idx, 'language_google'] = result.get('language')\n",
    "                gb_enriched.loc[idx, 'publisher_google'] = result.get('publisher')\n",
    "                gb_enriched.loc[idx, 'description_google'] = result.get('description')\n",
    "    \n",
    "    print(f\"✓ Merged ISBN-based results for {len(isbn_to_data)} books\")\n",
    "\n",
    "\n",
    "# merge title/author results back to gb_enriched\n",
    "\n",
    "if title_author_results:\n",
    "    google_title_df = pd.DataFrame(title_author_results)\n",
    "    print(\"\\nTitle/Author results preview:\")\n",
    "    print(google_title_df.head())\n",
    "    \n",
    "    # Recreate books_without_isbn from new_to_impute\n",
    "    books_without_isbn = new_to_impute[new_to_impute['isbn_query'].isna()].copy()\n",
    "    \n",
    "    # Create title|author key mapping\n",
    "    for i, (idx, row) in enumerate(books_without_isbn.iterrows()):\n",
    "        if i < len(google_title_df):\n",
    "            title_author_key = f\"{row['title_clean']}|{row['author_clean']}\"\n",
    "            if title_author_key in google_cache:\n",
    "                result = google_cache[title_author_key]\n",
    "                if pd.isna(result.get('error')):\n",
    "                    gb_enriched.loc[idx, 'pageCount_google'] = result.get('pageCount')\n",
    "                    gb_enriched.loc[idx, 'publishedDate_google'] = result.get('publishedDate')\n",
    "                    gb_enriched.loc[idx, 'categories_google'] = result.get('categories')\n",
    "                    gb_enriched.loc[idx, 'language_google'] = result.get('language')\n",
    "                    gb_enriched.loc[idx, 'publisher_google'] = result.get('publisher')\n",
    "                    gb_enriched.loc[idx, 'description_google'] = result.get('description')\n",
    "    \n",
    "    print(f\"✓ Merged title/author-based results for {len(books_without_isbn)} books\")\n",
    "\n",
    "# Verify merge\n",
    "print(\"\\nGoogle Books data merged:\")\n",
    "for col in google_columns:\n",
    "    count = gb_enriched[col].notna().sum()\n",
    "    print(f\"  - {col}: {count} values\")\n",
    "\n",
    "# clean Google Books API data\n",
    "from src.cleaning.utils.pipeline import apply_cleaners_selectively\n",
    "\n",
    "gb_enriched = apply_cleaners_selectively(\n",
    "    gb_enriched,\n",
    "    fields_to_clean=[\n",
    "        'pageCount',\n",
    "        'publishedDate',\n",
    "        'language',\n",
    "        'categories',\n",
    "        'publisher',\n",
    "        'description'\n",
    "        ],\n",
    "    source_suffix='_google',\n",
    "    target_suffix='_google_clean',\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# Verify cleaning\n",
    "print(\"\\nSample of cleaned Google Books data:\")\n",
    "display(gb_enriched[[\n",
    "    'title_clean',\n",
    "    'pages_clean',\n",
    "    'pageCount_google',\n",
    "    'pageCount_google_clean',\n",
    "    'publication_date_clean',\n",
    "    'publishedDate_google',\n",
    "    'publishedDate_google_clean',\n",
    "    'language_clean',\n",
    "    'language_google',\n",
    "    'language_google_clean',\n",
    "    'genres_clean',\n",
    "    'categories_google',\n",
    "    'categories_google_clean',\n",
    "    'genres_simplified',\n",
    "    'publisher_google',\n",
    "    'publisher_clean',\n",
    "    'description_google',\n",
    "    'description_clean',\n",
    "]].dropna(subset=['pageCount_google_clean', 'language_google_clean'], how='all').sample(min(15, len(gb_enriched)), random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76157a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after cleaning Google Books data, we'll add genre mapping\n",
    "print(\"\\n--- Generating genres_simplified from Google Books categories ---\")\n",
    "\n",
    "books_needing_google_genre_mapping = (\n",
    "    (gb_enriched['genres_simplified'].isna()) & \n",
    "    (gb_enriched['categories_google_clean'].notna())\n",
    ")\n",
    "\n",
    "print(f\"Books with Google categories but no genres_simplified: {books_needing_google_genre_mapping.sum()}\")\n",
    "\n",
    "if books_needing_google_genre_mapping.sum() > 0:\n",
    "    gb_enriched.loc[books_needing_google_genre_mapping, 'genres_simplified'] = (\n",
    "        gb_enriched.loc[books_needing_google_genre_mapping, 'categories_google_clean']\n",
    "        .apply(lambda x: map_subjects_to_genres(x) if isinstance(x, list) else None)\n",
    "    )\n",
    "    \n",
    "    filled_genres = (\n",
    "        gb_enriched.loc[books_needing_google_genre_mapping, 'genres_simplified'].notna().sum()\n",
    "    )\n",
    "    print(f\"genres_simplified: mapped {filled_genres} values from Google Books categories\")\n",
    "\n",
    "\n",
    "# fill missing values with cleaned Google Books data\n",
    "print(\"\\n--- Filling missing values with cleaned Google Books data ---\")\n",
    "\n",
    "# Fill pages_clean\n",
    "print(\"\\n--- Filling remaining page_clean using Google Books data ---\")\n",
    "before_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "gb_enriched['pages_clean'] = gb_enriched['pages_clean'].fillna(gb_enriched['pageCount_google_clean'])\n",
    "after_pages = gb_enriched['pages_clean'].isna().sum()\n",
    "print(f\"pages_clean: filled {before_pages - after_pages} values from Google Books\")\n",
    "\n",
    "# Fill publication_date_clean\n",
    "print(\"\\n--- Filling remaining publication_date_clean using Google Books data ---\")\n",
    "before_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "gb_enriched['publication_date_clean'] = gb_enriched['publication_date_clean'].fillna(gb_enriched['publishedDate_google_clean'])\n",
    "after_date = gb_enriched['publication_date_clean'].isna().sum()\n",
    "print(f\"publication_date_clean: filled {before_date - after_date} values from Google Books\")\n",
    "\n",
    "# Fill language_clean\n",
    "print(\"\\n--- Filling remaining language_clean using Google Books data ---\")\n",
    "before_lang = (gb_enriched['language_clean'].isna() | \n",
    "               gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "mask = (gb_enriched['language_clean'].isna() | \n",
    "        gb_enriched['language_clean'].isin(['unknown', '', 'None']))\n",
    "gb_enriched.loc[mask, 'language_clean'] = gb_enriched.loc[mask, 'language_google_clean']\n",
    "after_lang = (gb_enriched['language_clean'].isna() | \n",
    "              gb_enriched['language_clean'].isin(['unknown', '', 'None'])).sum()\n",
    "\n",
    "print(f\"language_clean: filled {before_lang - after_lang} values from Google Books\")\n",
    "\n",
    "# Fill publisher_clean\n",
    "print(\"\\n--- Filling remaining publisher_clean using Google Books data ---\")\n",
    "before_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "gb_enriched['publisher_clean'] = gb_enriched['publisher_clean'].fillna(\n",
    "    gb_enriched['publisher_google_clean']\n",
    ")\n",
    "after_publisher = gb_enriched['publisher_clean'].isna().sum()\n",
    "print(f\"publisher_clean: filled {before_publisher - after_publisher} values from Google Books\")\n",
    "\n",
    "\n",
    "# Fill description_clean\n",
    "print(\"\\n--- Filling remaining description_clean using Google Books data ---\")\n",
    "before_desc_google = gb_enriched['description_clean'].isna().sum()\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].fillna(\n",
    "    gb_enriched['description_google_clean']\n",
    ")\n",
    "after_desc_google = gb_enriched['description_clean'].isna().sum()\n",
    "\n",
    "print(f\"description_clean (Google): filled {before_desc_google - after_desc_google} values\")\n",
    "\n",
    "# final enrichment summary\n",
    "print(\"\\n--- FINAL ENRICHMENT SUMMARY (ALL SOURCES) ---\")\n",
    "\n",
    "print(f\"\\nTotal books enriched with Google Books data: {gb_enriched[gb_enriched['categories_google_clean'].notna()].shape[0]}\")\n",
    "\n",
    "print(\"\\n--- FINAL METADATA COVERAGE ---\")\n",
    "print(f\"Books with pages_clean: {gb_enriched['pages_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['pages_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with publication_date_clean: {gb_enriched['publication_date_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['publication_date_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with publisher_clean: {gb_enriched['publisher_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['publisher_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with description_clean: {gb_enriched['description_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['description_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "valid_language = gb_enriched['language_clean'].notna() & ~gb_enriched['language_clean'].isin(['unknown', '', 'None'])\n",
    "print(f\"Books with valid language_clean: {valid_language.sum()} / {len(gb_enriched)} ({valid_language.sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n--- FINAL GENRE COVERAGE ---\")\n",
    "print(f\"Books with genres_clean: {gb_enriched['genres_clean'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['genres_clean'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")\n",
    "print(f\"Books with genres_simplified: {gb_enriched['genres_simplified'].notna().sum()} / {len(gb_enriched)} ({gb_enriched['genres_simplified'].notna().sum() / len(gb_enriched) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'gb_enriched'\n",
    "clean_merge_path = Path(\"data/cleaned/merge\")\n",
    "clean_merge_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "version = 4\n",
    "\n",
    "gb_enriched.to_csv(clean_merge_path / f\"{file_name}_v{version}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} v{version} saved successfully in data/interim/merge directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fdb70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gb_enriched = pd.read_csv('data/cleaned/merge/gb_enriched_v4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14412823",
   "metadata": {},
   "source": [
    "Our multi-source enrichment strategy (BBE → OpenLibrary → Google Books) achieved excellent metadata coverage: **95.1%** for page counts, **100%** for publication dates, **99.4%** for valid language codes, **94.7%** publishers and **85.2%** descriptions. Genre coverage reached **80.8%** for `genres_clean` and **90.7%** for `genres_simplified`, a significant improvement from the original Goodbooks dataset which lacked genre information entirely.\n",
    "\n",
    "This enriched dataset now provides a comprehensive foundation for modeling and analysis. The combination of catalog metadata from BBE, behavioral data from Goodbooks ratings, and API-sourced supplemental information creates a unified dataset that supports both predictive modeling and catalog diversity analysis. The next step is filtering to English-language titles and preparing the final model-ready dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f7600",
   "metadata": {},
   "source": [
    "#### Final Metadata Enrichment Steps\n",
    "\n",
    "After completing API-based enrichment, we perform three final metadata enhancement steps to ensure completeness and consistency across all enriched records:\n",
    "\n",
    "1. **Major Publisher Classification**: We apply the `is_major_publisher` flag to books that received publisher data from APIs but weren't present in the BBE dataset. Using the same publisher pattern matching from Notebook 02, we classify publishers against our curated list of major publishing houses.\n",
    "\n",
    "2. **Awards Flag Completion**: We fill missing `has_award` values with `False` for all books that weren't in the BBE dataset (which contains award metadata). Since API sources don't reliably provide award information, we assume absence of award data means no awards.\n",
    "\n",
    "3. **NLP-Ready Description Generation**: For books enriched with API descriptions, we apply the same NLP cleaning pipeline used in Notebook 02. This converts descriptions into analysis-ready text by removing HTML tags, normalizing whitespace, and standardizing punctuatio, ensuring consistency across BBE and API-sourced descriptions for future text analysis tasks.\n",
    "\n",
    "These steps ensure that all enriched books have the same metadata structure and quality as the original BBE dataset, maintaining consistency across the entire unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813f551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books without is_major_publisher flag: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# load publisher patterns from JSON file\n",
    "with open(\"src/cleaning/mappings/publisher_parent_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    major_publishers = json.load(f)\n",
    "\n",
    "mask_missing_major = gb_enriched['is_major_publisher'].isna()\n",
    "\n",
    "gb_enriched.loc[mask_missing_major, 'is_major_publisher'] = (\n",
    "    gb_enriched.loc[mask_missing_major, 'publisher_clean']\n",
    "        .str.lower()\n",
    "        .apply(lambda x: any(mp in x for mp in major_publishers) if isinstance(x, str) else False)\n",
    ")\n",
    "print(f\"Books without is_major_publisher flag: {gb_enriched['is_major_publisher'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books without has_awards flag: 1918\n",
      "Remaining books without has_awards flag: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reisl\\AppData\\Local\\Temp\\ipykernel_35384\\1980202524.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  .fillna(False)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Books without has_awards flag: {gb_enriched['has_award'].isna().sum()}\")\n",
    "gb_enriched['has_award'] = (\n",
    "    gb_enriched['has_award']\n",
    "    .fillna(False)\n",
    "    .astype('bool')\n",
    ")\n",
    "print(f\"Remaining books without has_awards flag: {gb_enriched['has_award'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cleaning.utils.text_cleaning import (\n",
    "    clean_description_nlp\n",
    ")\n",
    "# generate description_nlp from description_clean where API descriptions exist\n",
    "mask_api_desc = (\n",
    "    gb_enriched['description_openlib'].notna() |\n",
    "    gb_enriched['description_google'].notna()\n",
    ")\n",
    "\n",
    "gb_enriched.loc[mask_api_desc, 'description_nlp'] = (\n",
    "    gb_enriched.loc[mask_api_desc, 'description_clean']\n",
    "        .apply(clean_description_nlp)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77e3add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing description_clean: 1484\n",
      "Missing description_nlp: 1484\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fix_missing_text(col):\n",
    "    return (\n",
    "        gb_enriched[col]\n",
    "        .replace([\"\", \" \", \"None\", \"none\", \"nan\", \"Nan\", \"NAN\"], np.nan)\n",
    "        .replace(r\"^\\s+$\", np.nan, regex=True)\n",
    "    )\n",
    "\n",
    "gb_enriched['description_clean'] = fix_missing_text('description_clean')\n",
    "gb_enriched['description_nlp']   = fix_missing_text('description_nlp')\n",
    "\n",
    "print(\"Missing description_clean:\", gb_enriched['description_clean'].isna().sum())\n",
    "print(\"Missing description_nlp:\", gb_enriched['description_nlp'].isna().sum())\n",
    "gb_enriched['description_clean'] = gb_enriched['description_clean'].astype(\"string\")\n",
    "gb_enriched['description_nlp']   = gb_enriched['description_nlp'].astype(\"string\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c1b3c",
   "metadata": {},
   "source": [
    "### Filtering for English-Language Books\n",
    "\n",
    "To ensure consistency and focus for downstream analysis and modeling, we filter the enriched dataset to include only **English-language books**. This step is critical for:\n",
    "\n",
    "- **Genre diversity analysis**: Comparing genre distributions across a linguistically consistent corpus\n",
    "- **Ratings behavior modeling**: Ensuring user rating patterns reflect a common language context\n",
    "- **Text analysis (stretch)**: Enabling NLP tasks on descriptions without multilingual complexity\n",
    "\n",
    "We create a filtered copy of `gb_enriched` containing only books where `language_clean` is identified as English (using ISO 639 language code`'en'`). This filtered dataset will serve as the primary input for modeling and analysis, while the full enriched dataset (including non-English titles) is preserved for reference.\n",
    "\n",
    "The English-only dataset is saved as the final output, ready for exploratory analysis and model development in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c3ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered (EN only): (9742, 60)\n"
     ]
    }
   ],
   "source": [
    "gb_enriched_en = gb_enriched[gb_enriched['language_clean'] == 'en'].copy()\n",
    "print(\"Filtered (EN only):\", gb_enriched_en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff346538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_book_catalog saved successfully in outputs\\datasets\\cleaned directory.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create data folder if not exists\n",
    "file_name = 'en_book_catalog'\n",
    "clean_path = Path(\"outputs/datasets/cleaned\")\n",
    "clean_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gb_enriched_en.to_csv(clean_path / f\"{file_name}.csv\", index=False)\n",
    "\n",
    "print(f\"{file_name} saved successfully in {clean_path} directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
